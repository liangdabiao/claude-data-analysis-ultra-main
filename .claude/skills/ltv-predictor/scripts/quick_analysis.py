#!/usr/bin/env python3
"""
å¿«é€Ÿåˆ†æè„šæœ¬
ä¸€é”®å®ŒæˆLTVé¢„æµ‹åˆ†ææµç¨‹çš„ä¾¿åˆ©å·¥å…·
åŸºäºç¬¬3è¯¾ç†è®ºå®ç°çš„å®Œæ•´è‡ªåŠ¨åŒ–åˆ†æ
"""

import sys
import os
import pandas as pd
import numpy as np
from pathlib import Path
from typing import Dict, Any, Optional
import argparse
import warnings
warnings.filterwarnings('ignore')

# æ·»åŠ æ¨¡å—è·¯å¾„
current_dir = Path(__file__).parent
sys.path.append(str(current_dir))

from ltv_predictor import LTVPredictor, complete_ltv_analysis
from visualizer import DataVisualizer, quick_visualization
from report_generator import generate_comprehensive_report

def quick_ltv_analysis(file_path: str,
                      feature_period_months: int = 3,
                      prediction_period_months: int = 12,
                      output_dir: str = './ltv_analysis_results',
                      config: Optional[Dict] = None,
                      generate_charts: bool = True,
                      generate_reports: bool = True) -> Dict[str, Any]:
    """
    å¿«é€ŸLTVåˆ†æ - ä¸€é”®å®Œæˆå®Œæ•´çš„LTVé¢„æµ‹æµç¨‹

    Args:
        file_path: è®¢å•æ•°æ®æ–‡ä»¶è·¯å¾„
        feature_period_months: ç‰¹å¾è®¡ç®—æ—¶é—´çª—å£ï¼ˆæœˆï¼‰
        prediction_period_months: é¢„æµ‹æ—¶é—´çª—å£ï¼ˆæœˆï¼‰
        output_dir: è¾“å‡ºç›®å½•
        config: è‡ªå®šä¹‰é…ç½®
        generate_charts: æ˜¯å¦ç”Ÿæˆå›¾è¡¨
        generate_reports: æ˜¯å¦ç”ŸæˆæŠ¥å‘Š

    Returns:
        å®Œæ•´åˆ†æç»“æœå­—å…¸
    """
    print("ğŸš€ å¯åŠ¨å¿«é€ŸLTVåˆ†æ...")
    print("=" * 60)
    print(f"ğŸ“ è¾“å…¥æ–‡ä»¶: {file_path}")
    print(f"ğŸ“Š ç‰¹å¾è®¡ç®—æœŸ: {feature_period_months}ä¸ªæœˆ")
    print(f"ğŸ¯ é¢„æµ‹æœŸ: {prediction_period_months}ä¸ªæœˆ")
    print(f"ğŸ“ è¾“å‡ºç›®å½•: {output_dir}")
    print("=" * 60)

    # åˆ›å»ºè¾“å‡ºç›®å½•
    Path(output_dir).mkdir(parents=True, exist_ok=True)

    # å‡†å¤‡é…ç½®
    if config is None:
        config = {
            'data_processor_config': {
                'feature_period_months': feature_period_months,
                'prediction_period_months': prediction_period_months,
                'remove_outliers': True,
                'min_orders_per_customer': 1
            },
            'regression_config': {
                'test_size': 0.2,
                'cv_folds': 5,
                'scoring_metric': 'r2',
                'enable_hyperparameter_tuning': False
            },
            'models_to_train': ['linear_regression', 'random_forest']
        }

    try:
        # 1. å®Œæ•´LTVåˆ†ææµç¨‹
        results = complete_ltv_analysis(file_path, output_dir, config)

        # æå–ç»“æœ
        predictor = results['predictor']
        rfm_data = results['rfm_data']
        training_results = results['training_results']
        summary_report = results['summary_report']

        # 2. ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨
        chart_paths = {}
        if generate_charts:
            print("\nğŸ“ˆ ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨...")
            try:
                # è·å–é¢„æµ‹ç»“æœç”¨äºå¯è§†åŒ–
                X_test = predictor.training_data['X_test']
                y_test = predictor.training_data['y_test']
                y_pred = predictor.predict(predictor.regression_models.best_model_name, X_test)

                charts_dir = os.path.join(output_dir, 'charts')
                chart_paths = quick_visualization(
                    rfm_data=rfm_data,
                    model_results=training_results['model_results'],
                    output_dir=charts_dir,
                    show_plots=False
                )

                # æ·»åŠ é¢„æµ‹åˆ†æå›¾
                visualizer = DataVisualizer()
                pred_chart_path = os.path.join(charts_dir, '06_prediction_analysis.png')
                visualizer.plot_prediction_analysis(
                    y_test, y_pred,
                    model_name=predictor.regression_models.best_model_name,
                    save_path=pred_chart_path,
                    show_plot=False
                )
                chart_paths['prediction_analysis'] = pred_chart_path

            except Exception as e:
                print(f"âš ï¸ å›¾è¡¨ç”Ÿæˆå¤±è´¥: {str(e)}")

        # 3. ç”Ÿæˆåˆ†ææŠ¥å‘Š
        report_paths = {}
        if generate_reports:
            print("\nğŸ“‹ ç”Ÿæˆåˆ†ææŠ¥å‘Š...")
            try:
                reports_dir = os.path.join(output_dir, 'reports')
                Path(reports_dir).mkdir(parents=True, exist_ok=True)

                feature_importance = training_results.get('feature_importance', {})
                report_paths = generate_comprehensive_report(
                    rfm_data=rfm_data,
                    model_results=training_results['model_results'],
                    feature_importance=feature_importance,
                    output_dir=reports_dir,
                    report_title=f'å®¢æˆ·ç”Ÿå‘½å‘¨æœŸä»·å€¼åˆ†ææŠ¥å‘Š - {Path(file_path).stem}'
                )

            except Exception as e:
                print(f"âš ï¸ æŠ¥å‘Šç”Ÿæˆå¤±è´¥: {str(e)}")

        # 4. ç”Ÿæˆåˆ†ææ‘˜è¦
        summary = generate_analysis_summary(results, chart_paths, report_paths)

        # ä¿å­˜æ‘˜è¦
        summary_path = os.path.join(output_dir, 'analysis_summary.json')
        with open(summary_path, 'w', encoding='utf-8') as f:
            import json
            json.dump(summary, f, ensure_ascii=False, indent=2, default=str)

        # 5. æ‰“å°æœ€ç»ˆç»“æœ
        print("\n" + "=" * 60)
        print("ğŸ‰ å¿«é€ŸLTVåˆ†æå®Œæˆï¼")
        print("=" * 60)

        if summary_report and 'best_model_performance' in summary_report:
            best_perf = summary_report['best_model_performance']
            print(f"\nğŸ“Š æ ¸å¿ƒç»“æœ:")
            print(f"  âœ… æœ€ä½³æ¨¡å‹: {best_perf.get('model_name', 'Unknown')}")
            print(f"  ğŸ“ˆ æ¨¡å‹RÂ²: {best_perf.get('r2_score', 0):.4f}")
            print(f"  ğŸ‘¥ åˆ†æå®¢æˆ·: {summary['data_summary']['total_customers']:,}")
            print(f"  ğŸ’° å¹³å‡LTV: {summary['data_summary']['avg_ltv']:,.0f}")

        print(f"\nğŸ“ ç”Ÿæˆæ–‡ä»¶:")
        print(f"  ğŸ“„ RFMç‰¹å¾æ•°æ®: {results['output_paths']['rfm_features']}")
        print(f"  ğŸ¤– è®­ç»ƒæ¨¡å‹: {results['output_paths']['models']}")
        print(f"  ğŸ“‹ åˆ†ææ‘˜è¦: {summary_path}")

        if chart_paths:
            print(f"  ğŸ“ˆ å¯è§†åŒ–å›¾è¡¨: {len(chart_paths)}ä¸ª")

        if report_paths:
            print(f"  ğŸ“„ åˆ†ææŠ¥å‘Š: {len(report_paths)}ä¸ª")

        # ç»„åˆå®Œæ•´ç»“æœ
        complete_results = {
            **results,
            'chart_paths': chart_paths,
            'report_paths': report_paths,
            'summary': summary,
            'summary_path': summary_path
        }

        return complete_results

    except Exception as e:
        print(f"\nâŒ å¿«é€ŸLTVåˆ†æå¤±è´¥: {str(e)}")
        raise

def generate_analysis_summary(results: Dict[str, Any], chart_paths: Dict[str, str], report_paths: Dict[str, str]) -> Dict[str, Any]:
    """ç”Ÿæˆåˆ†ææ‘˜è¦"""
    predictor = results['predictor']
    rfm_data = results['rfm_data']
    training_results = results['training_results']
    summary_report = results['summary_report']

    # åŸºç¡€ç»Ÿè®¡
    data_summary = {
        'total_customers': len(rfm_data),
        'total_orders': summary_report.get('data_summary', {}).get('total_orders', 'Unknown'),
        'date_range': summary_report.get('data_summary', {}).get('date_range', ['Unknown', 'Unknown']),
        'avg_ltv': rfm_data['å¹´åº¦LTV'].mean() if 'å¹´åº¦LTV' in rfm_data.columns else 0,
        'median_ltv': rfm_data['å¹´åº¦LTV'].median() if 'å¹´åº¦LTV' in rfm_data.columns else 0
    }

    # æ¨¡å‹æ€§èƒ½æ‘˜è¦
    model_summary = {}
    if summary_report and 'best_model_performance' in summary_report:
        best_perf = summary_report['best_model_performance']
        model_summary = {
            'best_model': best_perf.get('model_name', 'Unknown'),
            'best_r2_score': best_perf.get('r2_score', 0),
            'best_mae': best_perf.get('mae', 0),
            'best_rmse': best_perf.get('rmse', 0),
            'best_mape': best_perf.get('mape', 0)
        }

    # ç‰¹å¾é‡è¦æ€§æ‘˜è¦
    feature_summary = {}
    if 'feature_analysis' in summary_report and 'best_model_features' in summary_report['feature_analysis']:
        best_features = summary_report['feature_analysis']['best_model_features']
        if best_features:
            feature_summary = {
                'most_important_feature': summary_report['feature_analysis'].get('most_important_feature', 'Unknown'),
                'feature_importance': best_features
            }

    # å®¢æˆ·åˆ†ç¾¤æ‘˜è¦
    segment_summary = {}
    if 'å®¢æˆ·ä»·å€¼åˆ†å±‚' in rfm_data.columns:
        segment_counts = rfm_data['å®¢æˆ·ä»·å€¼åˆ†å±‚'].value_counts()
        segment_summary = {
            'segment_counts': segment_counts.to_dict(),
            'high_value_customers': segment_counts.get('ç™½é‡‘å®¢æˆ·', 0) + segment_counts.get('é’»çŸ³å®¢æˆ·', 0)
        }

    # ä¸šåŠ¡å»ºè®®æ‘˜è¦
    recommendations_summary = []
    if summary_report and 'recommendations' in summary_report:
        recommendations_summary = summary_report['recommendations']

    summary = {
        'timestamp': pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S'),
        'data_summary': data_summary,
        'model_summary': model_summary,
        'feature_summary': feature_summary,
        'segment_summary': segment_summary,
        'recommendations_summary': recommendations_summary,
        'generated_files': {
            'charts': chart_paths,
            'reports': report_paths
        }
    }

    return summary

def predict_new_customers(model_dir: str, new_orders_file: str, output_path: str = 'new_customer_predictions.csv'):
    """
    ä¸ºæ–°å®¢æˆ·é¢„æµ‹LTV

    Args:
        model_dir: è®­ç»ƒå¥½çš„æ¨¡å‹ç›®å½•
        new_orders_file: æ–°å®¢æˆ·è®¢å•æ•°æ®æ–‡ä»¶
        output_path: é¢„æµ‹ç»“æœè¾“å‡ºè·¯å¾„
    """
    print("ğŸ”® ä¸ºæ–°å®¢æˆ·é¢„æµ‹LTV...")

    # åˆå§‹åŒ–é¢„æµ‹å™¨å¹¶åŠ è½½æ¨¡å‹
    predictor = LTVPredictor()
    predictor.load_results(model_dir)

    # åŠ è½½æ–°å®¢æˆ·æ•°æ®
    new_customers_data = pd.read_csv(new_orders_file)
    print(f"  - æ–°å®¢æˆ·æ•°æ®: {new_customers_data.shape}")

    # é¢„æµ‹LTV
    predictions = predictor.predict_new_customers(new_customers_data)

    # ä¿å­˜é¢„æµ‹ç»“æœ
    predictions.to_csv(output_path, index=False, encoding='utf-8-sig')
    print(f"âœ“ é¢„æµ‹ç»“æœå·²ä¿å­˜: {output_path}")

    return predictions

def batch_predict_ltv(model_dir: str, rfm_features_file: str, output_path: str = 'batch_predictions.csv'):
    """
    æ‰¹é‡é¢„æµ‹LTV

    Args:
        model_dir: è®­ç»ƒå¥½çš„æ¨¡å‹ç›®å½•
        rfm_features_file: RFMç‰¹å¾æ–‡ä»¶
        output_path: é¢„æµ‹ç»“æœè¾“å‡ºè·¯å¾„
    """
    print("ğŸ“Š æ‰¹é‡LTVé¢„æµ‹...")

    # åˆå§‹åŒ–é¢„æµ‹å™¨å¹¶åŠ è½½æ¨¡å‹
    predictor = LTVPredictor()
    predictor.load_results(model_dir)

    # åŠ è½½RFMç‰¹å¾æ•°æ®
    rfm_data = pd.read_csv(rfm_features_file)
    print(f"  - é¢„æµ‹å®¢æˆ·æ•°: {len(rfm_data)}")

    # é¢„æµ‹LTV
    feature_columns = predictor.config['feature_columns']
    X = rfm_data[feature_columns]
    predictions = predictor.predict(X)

    # ä¿å­˜é¢„æµ‹ç»“æœ
    result_df = rfm_data.copy()
    result_df['é¢„æµ‹LTV'] = predictions
    result_df['é¢„æµ‹æ—¶é—´'] = pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')

    result_df.to_csv(output_path, index=False, encoding='utf-8-sig')
    print(f"âœ“ æ‰¹é‡é¢„æµ‹ç»“æœå·²ä¿å­˜: {output_path}")

    return result_df

def compare_models_performance(model_dir: str):
    """
    æ¯”è¾ƒå·²è®­ç»ƒæ¨¡å‹çš„æ€§èƒ½

    Args:
        model_dir: æ¨¡å‹ç›®å½•
    """
    print("ğŸ“ˆ æ¨¡å‹æ€§èƒ½æ¯”è¾ƒ...")

    # åŠ è½½æ¨¡å‹
    predictor = LTVPredictor()
    predictor.load_results(model_dir)

    # è·å–æ¨¡å‹æ‘˜è¦
    summary = predictor.regression_models.get_model_summary()

    print("\næ¨¡å‹æ€§èƒ½æ’å:")
    print("-" * 50)

    if summary and 'model_performance' in summary:
        # æŒ‰RÂ²åˆ†æ•°æ’åº
        model_perf = summary['model_performance']
        sorted_models = sorted(model_perf.items(), key=lambda x: x[1].get('r2_score', 0), reverse=True)

        for rank, (model_name, performance) in enumerate(sorted_models, 1):
            r2_score = performance.get('r2_score', 0)
            mae = performance.get('mae', 0)
            rmse = performance.get('rmse', 0)
            mape = performance.get('mape', 0)

            print(f"{rank}. {model_name}")
            print(f"   RÂ² åˆ†æ•°: {r2_score:.4f}")
            print(f"   MAE: {mae:.2f}")
            print(f"   RMSE: {rmse:.2f}")
            print(f"   MAPE: {mape:.2f}%")
            print()

    # æœ€ä½³æ¨¡å‹
    if summary.get('best_model'):
        print(f"ğŸ† æœ€ä½³æ¨¡å‹: {summary['best_model']}")
        if 'best_performance' in summary:
            best_perf = summary['best_performance']
            print(f"   RÂ² åˆ†æ•°: {best_perf.get('r2_score', 0):.4f}")

def main():
    """ä¸»å‡½æ•° - å‘½ä»¤è¡Œæ¥å£"""
    parser = argparse.ArgumentParser(description='LTVé¢„æµ‹åˆ†æå·¥å…·')
    subparsers = parser.add_subparsers(dest='command', help='å¯ç”¨å‘½ä»¤')

    # åˆ†æå‘½ä»¤
    analyze_parser = subparsers.add_parser('analyze', help='æ‰§è¡Œå®Œæ•´çš„LTVåˆ†æ')
    analyze_parser.add_argument('input_file', help='è®¢å•æ•°æ®æ–‡ä»¶è·¯å¾„')
    analyze_parser.add_argument('--output-dir', '-o', default='./ltv_results', help='è¾“å‡ºç›®å½•')
    analyze_parser.add_argument('--feature-period', type=int, default=3, help='ç‰¹å¾è®¡ç®—æœŸï¼ˆæœˆï¼‰')
    analyze_parser.add_argument('--prediction-period', type=int, default=12, help='é¢„æµ‹æœŸï¼ˆæœˆï¼‰')
    analyze_parser.add_argument('--no-charts', action='store_true', help='ä¸ç”Ÿæˆå›¾è¡¨')
    analyze_parser.add_argument('--no-reports', action='store_true', help='ä¸ç”ŸæˆæŠ¥å‘Š')

    # é¢„æµ‹å‘½ä»¤
    predict_parser = subparsers.add_parser('predict', help='é¢„æµ‹æ–°å®¢æˆ·LTV')
    predict_parser.add_argument('model_dir', help='è®­ç»ƒå¥½çš„æ¨¡å‹ç›®å½•')
    predict_parser.add_argument('input_file', help='æ–°å®¢æˆ·è®¢å•æ•°æ®æ–‡ä»¶')
    predict_parser.add_argument('--output', '-o', default='predictions.csv', help='è¾“å‡ºæ–‡ä»¶è·¯å¾„')

    # æ‰¹é‡é¢„æµ‹å‘½ä»¤
    batch_parser = subparsers.add_parser('batch', help='æ‰¹é‡é¢„æµ‹LTV')
    batch_parser.add_argument('model_dir', help='è®­ç»ƒå¥½çš„æ¨¡å‹ç›®å½•')
    batch_parser.add_argument('rfm_file', help='RFMç‰¹å¾æ–‡ä»¶')
    batch_parser.add_argument('--output', '-o', default='batch_predictions.csv', help='è¾“å‡ºæ–‡ä»¶è·¯å¾„')

    # æ¯”è¾ƒå‘½ä»¤
    compare_parser = subparsers.add_parser('compare', help='æ¯”è¾ƒæ¨¡å‹æ€§èƒ½')
    compare_parser.add_argument('model_dir', help='æ¨¡å‹ç›®å½•')

    args = parser.parse_args()

    if args.command == 'analyze':
        # æ‰§è¡Œåˆ†æ
        results = quick_ltv_analysis(
            file_path=args.input_file,
            feature_period_months=args.feature_period,
            prediction_period_months=args.prediction_period,
            output_dir=args.output_dir,
            generate_charts=not args.no_charts,
            generate_reports=not args.no_reports
        )

    elif args.command == 'predict':
        # é¢„æµ‹æ–°å®¢æˆ·
        predict_new_customers(args.model_dir, args.input_file, args.output)

    elif args.command == 'batch':
        # æ‰¹é‡é¢„æµ‹
        batch_predict_ltv(args.model_dir, args.rfm_file, args.output)

    elif args.command == 'compare':
        # æ¯”è¾ƒæ¨¡å‹
        compare_models_performance(args.model_dir)

    else:
        parser.print_help()

if __name__ == "__main__":
    # ç¤ºä¾‹ä½¿ç”¨
    print("ğŸ”§ å¿«é€Ÿåˆ†æå·¥å…·æµ‹è¯•")

    # å¦‚æœé€šè¿‡å‘½ä»¤è¡Œè¿è¡Œï¼Œä½¿ç”¨argparse
    if len(sys.argv) > 1:
        main()
    else:
        print("ä½¿ç”¨ 'python quick_analysis.py --help' æŸ¥çœ‹ä½¿ç”¨è¯´æ˜")

        # ç®€å•æµ‹è¯•
        sample_file = '../data/sample_orders.csv'
        if os.path.exists(sample_file):
            print(f"å‘ç°ç¤ºä¾‹æ•°æ®æ–‡ä»¶: {sample_file}")
            print("å¯ä»¥ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤è¿›è¡Œæµ‹è¯•:")
            print(f"python quick_analysis.py analyze {sample_file} --output-dir ./test_results")
        else:
            print("âš ï¸ æœªæ‰¾åˆ°ç¤ºä¾‹æ•°æ®æ–‡ä»¶")