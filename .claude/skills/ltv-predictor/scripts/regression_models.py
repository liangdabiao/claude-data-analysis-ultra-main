#!/usr/bin/env python3
"""
å›å½’ç®—æ³•æ¨¡å—
åŸºäºç¬¬3è¯¾æ ¸å¿ƒç®—æ³•å®ç°çº¿æ€§å›å½’å’Œéšæœºæ£®æ—ç­‰LTVé¢„æµ‹ç®—æ³•
"""

import numpy as np
import pandas as pd
from typing import Dict, List, Tuple, Optional, Any, Union
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error
import joblib
import warnings
warnings.filterwarnings('ignore')

class RegressionModels:
    """
    å›å½’ç®—æ³•é›†åˆ

    åŸºäºç¬¬3è¯¾ç†è®ºå®ç°çš„LTVé¢„æµ‹å›å½’æ¨¡å‹
    æ”¯æŒçº¿æ€§å›å½’ã€éšæœºæ£®æ—ç­‰å¤šç§ç®—æ³•
    """

    def __init__(self, config: Optional[Dict] = None):
        """
        åˆå§‹åŒ–å›å½’æ¨¡å‹

        Args:
            config: é…ç½®å‚æ•°å­—å…¸
        """
        # é»˜è®¤é…ç½®
        self.config = {
            'test_size': 0.2,
            'random_state': 42,
            'cv_folds': 5,
            'scoring_metric': 'r2',
            'enable_hyperparameter_tuning': False,
            'n_iter_search': 50,
            'feature_columns': ['Rå€¼', 'Få€¼', 'Må€¼'],
            'target_column': 'å¹´åº¦LTV'
        }

        # æ›´æ–°é…ç½®
        if config:
            self.config.update(config)

        # æ¨¡å‹å­˜å‚¨
        self.models = {}
        self.trained_models = {}
        self.model_performance = {}
        self.feature_importance = {}
        self.best_model_name = None

        # æ•°æ®å­˜å‚¨
        self.X_train = None
        self.X_test = None
        self.y_train = None
        self.y_test = None
        self.feature_names = None

    def prepare_data(self, rfm_data: pd.DataFrame,
                    feature_columns: Optional[List[str]] = None,
                    target_column: Optional[str] = None) -> Tuple[pd.DataFrame, pd.Series]:
        """
        å‡†å¤‡è®­ç»ƒæ•°æ®

        Args:
            rfm_data: RFMç‰¹å¾æ•°æ®
            feature_columns: ç‰¹å¾åˆ—ååˆ—è¡¨
            target_column: ç›®æ ‡åˆ—å

        Returns:
            ç‰¹å¾çŸ©é˜µå’Œç›®æ ‡å‘é‡
        """
        if feature_columns is None:
            feature_columns = self.config['feature_columns']
        if target_column is None:
            target_column = self.config['target_column']

        # æ£€æŸ¥å¿…éœ€åˆ—æ˜¯å¦å­˜åœ¨
        missing_features = [col for col in feature_columns if col not in rfm_data.columns]
        if missing_features:
            raise ValueError(f"ç¼ºå°‘ç‰¹å¾åˆ—: {missing_features}")

        if target_column not in rfm_data.columns:
            raise ValueError(f"ç¼ºå°‘ç›®æ ‡åˆ—: {target_column}")

        # å‡†å¤‡ç‰¹å¾å’Œç›®æ ‡
        X = rfm_data[feature_columns].copy()
        y = rfm_data[target_column].copy()

        # å¤„ç†ç¼ºå¤±å€¼
        X = X.fillna(X.mean())
        y = y.fillna(y.mean())

        # ç§»é™¤å¼‚å¸¸å€¼ï¼ˆä½¿ç”¨3ÏƒåŸåˆ™ï¼‰
        for col in X.columns:
            mean = X[col].mean()
            std = X[col].std()
            outlier_mask = np.abs(X[col] - mean) > 3 * std
            if outlier_mask.any():
                print(f"  ç§»é™¤{col}åˆ—å¼‚å¸¸å€¼: {outlier_mask.sum()} ä¸ª")
                X = X[~outlier_mask]
                y = y[~outlier_mask]

        # ç§»é™¤LTVå¼‚å¸¸å€¼
        ltv_mean = y.mean()
        ltv_std = y.std()
        ltv_outlier_mask = np.abs(y - ltv_mean) > 3 * ltv_std
        if ltv_outlier_mask.any():
            print(f"  ç§»é™¤LTVå¼‚å¸¸å€¼: {ltv_outlier_mask.sum()} ä¸ª")
            X = X[~ltv_outlier_mask]
            y = y[~ltv_outlier_mask]

        self.feature_names = feature_columns
        print(f"âœ“ æ•°æ®å‡†å¤‡å®Œæˆ: {X.shape}, ç›®æ ‡å€¼èŒƒå›´: {y.min():.2f} ~ {y.max():.2f}")

        return X, y

    def split_data(self, X: pd.DataFrame, y: pd.Series) -> Tuple[pd.DataFrame, pd.DataFrame, pd.Series, pd.Series]:
        """
        åˆ†å‰²è®­ç»ƒå’Œæµ‹è¯•æ•°æ®

        Args:
            X: ç‰¹å¾çŸ©é˜µ
            y: ç›®æ ‡å‘é‡

        Returns:
            è®­ç»ƒå’Œæµ‹è¯•æ•°æ®
        """
        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(
            X, y,
            test_size=self.config['test_size'],
            random_state=self.config['random_state']
        )

        print(f"âœ“ æ•°æ®åˆ†å‰²å®Œæˆ:")
        print(f"  - è®­ç»ƒé›†: {self.X_train.shape}")
        print(f"  - æµ‹è¯•é›†: {self.X_test.shape}")

        return self.X_train, self.X_test, self.y_train, self.y_test

    def create_linear_regression_model(self) -> LinearRegression:
        """
        åˆ›å»ºçº¿æ€§å›å½’æ¨¡å‹

        Returns:
            çº¿æ€§å›å½’æ¨¡å‹å®ä¾‹
        """
        model = LinearRegression()
        self.models['linear_regression'] = model
        return model

    def create_random_forest_model(self, **kwargs) -> RandomForestRegressor:
        """
        åˆ›å»ºéšæœºæ£®æ—æ¨¡å‹

        Args:
            **kwargs: æ¨¡å‹å‚æ•°

        Returns:
            éšæœºæ£®æ—æ¨¡å‹å®ä¾‹
        """
        # é»˜è®¤å‚æ•°ï¼ˆåŸºäºç¬¬3è¯¾ä¼˜åŒ–ï¼‰
        default_params = {
            'n_estimators': 100,
            'max_depth': 10,
            'min_samples_split': 5,
            'min_samples_leaf': 2,
            'random_state': self.config['random_state'],
            'n_jobs': -1
        }

        # æ›´æ–°å‚æ•°
        params = {**default_params, **kwargs}
        model = RandomForestRegressor(**params)

        self.models['random_forest'] = model
        return model

    def train_single_model(self,
                          model_name: str,
                          X_train: Optional[pd.DataFrame] = None,
                          y_train: Optional[pd.Series] = None,
                          tune_hyperparameters: bool = False) -> Dict[str, Any]:
        """
        è®­ç»ƒå•ä¸ªæ¨¡å‹

        Args:
            model_name: æ¨¡å‹åç§°
            X_train: è®­ç»ƒç‰¹å¾
            y_train: è®­ç»ƒç›®æ ‡
            tune_hyperparameters: æ˜¯å¦è°ƒå‚

        Returns:
            è®­ç»ƒç»“æœå­—å…¸
        """
        if X_train is None:
            X_train = self.X_train
        if y_train is None:
            y_train = self.y_train

        if X_train is None or y_train is None:
            raise ValueError("è®­ç»ƒæ•°æ®æœªå‡†å¤‡ï¼Œè¯·å…ˆè°ƒç”¨split_data")

        if model_name not in self.models:
            if model_name == 'linear_regression':
                self.create_linear_regression_model()
            elif model_name == 'random_forest':
                self.create_random_forest_model()
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡å‹ç±»å‹: {model_name}")

        model = self.models[model_name]

        print(f"ğŸ¤– è®­ç»ƒ{model_name}æ¨¡å‹...")

        # è¶…å‚æ•°è°ƒä¼˜
        if tune_hyperparameters and model_name == 'random_forest':
            model = self._tune_random_forest(model, X_train, y_train)
            self.models[model_name] = model

        # è®­ç»ƒæ¨¡å‹
        model.fit(X_train, y_train)
        self.trained_models[model_name] = model

        # è¯„ä¼°æ¨¡å‹
        train_score = model.score(X_train, y_train)
        cv_scores = cross_val_score(model, X_train, y_train,
                                   cv=self.config['cv_folds'],
                                   scoring=self.config['scoring_metric'])

        results = {
            'model_name': model_name,
            'model': model,
            'train_score': train_score,
            'cv_mean_score': cv_scores.mean(),
            'cv_std_score': cv_scores.std(),
            'feature_importance': self._get_feature_importance(model) if hasattr(model, 'feature_importances_') else None
        }

        print(f"âœ“ {model_name}è®­ç»ƒå®Œæˆ:")
        print(f"  - è®­ç»ƒé›†RÂ²: {train_score:.4f}")
        print(f"  - äº¤å‰éªŒè¯RÂ²: {cv_scores.mean():.4f} (Â±{cv_scores.std():.4f})")

        return results

    def _tune_random_forest(self, model: RandomForestRegressor, X_train: pd.DataFrame, y_train: pd.Series) -> RandomForestRegressor:
        """
        éšæœºæ£®æ—è¶…å‚æ•°è°ƒä¼˜

        Args:
            model: éšæœºæ£®æ—æ¨¡å‹
            X_train: è®­ç»ƒç‰¹å¾
            y_train: è®­ç»ƒç›®æ ‡

        Returns:
            è°ƒä¼˜åçš„æ¨¡å‹
        """
        print("  è¿›è¡Œè¶…å‚æ•°è°ƒä¼˜...")

        # å‚æ•°æœç´¢ç©ºé—´
        param_grid = {
            'n_estimators': [50, 100, 200],
            'max_depth': [5, 10, 15, None],
            'min_samples_split': [2, 5, 10],
            'min_samples_leaf': [1, 2, 4]
        }

        # ç½‘æ ¼æœç´¢
        grid_search = GridSearchCV(
            model,
            param_grid,
            cv=self.config['cv_folds'],
            scoring=self.config['scoring_metric'],
            n_jobs=-1,
            verbose=0
        )

        grid_search.fit(X_train, y_train)

        best_model = grid_search.best_estimator_
        print(f"  æœ€ä½³å‚æ•°: {grid_search.best_params_}")
        print(f"  æœ€ä½³åˆ†æ•°: {grid_search.best_score_:.4f}")

        return best_model

    def _get_feature_importance(self, model) -> Optional[Dict[str, float]]:
        """
        è·å–ç‰¹å¾é‡è¦æ€§

        Args:
            model: è®­ç»ƒå¥½çš„æ¨¡å‹

        Returns:
            ç‰¹å¾é‡è¦æ€§å­—å…¸
        """
        if hasattr(model, 'feature_importances_') and self.feature_names:
            importance_dict = dict(zip(self.feature_names, model.feature_importances_))
            # æŒ‰é‡è¦æ€§æ’åº
            return dict(sorted(importance_dict.items(), key=lambda x: x[1], reverse=True))
        elif hasattr(model, 'coef_') and self.feature_names:
            importance_dict = dict(zip(self.feature_names, np.abs(model.coef_)))
            return dict(sorted(importance_dict.items(), key=lambda x: x[1], reverse=True))
        return None

    def evaluate_model(self,
                      model_name: str,
                      X_test: Optional[pd.DataFrame] = None,
                      y_test: Optional[pd.Series] = None) -> Dict[str, float]:
        """
        è¯„ä¼°æ¨¡å‹æ€§èƒ½

        Args:
            model_name: æ¨¡å‹åç§°
            X_test: æµ‹è¯•ç‰¹å¾
            y_test: æµ‹è¯•ç›®æ ‡

        Returns:
            è¯„ä¼°æŒ‡æ ‡å­—å…¸
        """
        if X_test is None:
            X_test = self.X_test
        if y_test is None:
            y_test = self.y_test

        if model_name not in self.trained_models:
            raise ValueError(f"æ¨¡å‹{model_name}æœªè®­ç»ƒ")

        model = self.trained_models[model_name]
        y_pred = model.predict(X_test)

        # è®¡ç®—è¯„ä¼°æŒ‡æ ‡
        r2 = r2_score(y_test, y_pred)
        mae = mean_absolute_error(y_test, y_pred)
        mse = mean_squared_error(y_test, y_pred)
        rmse = np.sqrt(mse)

        # ç›¸å¯¹è¯¯å·®æŒ‡æ ‡
        mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100

        metrics = {
            'r2_score': r2,
            'mae': mae,
            'mse': mse,
            'rmse': rmse,
            'mape': mape
        }

        # å­˜å‚¨è¯„ä¼°ç»“æœ
        self.model_performance[model_name] = metrics
        self.feature_importance[model_name] = self._get_feature_importance(model)

        print(f"ğŸ“Š {model_name}æ¨¡å‹è¯„ä¼°ç»“æœ:")
        print(f"  - RÂ² åˆ†æ•°: {r2:.4f}")
        print(f"  - MAE: {mae:.2f}")
        print(f"  - RMSE: {rmse:.2f}")
        print(f"  - MAPE: {mape:.2f}%")

        return metrics

    def train_multiple_models(self,
                            model_names: List[str],
                            X_train: Optional[pd.DataFrame] = None,
                            y_train: Optional[pd.Series] = None,
                            X_test: Optional[pd.DataFrame] = None,
                            y_test: Optional[pd.Series] = None) -> Dict[str, Dict]:
        """
        è®­ç»ƒå¤šä¸ªæ¨¡å‹å¹¶æ¯”è¾ƒæ€§èƒ½

        Args:
            model_names: æ¨¡å‹åç§°åˆ—è¡¨
            X_train: è®­ç»ƒç‰¹å¾
            y_train: è®­ç»ƒç›®æ ‡
            X_test: æµ‹è¯•ç‰¹å¾
            y_test: æµ‹è¯•ç›®æ ‡

        Returns:
            æ‰€æœ‰æ¨¡å‹çš„è®­ç»ƒå’Œè¯„ä¼°ç»“æœ
        """
        results = {}

        for model_name in model_names:
            print(f"\n{'='*50}")
            print(f"è®­ç»ƒå’Œè¯„ä¼°æ¨¡å‹: {model_name}")
            print(f"{'='*50}")

            try:
                # è®­ç»ƒæ¨¡å‹
                train_result = self.train_single_model(model_name, X_train, y_train)

                # è¯„ä¼°æ¨¡å‹
                test_result = self.evaluate_model(model_name, X_test, y_test)

                # åˆå¹¶ç»“æœ
                results[model_name] = {**train_result, **test_result}

            except Exception as e:
                print(f"âŒ æ¨¡å‹{model_name}è®­ç»ƒå¤±è´¥: {str(e)}")
                results[model_name] = {'error': str(e)}

        # ç¡®å®šæœ€ä½³æ¨¡å‹
        self._select_best_model(results)

        # æ‰“å°æ¨¡å‹æ¯”è¾ƒ
        self._print_model_comparison(results)

        return results

    def _select_best_model(self, results: Dict[str, Dict]):
        """é€‰æ‹©æœ€ä½³æ¨¡å‹"""
        valid_models = {name: result for name, result in results.items()
                       if 'error' not in result and 'r2_score' in result}

        if valid_models:
            best_model_name = max(valid_models.keys(),
                                key=lambda x: valid_models[x]['r2_score'])
            self.best_model_name = best_model_name
            print(f"\nğŸ† æœ€ä½³æ¨¡å‹: {best_model_name} (RÂ²: {valid_models[best_model_name]['r2_score']:.4f})")

    def _print_model_comparison(self, results: Dict[str, Dict]):
        """æ‰“å°æ¨¡å‹æ¯”è¾ƒç»“æœ"""
        print(f"\nğŸ“ˆ æ¨¡å‹æ€§èƒ½å¯¹æ¯”:")
        print(f"{'æ¨¡å‹åç§°':<15} {'è®­ç»ƒRÂ²':<10} {'æµ‹è¯•RÂ²':<10} {'MAE':<10} {'RMSE':<10} {'MAPE':<10}")
        print("-" * 70)

        for model_name, result in results.items():
            if 'error' in result:
                print(f"{model_name:<15} {'ERROR':<50}")
            else:
                train_score = result.get('train_score', 'N/A')
                test_r2 = result.get('r2_score', 'N/A')
                mae = result.get('mae', 'N/A')
                rmse = result.get('rmse', 'N/A')
                mape = result.get('mape', 'N/A')

                print(f"{model_name:<15} {train_score:<10.4f} {test_r2:<10.4f} {mae:<10.2f} {rmse:<10.2f} {mape:<10.2f}%")

    def predict(self,
               model_name: Optional[str] = None,
               X: Optional[pd.DataFrame] = None) -> np.ndarray:
        """
        è¿›è¡Œé¢„æµ‹

        Args:
            model_name: æ¨¡å‹åç§°ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨æœ€ä½³æ¨¡å‹
            X: é¢„æµ‹ç‰¹å¾

        Returns:
            é¢„æµ‹ç»“æœ
        """
        if model_name is None:
            model_name = self.best_model_name

        if model_name is None:
            raise ValueError("æ²¡æœ‰å¯ç”¨çš„æ¨¡å‹ï¼Œè¯·å…ˆè®­ç»ƒæ¨¡å‹")

        if model_name not in self.trained_models:
            raise ValueError(f"æ¨¡å‹{model_name}æœªè®­ç»ƒ")

        model = self.trained_models[model_name]

        if X is None:
            X = self.X_test

        predictions = model.predict(X)
        return predictions

    def save_models(self, directory: str):
        """
        ä¿å­˜è®­ç»ƒå¥½çš„æ¨¡å‹

        Args:
            directory: ä¿å­˜ç›®å½•
        """
        import os
        from pathlib import Path

        Path(directory).mkdir(parents=True, exist_ok=True)

        for model_name, model in self.trained_models.items():
            model_path = os.path.join(directory, f"{model_name}.joblib")
            joblib.dump(model, model_path)
            print(f"âœ“ æ¨¡å‹å·²ä¿å­˜: {model_path}")

        # ä¿å­˜å…ƒæ•°æ®
        metadata = {
            'config': self.config,
            'feature_names': self.feature_names,
            'model_performance': self.model_performance,
            'feature_importance': self.feature_importance,
            'best_model_name': self.best_model_name
        }

        metadata_path = os.path.join(directory, "metadata.joblib")
        joblib.dump(metadata, metadata_path)
        print(f"âœ“ å…ƒæ•°æ®å·²ä¿å­˜: {metadata_path}")

    def load_models(self, directory: str):
        """
        åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹

        Args:
            directory: æ¨¡å‹ç›®å½•
        """
        import os
        import glob

        # åŠ è½½æ¨¡å‹æ–‡ä»¶
        model_files = glob.glob(os.path.join(directory, "*.joblib"))

        for model_file in model_files:
            model_name = os.path.basename(model_file).replace('.joblib', '')

            if model_name == 'metadata':
                # åŠ è½½å…ƒæ•°æ®
                metadata = joblib.load(model_file)
                self.config = metadata.get('config', self.config)
                self.feature_names = metadata.get('feature_names')
                self.model_performance = metadata.get('model_performance', {})
                self.feature_importance = metadata.get('feature_importance', {})
                self.best_model_name = metadata.get('best_model_name')
            else:
                # åŠ è½½æ¨¡å‹
                model = joblib.load(model_file)
                self.trained_models[model_name] = model
                self.models[model_name] = model  # also add to models dict
                print(f"âœ“ æ¨¡å‹å·²åŠ è½½: {model_name}")

    def get_model_summary(self) -> Dict[str, Any]:
        """
        è·å–æ¨¡å‹æ‘˜è¦ä¿¡æ¯

        Returns:
            æ¨¡å‹æ‘˜è¦å­—å…¸
        """
        summary = {
            'total_models_trained': len(self.trained_models),
            'best_model': self.best_model_name,
            'feature_names': self.feature_names,
            'model_performance': self.model_performance,
            'feature_importance': self.feature_importance
        }

        if self.best_model_name and self.best_model_name in self.model_performance:
            best_performance = self.model_performance[self.best_model_name]
            summary['best_performance'] = best_performance

        return summary

# ä¾¿åˆ©å‡½æ•°
def quick_model_training(rfm_data: pd.DataFrame,
                        model_names: List[str] = ['linear_regression', 'random_forest'],
                        feature_columns: List[str] = ['Rå€¼', 'Få€¼', 'Må€¼'],
                        target_column: str = 'å¹´åº¦LTV',
                        test_size: float = 0.2) -> Dict[str, Any]:
    """
    å¿«é€Ÿæ¨¡å‹è®­ç»ƒ

    Args:
        rfm_data: RFMæ•°æ®
        model_names: è¦è®­ç»ƒçš„æ¨¡å‹åˆ—è¡¨
        feature_columns: ç‰¹å¾åˆ—
        target_column: ç›®æ ‡åˆ—
        test_size: æµ‹è¯•é›†æ¯”ä¾‹

    Returns:
        è®­ç»ƒç»“æœå­—å…¸
    """
    # åˆå§‹åŒ–å›å½’æ¨¡å‹
    regression = RegressionModels({
        'test_size': test_size,
        'feature_columns': feature_columns,
        'target_column': target_column
    })

    # å‡†å¤‡æ•°æ®
    X, y = regression.prepare_data(rfm_data, feature_columns, target_column)
    X_train, X_test, y_train, y_test = regression.split_data(X, y)

    # è®­ç»ƒå¤šä¸ªæ¨¡å‹
    results = regression.train_multiple_models(model_names, X_train, y_train, X_test, y_test)

    return {
        'regression': regression,
        'results': results,
        'X_train': X_train,
        'X_test': X_test,
        'y_train': y_train,
        'y_test': y_test
    }

if __name__ == "__main__":
    # ç¤ºä¾‹ä½¿ç”¨
    print("ğŸ¤– å›å½’ç®—æ³•æ¨¡å—æµ‹è¯•")

    # å¦‚æœæœ‰æ•°æ®ï¼Œå¯ä»¥è¿›è¡Œæµ‹è¯•
    # è¿™é‡Œåº”è¯¥æœ‰æµ‹è¯•æ•°æ®ï¼Œæš‚æ—¶è·³è¿‡
    print("æ¨¡å—åˆå§‹åŒ–å®Œæˆï¼Œç­‰å¾…æ•°æ®è¾“å…¥")