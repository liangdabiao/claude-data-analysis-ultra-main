#!/usr/bin/env python3
"""
é«˜çº§åˆ†ææ¨¡å—
æä¾›å®¢æˆ·è¡Œä¸ºåˆ†æã€æµå¤±é¢„æµ‹ã€èšç±»åˆ†æç­‰é«˜çº§åŠŸèƒ½
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import silhouette_score
from sklearn.ensemble import IsolationForest
from sklearn.decomposition import PCA
import warnings
warnings.filterwarnings('ignore')

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

class AdvancedAnalytics:
    """é«˜çº§åˆ†æç±»"""

    def __init__(self):
        self.scaler = StandardScaler()
        self.cluster_model = None
        self.anomaly_detector = None

    def customer_behavior_analysis(self, rfm_data, output_dir='./analysis_results'):
        """
        å®¢æˆ·è¡Œä¸ºæ·±åº¦åˆ†æ

        Args:
            rfm_data: RFMç‰¹å¾æ•°æ®
            output_dir: è¾“å‡ºç›®å½•

        Returns:
            åˆ†æç»“æœå­—å…¸
        """
        print("ğŸ” å¼€å§‹å®¢æˆ·è¡Œä¸ºåˆ†æ...")

        results = {}

        # 1. è´­ä¹°æ¨¡å¼åˆ†æ
        results['purchase_patterns'] = self._analyze_purchase_patterns(rfm_data)

        # 2. å®¢æˆ·ç”Ÿå‘½å‘¨æœŸé˜¶æ®µåˆ†æ
        results['lifecycle_stages'] = self._analyze_lifecycle_stages(rfm_data)

        # 3. ä»·å€¼åˆ†å¸ƒåˆ†æ
        results['value_distribution'] = self._analyze_value_distribution(rfm_data)

        # 4. æ´»è·ƒåº¦åˆ†æ
        results['activity_analysis'] = self._analyze_activity_patterns(rfm_data)

        # ç”Ÿæˆå¯è§†åŒ–
        self._plot_behavior_analysis(rfm_data, results, output_dir)

        print("âœ… å®¢æˆ·è¡Œä¸ºåˆ†æå®Œæˆ")
        return results

    def _analyze_purchase_patterns(self, rfm_data):
        """åˆ†æè´­ä¹°æ¨¡å¼"""
        print("  åˆ†æè´­ä¹°æ¨¡å¼...")

        patterns = {}

        # é¢‘ç‡åˆ†å¸ƒ
        freq_dist = rfm_data['Få€¼'].value_counts().sort_index()
        patterns['frequency_distribution'] = freq_dist.to_dict()

        # é‡‘é¢åˆ†å¸ƒ
        amount_stats = {
            'mean': rfm_data['Må€¼'].mean(),
            'median': rfm_data['Må€¼'].median(),
            'std': rfm_data['Må€¼'].std(),
            'min': rfm_data['Må€¼'].min(),
            'max': rfm_data['Må€¼'].max()
        }
        patterns['amount_statistics'] = amount_stats

        # æœ€è¿‘è´­ä¹°æ—¶é—´åˆ†å¸ƒ
        recency_stats = {
            'mean_days': rfm_data['Rå€¼'].mean(),
            'median_days': rfm_data['Rå€¼'].median(),
            'recent_customers': (rfm_data['Rå€¼'] <= 30).sum(),
            'inactive_customers': (rfm_data['Rå€¼'] > 90).sum()
        }
        patterns['recency_analysis'] = recency_stats

        return patterns

    def _analyze_lifecycle_stages(self, rfm_data):
        """åˆ†æå®¢æˆ·ç”Ÿå‘½å‘¨æœŸé˜¶æ®µ"""
        print("  åˆ†æç”Ÿå‘½å‘¨æœŸé˜¶æ®µ...")

        # å®šä¹‰ç”Ÿå‘½å‘¨æœŸé˜¶æ®µ
        def classify_lifecycle_stage(row):
            r, f, m = row['Rå€¼'], row['Få€¼'], row['Må€¼']

            if r <= 30 and f >= 3 and m >= m * 0.8:  # é«˜Rå€¼ã€é«˜Få€¼ã€é«˜Må€¼
                return "æˆç†ŸæœŸ"
            elif r <= 60 and f >= 2:
                return "æˆé•¿æœŸ"
            elif r <= 30 and f == 1:
                return "æ–°å®¢æˆ·æœŸ"
            elif r > 90 and f <= 2:
                return "è¡°é€€æœŸ"
            elif r > 180:
                return "æµå¤±æœŸ"
            else:
                return "ç¨³å®šæœŸ"

        rfm_data['ç”Ÿå‘½å‘¨æœŸé˜¶æ®µ'] = rfm_data.apply(classify_lifecycle_stage, axis=1)

        stage_counts = rfm_data['ç”Ÿå‘½å‘¨æœŸé˜¶æ®µ'].value_counts()
        stage_percentages = (stage_counts / len(rfm_data) * 100).round(2)

        return {
            'stage_counts': stage_counts.to_dict(),
            'stage_percentages': stage_percentages.to_dict(),
            'stage_distribution': rfm_data.groupby('ç”Ÿå‘½å‘¨æœŸé˜¶æ®µ')['å¹´åº¦LTV'].mean().to_dict()
        }

    def _analyze_value_distribution(self, rfm_data):
        """åˆ†æä»·å€¼åˆ†å¸ƒ"""
        print("  åˆ†æä»·å€¼åˆ†å¸ƒ...")

        ltv_stats = {
            'mean': rfm_data['å¹´åº¦LTV'].mean(),
            'median': rfm_data['å¹´åº¦LTV'].median(),
            'std': rfm_data['å¹´åº¦LTV'].std(),
            'percentiles': {
                '25%': rfm_data['å¹´åº¦LTV'].quantile(0.25),
                '50%': rfm_data['å¹´åº¦LTV'].quantile(0.50),
                '75%': rfm_data['å¹´åº¦LTV'].quantile(0.75),
                '90%': rfm_data['å¹´åº¦LTV'].quantile(0.90),
                '95%': rfm_data['å¹´åº¦LTV'].quantile(0.95)
            }
        }

        # å¸•ç´¯æ‰˜åˆ†æï¼ˆ80/20æ³•åˆ™ï¼‰
        sorted_ltv = rfm_data['å¹´åº¦LTV'].sort_values(ascending=False)
        total_ltv = sorted_ltv.sum()
        cumulative_ltv = sorted_ltv.cumsum() / total_ltv

        # æ‰¾åˆ°è´¡çŒ®80%ä»·å€¼çš„å®¢æˆ·æ¯”ä¾‹
        eighty_percent_idx = (cumulative_ltv >= 0.8).idxmax()
        top_customer_percentage = (eighty_percent_idx + 1) / len(rfm_data) * 100

        ltv_stats['pareto_analysis'] = {
            'top_20_percent_customers_contribute': cumulative_ltv.iloc[int(len(rfm_data) * 0.2) - 1] * 100,
            'customers_for_80_percent_value': top_customer_percentage
        }

        return ltv_stats

    def _analyze_activity_patterns(self, rfm_data):
        """åˆ†ææ´»è·ƒåº¦æ¨¡å¼"""
        print("  åˆ†ææ´»è·ƒåº¦æ¨¡å¼...")

        # æ´»è·ƒåº¦åˆ†ç±»
        def classify_activity(row):
            r, f = row['Rå€¼'], row['Få€¼']

            if r <= 30 and f >= 3:
                return "é«˜æ´»è·ƒ"
            elif r <= 60 and f >= 2:
                return "ä¸­ç­‰æ´»è·ƒ"
            elif r <= 90 and f >= 1:
                return "ä½æ´»è·ƒ"
            else:
                return "éæ´»è·ƒ"

        rfm_data['æ´»è·ƒåº¦'] = rfm_data.apply(classify_activity, axis=1)

        activity_counts = rfm_data['æ´»è·ƒåº¦'].value_counts()
        activity_stats = rfm_data.groupby('æ´»è·ƒåº¦')['å¹´åº¦LTV'].agg(['mean', 'count'])

        return {
            'activity_distribution': activity_counts.to_dict(),
            'activity_value_stats': activity_stats.to_dict()
        }

    def _plot_behavior_analysis(self, rfm_data, results, output_dir):
        """ç»˜åˆ¶è¡Œä¸ºåˆ†æå›¾è¡¨"""
        import os
        os.makedirs(output_dir, exist_ok=True)

        print("  ç”Ÿæˆè¡Œä¸ºåˆ†æå›¾è¡¨...")

        # 1. RFMåˆ†å¸ƒå›¾
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))

        # Rå€¼åˆ†å¸ƒ
        axes[0, 0].hist(rfm_data['Rå€¼'], bins=20, alpha=0.7, color='skyblue')
        axes[0, 0].set_title('æœ€è¿‘è´­ä¹°æ—¶é—´åˆ†å¸ƒ (Rå€¼)')
        axes[0, 0].set_xlabel('è·ç¦»ä¸Šæ¬¡è´­ä¹°å¤©æ•°')
        axes[0, 0].set_ylabel('å®¢æˆ·æ•°')

        # Få€¼åˆ†å¸ƒ
        axes[0, 1].hist(rfm_data['Få€¼'], bins=10, alpha=0.7, color='lightgreen')
        axes[0, 1].set_title('è´­ä¹°é¢‘ç‡åˆ†å¸ƒ (Få€¼)')
        axes[0, 1].set_xlabel('è´­ä¹°æ¬¡æ•°')
        axes[0, 1].set_ylabel('å®¢æˆ·æ•°')

        # Må€¼åˆ†å¸ƒ
        axes[1, 0].hist(rfm_data['Må€¼'], bins=15, alpha=0.7, color='salmon')
        axes[1, 0].set_title('æ¶ˆè´¹é‡‘é¢åˆ†å¸ƒ (Må€¼)')
        axes[1, 0].set_xlabel('æ€»æ¶ˆè´¹é‡‘é¢')
        axes[1, 0].set_ylabel('å®¢æˆ·æ•°')

        # LTVåˆ†å¸ƒ
        axes[1, 1].hist(rfm_data['å¹´åº¦LTV'], bins=15, alpha=0.7, color='gold')
        axes[1, 1].set_title('å¹´åº¦LTVåˆ†å¸ƒ')
        axes[1, 1].set_xlabel('å¹´åº¦LTV')
        axes[1, 1].set_ylabel('å®¢æˆ·æ•°')

        plt.tight_layout()
        plt.savefig(f'{output_dir}/behavior_distributions.png', dpi=300, bbox_inches='tight')
        plt.close()

        # 2. ç”Ÿå‘½å‘¨æœŸé˜¶æ®µåˆ†å¸ƒ
        if 'ç”Ÿå‘½å‘¨æœŸé˜¶æ®µ' in rfm_data.columns:
            plt.figure(figsize=(10, 6))
            stage_counts = rfm_data['ç”Ÿå‘½å‘¨æœŸé˜¶æ®µ'].value_counts()
            plt.pie(stage_counts.values, labels=stage_counts.index, autopct='%1.1f%%')
            plt.title('å®¢æˆ·ç”Ÿå‘½å‘¨æœŸé˜¶æ®µåˆ†å¸ƒ')
            plt.savefig(f'{output_dir}/lifecycle_stages.png', dpi=300, bbox_inches='tight')
            plt.close()

        # 3. æ´»è·ƒåº¦ vs LTV
        if 'æ´»è·ƒåº¦' in rfm_data.columns:
            plt.figure(figsize=(10, 6))
            sns.boxplot(data=rfm_data, x='æ´»è·ƒåº¦', y='å¹´åº¦LTV')
            plt.title('ä¸åŒæ´»è·ƒåº¦å®¢æˆ·çš„LTVåˆ†å¸ƒ')
            plt.ylabel('å¹´åº¦LTV')
            plt.savefig(f'{output_dir}/activity_vs_ltv.png', dpi=300, bbox_inches='tight')
            plt.close()

    def advanced_customer_segmentation(self, rfm_data, n_clusters=5, output_dir='./analysis_results'):
        """
        é«˜çº§å®¢æˆ·ç»†åˆ†ï¼ˆåŸºäºèšç±»ç®—æ³•ï¼‰

        Args:
            rfm_data: RFMç‰¹å¾æ•°æ®
            n_clusters: èšç±»æ•°é‡
            output_dir: è¾“å‡ºç›®å½•

        Returns:
            èšç±»ç»“æœ
        """
        print("ğŸ¯ å¼€å§‹é«˜çº§å®¢æˆ·ç»†åˆ†...")

        # å‡†å¤‡ç‰¹å¾æ•°æ®
        features = ['Rå€¼', 'Få€¼', 'Må€¼']
        X = rfm_data[features].copy()

        # æ•°æ®æ ‡å‡†åŒ–
        X_scaled = self.scaler.fit_transform(X)

        # å¯»æ‰¾æœ€ä½³èšç±»æ•°é‡
        best_k = self._find_optimal_clusters(X_scaled, max_k=10)

        print(f"  æœ€ä½³èšç±»æ•°é‡: {best_k}")

        # æ‰§è¡ŒK-meansèšç±»
        self.cluster_model = KMeans(n_clusters=best_k, random_state=42)
        cluster_labels = self.cluster_model.fit_predict(X_scaled)

        # æ·»åŠ èšç±»æ ‡ç­¾åˆ°æ•°æ®
        rfm_data_copy = rfm_data.copy()
        rfm_data_copy['èšç±»æ ‡ç­¾'] = cluster_labels

        # åˆ†æèšç±»ç»“æœ
        cluster_analysis = self._analyze_clusters(rfm_data_copy, features)

        # ç”Ÿæˆå¯è§†åŒ–
        self._plot_clustering_results(rfm_data_copy, features, output_dir)

        print(f"âœ… å®¢æˆ·ç»†åˆ†å®Œæˆï¼Œå…±è¯†åˆ«{best_k}ä¸ªå®¢æˆ·ç¾¤ä½“")
        return {
            'cluster_labels': cluster_labels,
            'cluster_analysis': cluster_analysis,
            'optimal_clusters': best_k,
            'data_with_clusters': rfm_data_copy
        }

    def _find_optimal_clusters(self, X_scaled, max_k=10):
        """ä½¿ç”¨è‚˜éƒ¨æ³•åˆ™å’Œè½®å»“ç³»æ•°æ‰¾æœ€ä½³èšç±»æ•°é‡"""
        print("  å¯»æ‰¾æœ€ä½³èšç±»æ•°é‡...")

        inertias = []
        silhouette_scores = []

        for k in range(2, max_k + 1):
            kmeans = KMeans(n_clusters=k, random_state=42)
            labels = kmeans.fit_predict(X_scaled)

            inertias.append(kmeans.inertia_)
            silhouette_scores.append(silhouette_score(X_scaled, labels))

        # ç»¼åˆè€ƒè™‘è‚˜éƒ¨æ³•åˆ™å’Œè½®å»“ç³»æ•°
        # é€‰æ‹©è½®å»“ç³»æ•°æœ€é«˜çš„kå€¼
        best_k = np.argmax(silhouette_scores) + 2

        return best_k

    def _analyze_clusters(self, data, features):
        """åˆ†æèšç±»ç‰¹å¾"""
        print("  åˆ†æèšç±»ç‰¹å¾...")

        cluster_analysis = {}

        for cluster_id in sorted(data['èšç±»æ ‡ç­¾'].unique()):
            cluster_data = data[data['èšç±»æ ‡ç­¾'] == cluster_id]

            analysis = {
                'size': len(cluster_data),
                'percentage': len(cluster_data) / len(data) * 100,
                'feature_means': {},
                'ltv_stats': {
                    'mean': cluster_data['å¹´åº¦LTV'].mean(),
                    'median': cluster_data['å¹´åº¦LTV'].median(),
                    'std': cluster_data['å¹´åº¦LTV'].std()
                }
            }

            for feature in features:
                analysis['feature_means'][feature] = cluster_data[feature].mean()

            # ä¸ºèšç±»å‘½å
            r_mean = analysis['feature_means']['Rå€¼']
            f_mean = analysis['feature_means']['Få€¼']
            m_mean = analysis['feature_means']['Må€¼']

            if r_mean <= 30 and f_mean >= 3 and m_mean >= 500:
                cluster_name = "é«˜ä»·å€¼æ´»è·ƒå®¢æˆ·"
            elif r_mean <= 60 and f_mean >= 2:
                cluster_name = "ä¸­ä»·å€¼æ½œåŠ›å®¢æˆ·"
            elif r_mean > 90 and f_mean <= 2:
                cluster_name = "ä½ä»·å€¼æµå¤±é£é™©å®¢æˆ·"
            elif r_mean <= 30 and f_mean == 1:
                cluster_name = "æ–°å®¢æˆ·"
            else:
                cluster_name = f"å®¢æˆ·ç¾¤ä½“{cluster_id + 1}"

            analysis['cluster_name'] = cluster_name
            cluster_analysis[cluster_id] = analysis

        return cluster_analysis

    def _plot_clustering_results(self, data, features, output_dir):
        """ç»˜åˆ¶èšç±»ç»“æœ"""
        import os
        os.makedirs(output_dir, exist_ok=True)

        print("  ç”Ÿæˆèšç±»å¯è§†åŒ–...")

        # 1. èšç±»æ•£ç‚¹å›¾ (ä½¿ç”¨PCAé™ç»´)
        X = data[features]
        X_scaled = self.scaler.transform(X)

        pca = PCA(n_components=2)
        X_pca = pca.fit_transform(X_scaled)

        plt.figure(figsize=(12, 8))
        scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=data['èšç±»æ ‡ç­¾'],
                            cmap='viridis', alpha=0.6, s=100)
        plt.colorbar(scatter)
        plt.xlabel(f'ä¸»æˆåˆ†1 (è§£é‡Šæ–¹å·®: {pca.explained_variance_ratio_[0]:.2%})')
        plt.ylabel(f'ä¸»æˆåˆ†2 (è§£é‡Šæ–¹å·®: {pca.explained_variance_ratio_[1]:.2%})')
        plt.title('å®¢æˆ·èšç±»ç»“æœ (PCAé™ç»´)')
        plt.savefig(f'{output_dir}/customer_clusters_pca.png', dpi=300, bbox_inches='tight')
        plt.close()

        # 2. èšç±»ç‰¹å¾é›·è¾¾å›¾
        self._plot_cluster_radar_chart(data, features, output_dir)

        # 3. èšç±»å¤§å°åˆ†å¸ƒ
        plt.figure(figsize=(10, 6))
        cluster_counts = data['èšç±»æ ‡ç­¾'].value_counts().sort_index()
        plt.bar(range(len(cluster_counts)), cluster_counts.values)
        plt.xlabel('èšç±»æ ‡ç­¾')
        plt.ylabel('å®¢æˆ·æ•°')
        plt.title('å„èšç±»å®¢æˆ·æ•°é‡åˆ†å¸ƒ')
        plt.xticks(range(len(cluster_counts)), [f'èšç±»{i}' for i in cluster_counts.index])
        plt.savefig(f'{output_dir}/cluster_sizes.png', dpi=300, bbox_inches='tight')
        plt.close()

    def _plot_cluster_radar_chart(self, data, features, output_dir):
        """ç»˜åˆ¶èšç±»ç‰¹å¾é›·è¾¾å›¾"""
        from math import pi

        # è®¡ç®—å„èšç±»çš„å¹³å‡ç‰¹å¾å€¼
        cluster_means = data.groupby('èšç±»æ ‡ç­¾')[features].mean()

        # å½’ä¸€åŒ–åˆ°0-1èŒƒå›´
        normalized_means = (cluster_means - cluster_means.min()) / (cluster_means.max() - cluster_means.min())

        # é›·è¾¾å›¾è®¾ç½®
        angles = [n / float(len(features)) * 2 * pi for n in range(len(features))]
        angles += angles[:1]  # é—­åˆå›¾å½¢

        plt.figure(figsize=(10, 10))
        ax = plt.subplot(111, polar=True)

        # ä¸ºæ¯ä¸ªèšç±»ç»˜åˆ¶é›·è¾¾å›¾
        colors = plt.cm.Set3(np.linspace(0, 1, len(cluster_means)))

        for i, (cluster_id, row) in enumerate(normalized_means.iterrows()):
            values = row.values.tolist()
            values += values[:1]  # é—­åˆå›¾å½¢

            ax.plot(angles, values, 'o-', linewidth=2, label=f'èšç±»{cluster_id}', color=colors[i])
            ax.fill(angles, values, alpha=0.25, color=colors[i])

        # è®¾ç½®æ ‡ç­¾
        plt.xticks(angles[:-1], features)
        plt.yticks([0.2, 0.4, 0.6, 0.8, 1.0], ['0.2', '0.4', '0.6', '0.8', '1.0'])
        plt.title('å®¢æˆ·èšç±»ç‰¹å¾é›·è¾¾å›¾', size=16, y=1.08)
        plt.legend(loc='upper right', bbox_to_anchor=(1.3, 1.0))
        plt.savefig(f'{output_dir}/cluster_radar_chart.png', dpi=300, bbox_inches='tight')
        plt.close()

    def churn_prediction(self, rfm_data, output_dir='./analysis_results'):
        """
        å®¢æˆ·æµå¤±é¢„æµ‹åˆ†æ

        Args:
            rfm_data: RFMç‰¹å¾æ•°æ®
            output_dir: è¾“å‡ºç›®å½•

        Returns:
            æµå¤±é¢„æµ‹ç»“æœ
        """
        print("âš ï¸ å¼€å§‹æµå¤±é¢„æµ‹åˆ†æ...")

        # å®šä¹‰æµå¤±æ ‡ç­¾
        def define_churn(row):
            # åŸºäºRå€¼å®šä¹‰æµå¤±ï¼š90å¤©æœªè´­ä¹°ä¸ºæµå¤±
            return 1 if row['Rå€¼'] > 90 else 0

        rfm_data_copy = rfm_data.copy()
        rfm_data_copy['æµå¤±æ ‡ç­¾'] = rfm_data_copy.apply(define_churn, axis=1)

        # æµå¤±ç»Ÿè®¡åˆ†æ
        churn_rate = rfm_data_copy['æµå¤±æ ‡ç­¾'].mean()
        print(f"  å½“å‰æµå¤±ç‡: {churn_rate:.2%}")

        # æµå¤±ç‰¹å¾åˆ†æ
        churn_features = self._analyze_churn_features(rfm_data_copy)

        # é«˜é£é™©æµå¤±å®¢æˆ·è¯†åˆ«
        high_risk_customers = self._identify_high_risk_customers(rfm_data_copy)

        # ç”Ÿæˆå¯è§†åŒ–
        self._plot_churn_analysis(rfm_data_copy, output_dir)

        print("âœ… æµå¤±é¢„æµ‹åˆ†æå®Œæˆ")
        return {
            'churn_rate': churn_rate,
            'churn_features': churn_features,
            'high_risk_customers': high_risk_customers,
            'data_with_churn_labels': rfm_data_copy
        }

    def _analyze_churn_features(self, data):
        """åˆ†ææµå¤±ç›¸å…³ç‰¹å¾"""
        churn_group = data.groupby('æµå¤±æ ‡ç­¾')[['Rå€¼', 'Få€¼', 'Må€¼']].mean()
        return churn_group.to_dict()

    def _identify_high_risk_customers(self, data, risk_threshold=0.7):
        """è¯†åˆ«é«˜é£é™©æµå¤±å®¢æˆ·"""
        # åŸºäºRå€¼å’ŒFå€¼è¯†åˆ«é«˜é£é™©å®¢æˆ·
        high_risk = data[
            (data['Rå€¼'] > 60) |  # 60å¤©æœªè´­ä¹°
            (data['Få€¼'] <= 1)     # åªè´­ä¹°è¿‡ä¸€æ¬¡
        ].sort_values('Rå€¼', ascending=False)

        return high_risk.head(20)  # è¿”å›å‰20ä¸ªé«˜é£é™©å®¢æˆ·

    def _plot_churn_analysis(self, data, output_dir):
        """ç»˜åˆ¶æµå¤±åˆ†æå›¾è¡¨"""
        import os
        os.makedirs(output_dir, exist_ok=True)

        # 1. æµå¤±vséæµå¤±å®¢æˆ·ç‰¹å¾å¯¹æ¯”
        fig, axes = plt.subplots(1, 3, figsize=(15, 5))

        features = ['Rå€¼', 'Få€¼', 'Må€¼']
        feature_names = ['æœ€è¿‘è´­ä¹°', 'è´­ä¹°é¢‘ç‡', 'æ¶ˆè´¹é‡‘é¢']

        for i, (feature, name) in enumerate(zip(features, feature_names)):
            churn_0 = data[data['æµå¤±æ ‡ç­¾'] == 0][feature]
            churn_1 = data[data['æµå¤±æ ‡ç­¾'] == 1][feature]

            axes[i].hist([churn_0, churn_1], bins=20, alpha=0.7,
                        label=['éæµå¤±', 'æµå¤±'], color=['green', 'red'])
            axes[i].set_title(f'{name}åˆ†å¸ƒ')
            axes[i].set_xlabel(name)
            axes[i].set_ylabel('å®¢æˆ·æ•°')
            axes[i].legend()

        plt.tight_layout()
        plt.savefig(f'{output_dir}/churn_feature_comparison.png', dpi=300, bbox_inches='tight')
        plt.close()

        # 2. æµå¤±é£é™©æ•£ç‚¹å›¾
        plt.figure(figsize=(10, 6))
        scatter = plt.scatter(data['Rå€¼'], data['Få€¼'], c=data['æµå¤±æ ‡ç­¾'],
                            cmap='RdYlGn', alpha=0.6, s=100)
        plt.colorbar(scatter)
        plt.xlabel('æœ€è¿‘è´­ä¹°å¤©æ•° (Rå€¼)')
        plt.ylabel('è´­ä¹°é¢‘ç‡ (Få€¼)')
        plt.title('å®¢æˆ·æµå¤±é£é™©åˆ†å¸ƒ')
        plt.savefig(f'{output_dir}/churn_risk_scatter.png', dpi=300, bbox_inches='tight')
        plt.close()

    def generate_comprehensive_insights(self, rfm_data, behavior_results,
                                      segmentation_results, churn_results,
                                      output_path='comprehensive_insights.md'):
        """ç”Ÿæˆç»¼åˆæ´å¯ŸæŠ¥å‘Š"""
        print("ğŸ“‹ ç”Ÿæˆç»¼åˆæ´å¯ŸæŠ¥å‘Š...")

        report = "# å®¢æˆ·ç”Ÿå‘½å‘¨æœŸä»·å€¼ç»¼åˆæ´å¯ŸæŠ¥å‘Š\n\n"
        report += f"ç”Ÿæˆæ—¶é—´: {pd.Timestamp.now()}\n\n"

        # å®¢æˆ·æ¦‚è§ˆ
        report += "## å®¢æˆ·æ¦‚è§ˆ\n\n"
        report += f"- æ€»å®¢æˆ·æ•°: {len(rfm_data):,}\n"
        report += f"- å¹³å‡å¹´åº¦LTV: {rfm_data['å¹´åº¦LTV'].mean():,.0f}\n"
        report += f"- LTVä¸­ä½æ•°: {rfm_data['å¹´åº¦LTV'].median():,.0f}\n"
        report += f"- å¹³å‡è´­ä¹°é¢‘ç‡: {rfm_data['Få€¼'].mean():.1f}æ¬¡\n"
        report += f"- å¹³å‡æœ€è¿‘è´­ä¹°: {rfm_data['Rå€¼'].mean():.0f}å¤©å‰\n\n"

        # è¡Œä¸ºåˆ†ææ´å¯Ÿ
        if behavior_results:
            report += "## å®¢æˆ·è¡Œä¸ºæ´å¯Ÿ\n\n"

            # ç”Ÿå‘½å‘¨æœŸé˜¶æ®µ
            if 'lifecycle_stages' in behavior_results:
                stages = behavior_results['lifecycle_stages']
                report += "### ç”Ÿå‘½å‘¨æœŸé˜¶æ®µåˆ†å¸ƒ\n\n"
                for stage, percentage in stages['stage_percentages'].items():
                    report += f"- {stage}: {percentage}%\n"
                report += "\n"

        # ç»†åˆ†åˆ†ææ´å¯Ÿ
        if segmentation_results:
            report += "## å®¢æˆ·ç»†åˆ†æ´å¯Ÿ\n\n"
            clusters = segmentation_results['cluster_analysis']

            for cluster_id, analysis in clusters.items():
                report += f"### {analysis['cluster_name']}\n\n"
                report += f"- å®¢æˆ·æ•°é‡: {analysis['size']} ({analysis['percentage']:.1f}%)\n"
                report += f"- å¹³å‡LTV: {analysis['ltv_stats']['mean']:,.0f}\n"
                report += f"- ç‰¹å¾ç‰¹å¾: Rå€¼{analysis['feature_means']['Rå€¼']:.1f}, "
                report += f"Få€¼{analysis['feature_means']['Få€¼']:.1f}, "
                report += f"Må€¼{analysis['feature_means']['Må€¼']:.0f}\n\n"

        # æµå¤±åˆ†ææ´å¯Ÿ
        if churn_results:
            report += "## æµå¤±é£é™©æ´å¯Ÿ\n\n"
            report += f"å½“å‰æµå¤±ç‡: {churn_results['churn_rate']:.1%}\n\n"

            high_risk = churn_results['high_risk_customers']
            if not high_risk.empty:
                report += f"é«˜é£é™©å®¢æˆ·æ•°: {len(high_risk)}\n"
                report += "ä¸»è¦é£é™©å› ç´ :\n"
                report += "- é•¿æ—¶é—´æœªè´­ä¹° (Rå€¼ > 60å¤©)\n"
                report += "- è´­ä¹°é¢‘ç‡ä½ (Få€¼ â‰¤ 1)\n\n"

        # ç­–ç•¥å»ºè®®
        report += "## è¥é”€ç­–ç•¥å»ºè®®\n\n"
        report += "åŸºäºä¸Šè¿°åˆ†æï¼Œå»ºè®®é‡‡å–ä»¥ä¸‹ç­–ç•¥:\n\n"

        if segmentation_results:
            # åŸºäºèšç±»ç»“æœçš„å»ºè®®
            clusters = segmentation_results['cluster_analysis']
            for cluster_id, analysis in clusters.items():
                cluster_name = analysis['cluster_name']

                if 'é«˜ä»·å€¼' in cluster_name:
                    report += f"### {cluster_name}\n"
                    report += "- æä¾›VIPä¸“å±æœåŠ¡\n"
                    report += "- å®‰æ’å®¢æˆ·ç»ç†ä¸“äººå¯¹æ¥\n"
                    report += "- å®šåˆ¶ä¸ªæ€§åŒ–è¥é”€æ–¹æ¡ˆ\n\n"
                elif 'æ½œåŠ›' in cluster_name:
                    report += f"### {cluster_name}\n"
                    report += "- æ¨èå‡çº§äº§å“å’Œå¥—é¤\n"
                    report += "- æä¾›ä¼šå‘˜æ¿€åŠ±è®¡åˆ’\n"
                    report += "- å¢åŠ äº’åŠ¨é¢‘ç‡\n\n"
                elif 'æµå¤±é£é™©' in cluster_name:
                    report += f"### {cluster_name}\n"
                    report += "- å‘æ”¾å¬å›ä¼˜æƒ åˆ¸\n"
                    report += "- æ¨èæ€§ä»·æ¯”é«˜çš„äº§å“\n"
                    report += "- ä¸»åŠ¨å…³æ€€å’Œæ²Ÿé€š\n\n"

        if churn_results and churn_results['churn_rate'] > 0.2:
            report += "### æµå¤±é¢„é˜²ç­–ç•¥\n"
            report += "- å»ºç«‹æµå¤±é¢„è­¦æœºåˆ¶\n"
            report += "- å®æ–½å®¢æˆ·å…³æ€€è®¡åˆ’\n"
            report += "- ä¼˜åŒ–äº§å“å’ŒæœåŠ¡ä½“éªŒ\n\n"

        # ä¿å­˜æŠ¥å‘Š
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(report)

        print(f"âœ… ç»¼åˆæ´å¯ŸæŠ¥å‘Šå·²ä¿å­˜: {output_path}")
        return output_path