#!/usr/bin/env python3
"""
æ•°æ®é¢„å¤„ç†å™¨ (Data Preprocessor) - æ™ºèƒ½æ•°æ®æ¸…æ´—å’Œè½¬æ¢æ¨¡å—

æä¾›å…¨é¢çš„æ•°æ®é¢„å¤„ç†åŠŸèƒ½ï¼ŒåŒ…æ‹¬ï¼š
- æ•°æ®æ¸…æ´—ï¼ˆç¼ºå¤±å€¼å¤„ç†ã€å¼‚å¸¸å€¼æ£€æµ‹å’Œå¤„ç†ï¼‰
- æ•°æ®ç±»å‹è½¬æ¢å’Œæ ‡å‡†åŒ–
- ç‰¹å¾å·¥ç¨‹ï¼ˆç‰¹å¾é€‰æ‹©ã€åˆ›å»ºã€è½¬æ¢ï¼‰
- æ•°æ®ç¼–ç ï¼ˆæ ‡ç­¾ç¼–ç ã€ç‹¬çƒ­ç¼–ç ï¼‰
- æ•°æ®åˆ†å‰²å’Œå¹³è¡¡
- æ•°æ®è´¨é‡è¯„ä¼°å’Œæ”¹è¿›å»ºè®®
"""

import pandas as pd
import numpy as np
from typing import Dict, List, Tuple, Optional, Union, Any
from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler, LabelEncoder, OneHotEncoder
from sklearn.impute import SimpleImputer, KNNImputer
from sklearn.feature_selection import SelectKBest, f_classif, f_regression, RFE
from sklearn.ensemble import IsolationForest
from sklearn.model_selection import train_test_split
from imblearn.over_sampling import SMOTE, RandomOverSampler
from imblearn.under_sampling import RandomUnderSampler
import warnings
from pathlib import Path
import json

warnings.filterwarnings('ignore')


class DataPreprocessor:
    """æ•°æ®é¢„å¤„ç†å™¨ - æ™ºèƒ½æ•°æ®æ¸…æ´—å’Œç‰¹å¾å·¥ç¨‹å¼•æ“"""

    def __init__(self, config: Optional[Dict] = None):
        """
        åˆå§‹åŒ–æ•°æ®é¢„å¤„ç†å™¨

        Parameters:
        - config: é…ç½®å‚æ•°å­—å…¸
        """
        self.config = config or {}
        self.preprocessing_steps = []
        self.scalers = {}
        self.encoders = {}
        self.imputers = {}
        self.feature_selectors = {}
        self.preprocessing_report = {}

        # é»˜è®¤é…ç½®
        self.default_config = {
            'missing_threshold': 0.5,  # ç¼ºå¤±å€¼é˜ˆå€¼
            'outlier_method': 'isolation_forest',  # å¼‚å¸¸å€¼æ£€æµ‹æ–¹æ³•
            'outlier_contamination': 0.1,  # å¼‚å¸¸å€¼æ¯”ä¾‹
            'scaling_method': 'standard',  # æ ‡å‡†åŒ–æ–¹æ³•
            'encoding_method': 'auto',  # ç¼–ç æ–¹æ³•
            'feature_selection': False,  # æ˜¯å¦è¿›è¡Œç‰¹å¾é€‰æ‹©
            'k_features': 10,  # é€‰æ‹©çš„ç‰¹å¾æ•°é‡
            'test_size': 0.2,  # æµ‹è¯•é›†æ¯”ä¾‹
            'random_state': 42,  # éšæœºç§å­
            'balance_data': False,  # æ˜¯å¦å¹³è¡¡æ•°æ®
            'balance_method': 'smote'  # æ•°æ®å¹³è¡¡æ–¹æ³•
        }

        # åˆå¹¶é…ç½®
        self.config = {**self.default_config, **self.config}

    def analyze_data_quality(self, data: pd.DataFrame) -> Dict:
        """
        åˆ†ææ•°æ®è´¨é‡

        Parameters:
        - data: æ•°æ®DataFrame

        Returns:
        - æ•°æ®è´¨é‡æŠ¥å‘Š
        """
        print("ğŸ” åˆ†ææ•°æ®è´¨é‡...")

        quality_report = {
            'shape': data.shape,
            'memory_usage': data.memory_usage(deep=True).sum() / 1024**2,  # MB
            'columns': {},
            'overall_score': 0,
            'issues': [],
            'recommendations': []
        }

        total_issues = 0
        total_checks = 0

        for col in data.columns:
            col_info = {
                'dtype': str(data[col].dtype),
                'non_null_count': data[col].count(),
                'null_count': data[col].isnull().sum(),
                'null_percentage': data[col].isnull().sum() / len(data) * 100,
                'unique_count': data[col].nunique(),
                'duplicate_count': data[col].duplicated().sum(),
                'issues': []
            }

            # æ£€æŸ¥ç¼ºå¤±å€¼
            total_checks += 1
            if col_info['null_percentage'] > 0:
                total_issues += 1
                col_info['issues'].append(f"ç¼ºå¤±å€¼: {col_info['null_percentage']:.1f}%")

                if col_info['null_percentage'] > self.config['missing_threshold'] * 100:
                    quality_report['issues'].append(
                        f"åˆ— '{col}' ç¼ºå¤±å€¼è¿‡é«˜ ({col_info['null_percentage']:.1f}%)"
                    )

            # æ£€æŸ¥é‡å¤å€¼
            if col_info['duplicate_count'] > 0:
                col_info['issues'].append(f"é‡å¤å€¼: {col_info['duplicate_count']}")

            # æ£€æŸ¥æ•°æ®ç±»å‹
            if data[col].dtype == 'object':
                # æ£€æŸ¥å¯èƒ½çš„æ•°å€¼å‹åˆ†ç±»å˜é‡
                try:
                    pd.to_numeric(data[col], errors='raise')
                    col_info['issues'].append("å¯èƒ½æ˜¯æ•°å€¼å‹ä½†å­˜å‚¨ä¸ºå­—ç¬¦ä¸²")
                    quality_report['recommendations'].append(
                        f"è€ƒè™‘å°†åˆ— '{col}' è½¬æ¢ä¸ºæ•°å€¼ç±»å‹"
                    )
                except:
                    pass
            elif pd.api.types.is_numeric_dtype(data[col]):
                # æ£€æŸ¥å¼‚å¸¸å€¼
                Q1 = data[col].quantile(0.25)
                Q3 = data[col].quantile(0.75)
                IQR = Q3 - Q1
                lower_bound = Q1 - 1.5 * IQR
                upper_bound = Q3 + 1.5 * IQR
                outliers = ((data[col] < lower_bound) | (data[col] > upper_bound)).sum()

                if outliers > 0:
                    outlier_percentage = outliers / len(data) * 100
                    col_info['issues'].append(f"å¼‚å¸¸å€¼: {outliers} ({outlier_percentage:.1f}%)")

            quality_report['columns'][col] = col_info

        # è®¡ç®—æ•´ä½“è´¨é‡åˆ†æ•°
        quality_report['overall_score'] = max(0, 100 - (total_issues / total_checks) * 100)

        # ç”Ÿæˆå»ºè®®
        if quality_report['overall_score'] < 80:
            quality_report['recommendations'].append("æ•°æ®è´¨é‡è¾ƒä½ï¼Œå»ºè®®è¿›è¡Œæ•°æ®æ¸…æ´—")

        print(f"   âœ“ æ•°æ®è´¨é‡åˆ†æå®Œæˆï¼Œè´¨é‡åˆ†æ•°: {quality_report['overall_score']:.1f}")
        return quality_report

    def clean_data(self, data: pd.DataFrame,
                   handle_missing: str = 'auto',
                   handle_outliers: str = 'auto',
                   handle_duplicates: bool = True) -> pd.DataFrame:
        """
        æ•°æ®æ¸…æ´—

        Parameters:
        - data: åŸå§‹æ•°æ®
        - handle_missing: ç¼ºå¤±å€¼å¤„ç†æ–¹æ³•
        - handle_outliers: å¼‚å¸¸å€¼å¤„ç†æ–¹æ³•
        - handle_duplicates: æ˜¯å¦å¤„ç†é‡å¤å€¼

        Returns:
        - æ¸…æ´—åçš„æ•°æ®
        """
        print("ğŸ§¹ å¼€å§‹æ•°æ®æ¸…æ´—...")
        cleaned_data = data.copy()
        original_shape = cleaned_data.shape

        # 1. å¤„ç†é‡å¤å€¼
        if handle_duplicates:
            before_count = len(cleaned_data)
            cleaned_data = cleaned_data.drop_duplicates()
            removed_duplicates = before_count - len(cleaned_data)
            if removed_duplicates > 0:
                print(f"   âœ“ ç§»é™¤äº† {removed_duplicates} ä¸ªé‡å¤è¡Œ")
                self.preprocessing_steps.append(f"ç§»é™¤é‡å¤å€¼: {removed_duplicates} è¡Œ")

        # 2. å¤„ç†ç¼ºå¤±å€¼
        if handle_missing != 'none':
            cleaned_data = self._handle_missing_values(cleaned_data, handle_missing)

        # 3. å¤„ç†å¼‚å¸¸å€¼
        if handle_outliers != 'none':
            cleaned_data = self._handle_outliers(cleaned_data, handle_outliers)

        final_shape = cleaned_data.shape
        print(f"   âœ“ æ•°æ®æ¸…æ´—å®Œæˆ: {original_shape} -> {final_shape}")

        return cleaned_data

    def _handle_missing_values(self, data: pd.DataFrame, method: str) -> pd.DataFrame:
        """å¤„ç†ç¼ºå¤±å€¼"""
        print("   å¤„ç†ç¼ºå¤±å€¼...")
        cleaned_data = data.copy()

        for col in data.columns:
            missing_percentage = data[col].isnull().sum() / len(data) * 100

            if missing_percentage > 0:
                if missing_percentage > self.config['missing_threshold'] * 100:
                    # åˆ é™¤ç¼ºå¤±å€¼è¿‡å¤šçš„åˆ—
                    cleaned_data = cleaned_data.drop(columns=[col])
                    print(f"     - åˆ é™¤åˆ— '{col}' (ç¼ºå¤±å€¼ {missing_percentage:.1f}%)")
                    self.preprocessing_steps.append(f"åˆ é™¤åˆ—: {col} (ç¼ºå¤±å€¼è¿‡å¤š)")
                    continue

                # æ ¹æ®æ•°æ®ç±»å‹é€‰æ‹©å¡«å……æ–¹æ³•
                if method == 'auto':
                    if pd.api.types.is_numeric_dtype(data[col]):
                        fill_method = 'median'
                    else:
                        fill_method = 'mode'
                else:
                    fill_method = method

                if fill_method == 'mean' and pd.api.types.is_numeric_dtype(data[col]):
                    cleaned_data[col] = cleaned_data[col].fillna(cleaned_data[col].mean())
                elif fill_method == 'median' and pd.api.types.is_numeric_dtype(data[col]):
                    cleaned_data[col] = cleaned_data[col].fillna(cleaned_data[col].median())
                elif fill_method == 'mode':
                    mode_value = cleaned_data[col].mode()
                    if len(mode_value) > 0:
                        cleaned_data[col] = cleaned_data[col].fillna(mode_value[0])
                elif fill_method == 'knn' and pd.api.types.is_numeric_dtype(data[col]):
                    # ä½¿ç”¨KNNå¡«å……
                    imputer = KNNImputer(n_neighbors=5)
                    cleaned_data[[col]] = imputer.fit_transform(cleaned_data[[col]])
                    self.imputers[col] = imputer
                elif fill_method == 'forward':
                    cleaned_data[col] = cleaned_data[col].fillna(method='ffill')
                elif fill_method == 'backward':
                    cleaned_data[col] = cleaned_data[col].fillna(method='bfill')

                print(f"     - å¡«å……åˆ— '{col}' ç¼ºå¤±å€¼ (æ–¹æ³•: {fill_method})")

        self.preprocessing_steps.append(f"å¤„ç†ç¼ºå¤±å€¼: {method}")
        return cleaned_data

    def _handle_outliers(self, data: pd.DataFrame, method: str) -> pd.DataFrame:
        """å¤„ç†å¼‚å¸¸å€¼"""
        print("   å¤„ç†å¼‚å¸¸å€¼...")
        cleaned_data = data.copy()
        outlier_count = 0

        numeric_cols = data.select_dtypes(include=[np.number]).columns

        if method == 'auto':
            method = self.config['outlier_method']

        if method == 'iqr':
            for col in numeric_cols:
                Q1 = data[col].quantile(0.25)
                Q3 = data[col].quantile(0.75)
                IQR = Q3 - Q1
                lower_bound = Q1 - 1.5 * IQR
                upper_bound = Q3 + 1.5 * IQR

                outlier_mask = ((data[col] < lower_bound) | (data[col] > upper_bound))
                col_outliers = outlier_mask.sum()
                if col_outliers > 0:
                    outlier_count += col_outliers
                    # ç”¨è¾¹ç•Œå€¼æ›¿æ¢å¼‚å¸¸å€¼
                    cleaned_data[col] = np.where(data[col] < lower_bound, lower_bound, data[col])
                    cleaned_data[col] = np.where(data[col] > upper_bound, upper_bound, cleaned_data[col])

        elif method == 'isolation_forest':
            # ä½¿ç”¨Isolation Forestæ£€æµ‹å¼‚å¸¸å€¼
            iso_forest = IsolationForest(contamination=self.config['outlier_contamination'],
                                       random_state=self.config['random_state'])

            # åªä½¿ç”¨æ•°å€¼åˆ—è¿›è¡Œæ£€æµ‹
            numeric_data = data[numeric_cols].dropna()
            if len(numeric_data) > 0:
                outlier_labels = iso_forest.fit_predict(numeric_data)
                outlier_mask = outlier_labels == -1

                # ç§»é™¤å¼‚å¸¸å€¼è¡Œ
                outlier_indices = numeric_data.index[outlier_mask]
                outlier_count = len(outlier_indices)
                cleaned_data = cleaned_data.drop(outlier_indices)

        print(f"     - å¤„ç†äº† {outlier_count} ä¸ªå¼‚å¸¸å€¼")
        self.preprocessing_steps.append(f"å¤„ç†å¼‚å¸¸å€¼: {method}")
        return cleaned_data

    def transform_data_types(self, data: pd.DataFrame, auto_detect: bool = True) -> pd.DataFrame:
        """
        è½¬æ¢æ•°æ®ç±»å‹

        Parameters:
        - data: æ•°æ®DataFrame
        - auto_detect: æ˜¯å¦è‡ªåŠ¨æ£€æµ‹æ•°æ®ç±»å‹

        Returns:
        - ç±»å‹è½¬æ¢åçš„æ•°æ®
        """
        print("ğŸ”„ è½¬æ¢æ•°æ®ç±»å‹...")
        transformed_data = data.copy()
        type_conversions = []

        for col in data.columns:
            original_type = str(data[col].dtype)

            if auto_detect:
                # å°è¯•è‡ªåŠ¨æ£€æµ‹æœ€ä½³ç±»å‹
                if data[col].dtype == 'object':
                    # å°è¯•è½¬æ¢ä¸ºæ•°å€¼ç±»å‹
                    try:
                        numeric_data = pd.to_numeric(data[col], errors='raise')
                        if (numeric_data % 1 == 0).all():
                            transformed_data[col] = numeric_data.astype('int64')
                            type_conversions.append(f"{col}: {original_type} -> int64")
                        else:
                            transformed_data[col] = numeric_data.astype('float64')
                            type_conversions.append(f"{col}: {original_type} -> float64")
                    except:
                        # å°è¯•è½¬æ¢ä¸ºæ—¥æœŸæ—¶é—´
                        try:
                            transformed_data[col] = pd.to_datetime(data[col], errors='raise')
                            type_conversions.append(f"{col}: {original_type} -> datetime64")
                        except:
                            # å°è¯•è½¬æ¢ä¸ºåˆ†ç±»ç±»å‹
                            unique_ratio = data[col].nunique() / len(data)
                            if unique_ratio < 0.5:  # å¦‚æœå”¯ä¸€å€¼æ¯”ä¾‹å°äº50%
                                transformed_data[col] = data[col].astype('category')
                                type_conversions.append(f"{col}: {original_type} -> category")

        if type_conversions:
            print("   æ•°æ®ç±»å‹è½¬æ¢:")
            for conversion in type_conversions:
                print(f"     âœ“ {conversion}")
            self.preprocessing_steps.append("æ•°æ®ç±»å‹è‡ªåŠ¨è½¬æ¢")

        return transformed_data

    def encode_categorical(self, data: pd.DataFrame, columns: Optional[List[str]] = None,
                          method: str = 'auto') -> pd.DataFrame:
        """
        ç¼–ç åˆ†ç±»å˜é‡

        Parameters:
        - data: æ•°æ®DataFrame
        - columns: è¦ç¼–ç çš„åˆ—ååˆ—è¡¨
        - method: ç¼–ç æ–¹æ³•

        Returns:
        - ç¼–ç åçš„æ•°æ®
        """
        print("ğŸ·ï¸ ç¼–ç åˆ†ç±»å˜é‡...")
        encoded_data = data.copy()

        if columns is None:
            columns = data.select_dtypes(include=['object', 'category']).columns.tolist()

        if not columns:
            print("   âœ“ æ²¡æœ‰éœ€è¦ç¼–ç çš„åˆ†ç±»å˜é‡")
            return encoded_data

        if method == 'auto':
            # è‡ªåŠ¨é€‰æ‹©ç¼–ç æ–¹æ³•
            for col in columns:
                unique_count = data[col].nunique()

                if unique_count == 2:
                    # äºŒåˆ†ç±»å˜é‡ä½¿ç”¨æ ‡ç­¾ç¼–ç 
                    encoder = LabelEncoder()
                    encoded_data[col] = encoder.fit_transform(data[col].astype(str))
                    self.encoders[col] = encoder
                    print(f"     âœ“ æ ‡ç­¾ç¼–ç : {col} ({unique_count} ç±»åˆ«)")

                elif unique_count <= 10:
                    # å°‘é‡ç±»åˆ«ä½¿ç”¨ç‹¬çƒ­ç¼–ç 
                    dummies = pd.get_dummies(data[col], prefix=col)
                    encoded_data = pd.concat([encoded_data.drop(columns=[col]), dummies], axis=1)
                    print(f"     âœ“ ç‹¬çƒ­ç¼–ç : {col} ({unique_count} ç±»åˆ«)")

                else:
                    # å¤šç±»åˆ«ä½¿ç”¨æ ‡ç­¾ç¼–ç 
                    encoder = LabelEncoder()
                    encoded_data[col] = encoder.fit_transform(data[col].astype(str))
                    self.encoders[col] = encoder
                    print(f"     âœ“ æ ‡ç­¾ç¼–ç : {col} ({unique_count} ç±»åˆ«)")

        else:
            # ä½¿ç”¨æŒ‡å®šçš„ç¼–ç æ–¹æ³•
            if method == 'label':
                for col in columns:
                    encoder = LabelEncoder()
                    encoded_data[col] = encoder.fit_transform(data[col].astype(str))
                    self.encoders[col] = encoder
                    print(f"     âœ“ æ ‡ç­¾ç¼–ç : {col}")

            elif method == 'onehot':
                for col in columns:
                    dummies = pd.get_dummies(data[col], prefix=col)
                    encoded_data = pd.concat([encoded_data.drop(columns=[col]), dummies], axis=1)
                    print(f"     âœ“ ç‹¬çƒ­ç¼–ç : {col}")

        self.preprocessing_steps.append(f"åˆ†ç±»å˜é‡ç¼–ç : {method}")
        return encoded_data

    def scale_features(self, data: pd.DataFrame, columns: Optional[List[str]] = None,
                      method: str = None) -> pd.DataFrame:
        """
        ç‰¹å¾ç¼©æ”¾

        Parameters:
        - data: æ•°æ®DataFrame
        - columns: è¦ç¼©æ”¾çš„åˆ—ååˆ—è¡¨
        - method: ç¼©æ”¾æ–¹æ³•

        Returns:
        - ç¼©æ”¾åçš„æ•°æ®
        """
        print("ğŸ“ ç‰¹å¾ç¼©æ”¾...")
        scaled_data = data.copy()

        if columns is None:
            columns = data.select_dtypes(include=[np.number]).columns.tolist()

        if not columns:
            print("   âœ“ æ²¡æœ‰éœ€è¦ç¼©æ”¾çš„æ•°å€¼å˜é‡")
            return scaled_data

        if method is None:
            method = self.config['scaling_method']

        # é€‰æ‹©ç¼©æ”¾å™¨
        if method == 'standard':
            scaler = StandardScaler()
        elif method == 'minmax':
            scaler = MinMaxScaler()
        elif method == 'robust':
            scaler = RobustScaler()
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„ç¼©æ”¾æ–¹æ³•: {method}")

        # åº”ç”¨ç¼©æ”¾
        scaled_data[columns] = scaler.fit_transform(data[columns])
        self.scalers['feature_scaler'] = scaler

        print(f"   âœ“ ä½¿ç”¨ {method} æ–¹æ³•ç¼©æ”¾äº† {len(columns)} ä¸ªç‰¹å¾")
        self.preprocessing_steps.append(f"ç‰¹å¾ç¼©æ”¾: {method}")

        return scaled_data

    def select_features(self, data: pd.DataFrame, target_col: str,
                       method: str = 'univariate', k: int = None) -> Tuple[pd.DataFrame, List[str]]:
        """
        ç‰¹å¾é€‰æ‹©

        Parameters:
        - data: æ•°æ®DataFrame
        - target_col: ç›®æ ‡åˆ—å
        - method: é€‰æ‹©æ–¹æ³•
        - k: é€‰æ‹©çš„ç‰¹å¾æ•°é‡

        Returns:
        - é€‰æ‹©åçš„æ•°æ®å’Œç‰¹å¾åˆ—è¡¨
        """
        print("ğŸ¯ ç‰¹å¾é€‰æ‹©...")

        if target_col not in data.columns:
            print(f"   âš ï¸ ç›®æ ‡åˆ— '{target_col}' ä¸å­˜åœ¨ï¼Œè·³è¿‡ç‰¹å¾é€‰æ‹©")
            return data, data.columns.tolist()

        if k is None:
            k = self.config['k_features']

        # å‡†å¤‡æ•°æ®
        X = data.drop(columns=[target_col])
        y = data[target_col]

        # åªä½¿ç”¨æ•°å€¼åˆ—è¿›è¡Œé€‰æ‹©
        numeric_cols = X.select_dtypes(include=[np.number]).columns.tolist()
        X_numeric = X[numeric_cols]

        if len(numeric_cols) == 0:
            print("   âš ï¸ æ²¡æœ‰æ•°å€¼ç‰¹å¾ï¼Œè·³è¿‡ç‰¹å¾é€‰æ‹©")
            return data, data.columns.tolist()

        if method == 'univariate':
            # å•å˜é‡ç»Ÿè®¡é€‰æ‹©
            if y.dtype == 'object' or len(y.unique()) < 10:
                # åˆ†ç±»é—®é¢˜
                selector = SelectKBest(score_func=f_classif, k=min(k, len(numeric_cols)))
            else:
                # å›å½’é—®é¢˜
                selector = SelectKBest(score_func=f_regression, k=min(k, len(numeric_cols)))

            X_selected = selector.fit_transform(X_numeric, y)
            selected_features = X_numeric.columns[selector.get_support()].tolist()

        elif method == 'rfe':
            # é€’å½’ç‰¹å¾æ¶ˆé™¤
            from sklearn.linear_model import LogisticRegression, LinearRegression

            if y.dtype == 'object' or len(y.unique()) < 10:
                estimator = LogisticRegression(max_iter=1000)
            else:
                estimator = LinearRegression()

            selector = RFE(estimator=estimator, n_features_to_select=min(k, len(numeric_cols)))
            X_selected = selector.fit_transform(X_numeric, y)
            selected_features = X_numeric.columns[selector.get_support()].tolist()

        else:
            raise ValueError(f"ä¸æ”¯æŒçš„ç‰¹å¾é€‰æ‹©æ–¹æ³•: {method}")

        # æ„å»ºé€‰æ‹©åçš„æ•°æ®
        other_cols = [col for col in data.columns if col not in numeric_cols and col != target_col]
        selected_data = pd.concat([
            data[other_cols],
            pd.DataFrame(X_selected, columns=selected_features, index=data.index),
            data[[target_col]]
        ], axis=1)

        print(f"   âœ“ ä» {len(numeric_cols)} ä¸ªç‰¹å¾ä¸­é€‰æ‹©äº† {len(selected_features)} ä¸ª")
        self.feature_selectors['feature_selector'] = selector
        self.preprocessing_steps.append(f"ç‰¹å¾é€‰æ‹©: {method} (é€‰æ‹©äº† {len(selected_features)} ä¸ªç‰¹å¾)")

        return selected_data, selected_features

    def split_data(self, data: pd.DataFrame, target_col: str,
                   test_size: float = None, stratify: bool = True) -> Dict:
        """
        æ•°æ®åˆ†å‰²

        Parameters:
        - data: æ•°æ®DataFrame
        - target_col: ç›®æ ‡åˆ—å
        - test_size: æµ‹è¯•é›†æ¯”ä¾‹
        - stratify: æ˜¯å¦åˆ†å±‚æŠ½æ ·

        Returns:
        - åˆ†å‰²åçš„æ•°æ®å­—å…¸
        """
        print("âœ‚ï¸ åˆ†å‰²æ•°æ®...")

        if test_size is None:
            test_size = self.config['test_size']

        X = data.drop(columns=[target_col])
        y = data[target_col]

        # åˆ†å±‚æŠ½æ ·å‚æ•°
        stratify_param = y if stratify and (y.dtype == 'object' or len(y.unique()) < 100) else None

        # åˆ†å‰²æ•°æ®
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=test_size, random_state=self.config['random_state'],
            stratify=stratify_param
        )

        split_info = {
            'X_train': X_train,
            'X_test': X_test,
            'y_train': y_train,
            'y_test': y_test,
            'train_size': len(X_train),
            'test_size': len(X_test),
            'train_ratio': len(X_train) / len(data),
            'test_ratio': len(X_test) / len(data),
            'feature_count': X.shape[1],
            'target_classes': y.nunique() if y.dtype == 'object' else 'continuous'
        }

        print(f"   âœ“ è®­ç»ƒé›†: {len(X_train)} æ ·æœ¬ ({len(X_train)/len(data):.1%})")
        print(f"   âœ“ æµ‹è¯•é›†: {len(X_test)} æ ·æœ¬ ({len(X_test)/len(data):.1%})")

        self.preprocessing_steps.append(f"æ•°æ®åˆ†å‰²: æµ‹è¯•é›†æ¯”ä¾‹ {test_size}")
        return split_info

    def balance_data(self, X_train: pd.DataFrame, y_train: pd.DataFrame,
                    method: str = None) -> Tuple[pd.DataFrame, pd.DataFrame]:
        """
        å¹³è¡¡æ•°æ®

        Parameters:
        - X_train: è®­ç»ƒç‰¹å¾
        - y_train: è®­ç»ƒæ ‡ç­¾
        - method: å¹³è¡¡æ–¹æ³•

        Returns:
        - å¹³è¡¡åçš„æ•°æ®
        """
        if method is None:
            method = self.config['balance_method']

        # æ£€æŸ¥æ˜¯å¦éœ€è¦å¹³è¡¡
        if y_train.dtype == 'object' or len(y_train.unique()) < 100:
            class_counts = y_train.value_counts()
            min_count = class_counts.min()
            max_count = class_counts.max()

            if max_count / min_count <= 2:  # å¦‚æœç±»åˆ«æ¯”ä¾‹å°äº2:1ï¼Œè®¤ä¸ºå·²ç»å¹³è¡¡
                print("   âœ“ æ•°æ®å·²ç»å¹³è¡¡ï¼Œæ— éœ€å¤„ç†")
                return X_train, y_train

        print(f"âš–ï¸ å¹³è¡¡æ•°æ® (æ–¹æ³•: {method})...")

        if method == 'smote':
            # SMOTEè¿‡é‡‡æ ·
            smote = SMOTE(random_state=self.config['random_state'])
            X_balanced, y_balanced = smote.fit_resample(X_train, y_train)

        elif method == 'oversample':
            # éšæœºè¿‡é‡‡æ ·
            ros = RandomOverSampler(random_state=self.config['random_state'])
            X_balanced, y_balanced = ros.fit_resample(X_train, y_train)

        elif method == 'undersample':
            # éšæœºæ¬ é‡‡æ ·
            rus = RandomUnderSampler(random_state=self.config['random_state'])
            X_balanced, y_balanced = rus.fit_resample(X_train, y_train)

        else:
            raise ValueError(f"ä¸æ”¯æŒçš„å¹³è¡¡æ–¹æ³•: {method}")

        print(f"   âœ“ å¹³è¡¡å‰: {X_train.shape[0]} æ ·æœ¬")
        print(f"   âœ“ å¹³è¡¡å: {X_balanced.shape[0]} æ ·æœ¬")

        # è½¬æ¢å›DataFrame
        if hasattr(X_balanced, 'toarray'):
            X_balanced = pd.DataFrame(X_balanced.toarray(), columns=X_train.columns)
        else:
            X_balanced = pd.DataFrame(X_balanced, columns=X_train.columns)

        y_balanced = pd.Series(y_balanced, name=y_train.name)

        self.preprocessing_steps.append(f"æ•°æ®å¹³è¡¡: {method}")
        return X_balanced, y_balanced

    def auto_preprocess(self, data: pd.DataFrame, target_col: str,
                       save_report: bool = True) -> Dict:
        """
        è‡ªåŠ¨åŒ–é¢„å¤„ç†æµç¨‹

        Parameters:
        - data: åŸå§‹æ•°æ®
        - target_col: ç›®æ ‡åˆ—å
        - save_report: æ˜¯å¦ä¿å­˜æŠ¥å‘Š

        Returns:
        - é¢„å¤„ç†ç»“æœå­—å…¸
        """
        print("ğŸš€ å¼€å§‹è‡ªåŠ¨åŒ–æ•°æ®é¢„å¤„ç†...")

        results = {
            'original_data': data,
            'preprocessed_data': None,
            'X_train': None,
            'X_test': None,
            'y_train': None,
            'y_test': None,
            'quality_report': None,
            'preprocessing_steps': [],
            'feature_info': {}
        }

        # 1. æ•°æ®è´¨é‡åˆ†æ
        quality_report = self.analyze_data_quality(data)
        results['quality_report'] = quality_report

        # 2. æ•°æ®æ¸…æ´—
        cleaned_data = self.clean_data(data)
        results['preprocessing_steps'].extend(self.preprocessing_steps)

        # 3. æ•°æ®ç±»å‹è½¬æ¢
        transformed_data = self.transform_data_types(cleaned_data)

        # 4. ç‰¹å¾å·¥ç¨‹
        # è¿™é‡Œå¯ä»¥æ·»åŠ æ›´å¤šçš„ç‰¹å¾å·¥ç¨‹æ­¥éª¤
        engineered_data = self._feature_engineering(transformed_data, target_col)

        # 5. ç¼–ç åˆ†ç±»å˜é‡
        encoded_data = self.encode_categorical(engineered_data)

        # 6. ç‰¹å¾ç¼©æ”¾
        numeric_cols = encoded_data.select_dtypes(include=[np.number]).columns.tolist()
        if target_col in numeric_cols:
            numeric_cols.remove(target_col)

        if numeric_cols:
            scaled_data = self.scale_features(encoded_data, numeric_cols)
        else:
            scaled_data = encoded_data

        results['preprocessed_data'] = scaled_data

        # 7. ç‰¹å¾é€‰æ‹©ï¼ˆå¯é€‰ï¼‰
        if self.config['feature_selection'] and target_col in scaled_data.columns:
            selected_data, selected_features = self.select_features(
                scaled_data, target_col, k=self.config['k_features']
            )
            results['feature_info']['selected_features'] = selected_features
            final_data = selected_data
        else:
            final_data = scaled_data
            results['feature_info']['all_features'] = [
                col for col in final_data.columns if col != target_col
            ]

        # 8. æ•°æ®åˆ†å‰²
        if target_col in final_data.columns:
            split_result = self.split_data(final_data, target_col)
            results.update(split_result)

        # 9. æ•°æ®å¹³è¡¡ï¼ˆå¯é€‰ï¼Œä»…å¯¹åˆ†ç±»é—®é¢˜ï¼‰
        if (self.config['balance_data'] and
            target_col in final_data.columns and
            (results['y_train'].dtype == 'object' or len(results['y_train'].unique()) < 100)):

            X_balanced, y_balanced = self.balance_data(
                results['X_train'], results['y_train'], method=self.config['balance_method']
            )
            results['X_train'] = X_balanced
            results['y_train'] = y_balanced

        # ä¿å­˜é¢„å¤„ç†æŠ¥å‘Š
        if save_report:
            self.preprocessing_report = {
                'timestamp': pd.Timestamp.now().isoformat(),
                'original_shape': data.shape,
                'final_shape': results['preprocessed_data'].shape,
                'preprocessing_steps': self.preprocessing_steps,
                'quality_score': quality_report['overall_score'],
                'feature_count': len(results['feature_info'].get('selected_features',
                                                results['feature_info'].get('all_features', [])))
            }

        print(f"\nğŸ‰ è‡ªåŠ¨åŒ–é¢„å¤„ç†å®Œæˆï¼")
        print(f"   åŸå§‹æ•°æ®: {data.shape}")
        print(f"   é¢„å¤„ç†å: {results['preprocessed_data'].shape}")
        print(f"   é¢„å¤„ç†æ­¥éª¤: {len(self.preprocessing_steps)}")

        return results

    def _feature_engineering(self, data: pd.DataFrame, target_col: str) -> pd.DataFrame:
        """åŸºç¡€ç‰¹å¾å·¥ç¨‹"""
        engineered_data = data.copy()
        numeric_cols = data.select_dtypes(include=[np.number]).columns.tolist()

        # åˆ›å»ºäº¤äº’ç‰¹å¾ï¼ˆå¯¹äºæ•°å€¼å˜é‡ï¼‰
        if len(numeric_cols) >= 2:
            # é€‰æ‹©å‰å‡ ä¸ªé‡è¦å˜é‡åˆ›å»ºäº¤äº’é¡¹
            important_cols = numeric_cols[:min(3, len(numeric_cols))]

            for i, col1 in enumerate(important_cols):
                for col2 in important_cols[i+1:]:
                    # ä¹˜ç§¯ç‰¹å¾
                    engineered_data[f'{col1}_x_{col2}'] = data[col1] * data[col2]
                    # æ¯”å€¼ç‰¹å¾ï¼ˆé¿å…é™¤é›¶ï¼‰
                    engineered_data[f'{col1}_div_{col2}'] = np.where(
                        data[col2] != 0, data[col1] / data[col2], 0
                    )

        # åˆ›å»ºå¤šé¡¹å¼ç‰¹å¾ï¼ˆå¯¹äºé‡è¦å˜é‡ï¼‰
        if len(important_cols) > 0:
            for col in important_cols[:2]:  # åªä¸ºå‰ä¸¤ä¸ªå˜é‡åˆ›å»º
                engineered_data[f'{col}_squared'] = data[col] ** 2
                engineered_data[f'{col}_sqrt'] = np.sqrt(np.abs(data[col]))

        print(f"   âœ“ åˆ›å»ºäº† {engineered_data.shape[1] - data.shape[1]} ä¸ªæ–°ç‰¹å¾")
        return engineered_data

    def get_preprocessing_summary(self) -> Dict:
        """
        è·å–é¢„å¤„ç†æ‘˜è¦

        Returns:
        - é¢„å¤„ç†æ‘˜è¦ä¿¡æ¯
        """
        return {
            'preprocessing_steps': self.preprocessing_steps,
            'scalers': list(self.scalers.keys()),
            'encoders': list(self.encoders.keys()),
            'imputers': list(self.imputers.keys()),
            'feature_selectors': list(self.feature_selectors.keys()),
            'preprocessing_report': getattr(self, 'preprocessing_report', {}),
            'config': self.config
        }

    def save_preprocessing_objects(self, output_dir: str):
        """
        ä¿å­˜é¢„å¤„ç†å¯¹è±¡

        Parameters:
        - output_dir: è¾“å‡ºç›®å½•
        """
        output_path = Path(output_dir)
        output_path.mkdir(exist_ok=True)

        # ä¿å­˜scalers
        if self.scalers:
            import joblib
            for name, scaler in self.scalers.items():
                joblib.dump(scaler, output_path / f"{name}.pkl")

        # ä¿å­˜encoders
        if self.encoders:
            import joblib
            for name, encoder in self.encoders.items():
                joblib.dump(encoder, output_path / f"{name}_encoder.pkl")

        # ä¿å­˜é¢„å¤„ç†æŠ¥å‘Š
        if hasattr(self, 'preprocessing_report'):
            with open(output_path / 'preprocessing_report.json', 'w', encoding='utf-8') as f:
                json.dump(self.preprocessing_report, f, ensure_ascii=False, indent=2)

        print(f"âœ… é¢„å¤„ç†å¯¹è±¡å·²ä¿å­˜åˆ° {output_dir}")

    def load_preprocessing_objects(self, input_dir: str):
        """
        åŠ è½½é¢„å¤„ç†å¯¹è±¡

        Parameters:
        - input_dir: è¾“å…¥ç›®å½•
        """
        input_path = Path(input_dir)

        # åŠ è½½scalers
        import joblib
        for scaler_file in input_path.glob("*.pkl"):
            if not str(scaler_file).endswith('_encoder.pkl'):
                name = scaler_file.stem
                self.scalers[name] = joblib.load(scaler_file)

        # åŠ è½½encoders
        for encoder_file in input_path.glob("*_encoder.pkl"):
            name = encoder_file.stem.replace('_encoder', '')
            self.encoders[name] = joblib.load(encoder_file)

        print(f"âœ… é¢„å¤„ç†å¯¹è±¡å·²ä» {input_dir} åŠ è½½")

    def transform_new_data(self, new_data: pd.DataFrame) -> pd.DataFrame:
        """
        å¯¹æ–°æ•°æ®åº”ç”¨ç›¸åŒçš„é¢„å¤„ç†

        Parameters:
        - new_data: æ–°æ•°æ®

        Returns:
        - é¢„å¤„ç†åçš„æ–°æ•°æ®
        """
        transformed_data = new_data.copy()

        # åº”ç”¨ç¼ºå¤±å€¼å¤„ç†
        for col, imputer in self.imputers.items():
            if col in transformed_data.columns:
                if hasattr(imputer, 'transform'):
                    transformed_data[[col]] = imputer.transform(transformed_data[[col]])

        # åº”ç”¨ç¼–ç 
        for col, encoder in self.encoders.items():
            if col in transformed_data.columns:
                transformed_data[col] = encoder.transform(transformed_data[col].astype(str))

        # åº”ç”¨ç¼©æ”¾
        if 'feature_scaler' in self.scalers:
            numeric_cols = transformed_data.select_dtypes(include=[np.number]).columns.tolist()
            if numeric_cols:
                transformed_data[numeric_cols] = self.scalers['feature_scaler'].transform(
                    transformed_data[numeric_cols]
                )

        return transformed_data