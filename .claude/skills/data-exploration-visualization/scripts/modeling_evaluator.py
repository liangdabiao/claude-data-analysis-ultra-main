#!/usr/bin/env python3
"""
å»ºæ¨¡è¯„ä¼°å™¨ (Modeling Evaluator) - æœºå™¨å­¦ä¹ æ¨¡å‹è®­ç»ƒå’Œè¯„ä¼°æ¨¡å—

æä¾›å…¨é¢çš„å»ºæ¨¡åŠŸèƒ½ï¼ŒåŒ…æ‹¬ï¼š
- å¤šç§åˆ†ç±»ç®—æ³•æ”¯æŒï¼ˆé€»è¾‘å›å½’ã€éšæœºæ£®æ—ã€XGBoostã€ç¥ç»ç½‘ç»œï¼‰
- è‡ªåŠ¨åŒ–æ¨¡å‹é€‰æ‹©å’Œè¶…å‚æ•°è°ƒä¼˜
- å…¨é¢çš„æ¨¡å‹è¯„ä¼°æŒ‡æ ‡
- æ¨¡å‹å¯è§£é‡Šæ€§åˆ†æï¼ˆç‰¹å¾é‡è¦æ€§ã€SHAPå€¼ï¼‰
- äº¤å‰éªŒè¯å’Œæ¨¡å‹æ¯”è¾ƒ
- åŒ»ç–—æ•°æ®å»ºæ¨¡ç‰¹åŒ–æ”¯æŒ
"""

import pandas as pd
import numpy as np
from typing import Dict, List, Tuple, Optional, Union, Any, Callable
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, RandomizedSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    roc_auc_score, classification_report, confusion_matrix,
    mean_squared_error, mean_absolute_error, r2_score
)
from sklearn.linear_model import LogisticRegression, LinearRegression, Ridge, Lasso
from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor, GradientBoostingClassifier
from sklearn.svm import SVC, SVR
from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor
from sklearn.naive_bayes import GaussianNB
from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor
import xgboost as xgb
import lightgbm as lgb
from sklearn.neural_network import MLPClassifier, MLPRegressor
import shap
import warnings
from pathlib import Path
import joblib
import json
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats

warnings.filterwarnings('ignore')


class ModelingEvaluator:
    """å»ºæ¨¡è¯„ä¼°å™¨ - è‡ªåŠ¨åŒ–æœºå™¨å­¦ä¹ æ¨¡å‹è®­ç»ƒå’Œè¯„ä¼°"""

    def __init__(self, config: Optional[Dict] = None):
        """
        åˆå§‹åŒ–å»ºæ¨¡è¯„ä¼°å™¨

        Parameters:
        - config: é…ç½®å‚æ•°å­—å…¸
        """
        self.config = config or {}
        self.models = {}
        self.model_results = {}
        self.best_model = None
        self.best_model_name = None
        self.explainers = {}
        self.model_history = []

        # é»˜è®¤é…ç½®
        self.default_config = {
            'cv_folds': 5,  # äº¤å‰éªŒè¯æŠ˜æ•°
            'scoring_metric': 'accuracy',  # ä¸»è¦è¯„ä¼°æŒ‡æ ‡
            'test_size': 0.2,  # æµ‹è¯•é›†æ¯”ä¾‹
            'random_state': 42,  # éšæœºç§å­
            'n_iter_search': 50,  # éšæœºæœç´¢è¿­ä»£æ¬¡æ•°
            'enable_hyperparameter_tuning': True,  # æ˜¯å¦å¯ç”¨è¶…å‚æ•°è°ƒä¼˜
            'enable_feature_importance': True,  # æ˜¯å¦è®¡ç®—ç‰¹å¾é‡è¦æ€§
            'enable_shap': True,  # æ˜¯å¦å¯ç”¨SHAPè§£é‡Š
            'model_timeout': 300,  # æ¨¡å‹è®­ç»ƒè¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
            'ensemble_models': True,  # æ˜¯å¦ä½¿ç”¨é›†æˆæ–¹æ³•
            'medical_specialization': False  # æ˜¯å¦å¯ç”¨åŒ»ç–—æ•°æ®ç‰¹åŒ–
        }

        # åˆå¹¶é…ç½®
        self.config = {**self.default_config, **self.config}

        # åˆå§‹åŒ–æ¨¡å‹å­—å…¸
        self._initialize_models()

    def _initialize_models(self):
        """åˆå§‹åŒ–å¯ç”¨æ¨¡å‹"""
        self.available_models = {
            # åˆ†ç±»æ¨¡å‹
            'classification': {
                'logistic_regression': {
                    'model': LogisticRegression(random_state=self.config['random_state'], max_iter=1000),
                    'param_grid': {
                        'C': [0.001, 0.01, 0.1, 1, 10, 100],
                        'penalty': ['l1', 'l2', 'elasticnet'],
                        'solver': ['liblinear', 'saga']
                    }
                },
                'random_forest': {
                    'model': RandomForestClassifier(random_state=self.config['random_state']),
                    'param_grid': {
                        'n_estimators': [100, 200, 300],
                        'max_depth': [None, 10, 20, 30],
                        'min_samples_split': [2, 5, 10],
                        'min_samples_leaf': [1, 2, 4]
                    }
                },
                'xgboost': {
                    'model': xgb.XGBClassifier(random_state=self.config['random_state']),
                    'param_grid': {
                        'n_estimators': [100, 200, 300],
                        'max_depth': [3, 6, 9],
                        'learning_rate': [0.01, 0.1, 0.2],
                        'subsample': [0.8, 0.9, 1.0]
                    }
                },
                'lightgbm': {
                    'model': lgb.LGBMClassifier(random_state=self.config['random_state']),
                    'param_grid': {
                        'n_estimators': [100, 200, 300],
                        'max_depth': [3, 6, 9],
                        'learning_rate': [0.01, 0.1, 0.2],
                        'num_leaves': [31, 62, 127]
                    }
                },
                'svm': {
                    'model': SVC(random_state=self.config['random_state'], probability=True),
                    'param_grid': {
                        'C': [0.1, 1, 10],
                        'kernel': ['rbf', 'linear', 'poly'],
                        'gamma': ['scale', 'auto']
                    }
                },
                'knn': {
                    'model': KNeighborsClassifier(),
                    'param_grid': {
                        'n_neighbors': [3, 5, 7, 9],
                        'weights': ['uniform', 'distance'],
                        'algorithm': ['auto', 'ball_tree', 'kd_tree']
                    }
                },
                'naive_bayes': {
                    'model': GaussianNB(),
                    'param_grid': {}
                },
                'decision_tree': {
                    'model': DecisionTreeClassifier(random_state=self.config['random_state']),
                    'param_grid': {
                        'max_depth': [None, 10, 20, 30],
                        'min_samples_split': [2, 5, 10],
                        'min_samples_leaf': [1, 2, 4]
                    }
                },
                'mlp': {
                    'model': MLPClassifier(random_state=self.config['random_state'], max_iter=1000),
                    'param_grid': {
                        'hidden_layer_sizes': [(50,), (100,), (50, 50)],
                        'activation': ['relu', 'tanh'],
                        'alpha': [0.0001, 0.001, 0.01]
                    }
                }
            },
            # å›å½’æ¨¡å‹
            'regression': {
                'linear_regression': {
                    'model': LinearRegression(),
                    'param_grid': {}
                },
                'ridge': {
                    'model': Ridge(random_state=self.config['random_state']),
                    'param_grid': {
                        'alpha': [0.1, 1.0, 10.0]
                    }
                },
                'lasso': {
                    'model': Lasso(random_state=self.config['random_state']),
                    'param_grid': {
                        'alpha': [0.1, 1.0, 10.0]
                    }
                },
                'random_forest': {
                    'model': RandomForestRegressor(random_state=self.config['random_state']),
                    'param_grid': {
                        'n_estimators': [100, 200, 300],
                        'max_depth': [None, 10, 20, 30],
                        'min_samples_split': [2, 5, 10]
                    }
                },
                'xgboost': {
                    'model': xgb.XGBRegressor(random_state=self.config['random_state']),
                    'param_grid': {
                        'n_estimators': [100, 200, 300],
                        'max_depth': [3, 6, 9],
                        'learning_rate': [0.01, 0.1, 0.2]
                    }
                },
                'svm': {
                    'model': SVR(),
                    'param_grid': {
                        'C': [0.1, 1, 10],
                        'kernel': ['rbf', 'linear'],
                        'gamma': ['scale', 'auto']
                    }
                }
            }
        }

    def detect_problem_type(self, y: pd.Series) -> str:
        """
        æ£€æµ‹é—®é¢˜ç±»å‹ï¼ˆåˆ†ç±»æˆ–å›å½’ï¼‰

        Parameters:
        - y: ç›®æ ‡å˜é‡

        Returns:
        - é—®é¢˜ç±»å‹
        """
        if y.dtype == 'object' or len(y.unique()) < 20:
            return 'classification'
        else:
            return 'regression'

    def train_single_model(self, X_train: pd.DataFrame, y_train: pd.Series,
                          X_test: pd.DataFrame, y_test: pd.Series,
                          model_name: str, tune_hyperparameters: bool = None) -> Dict:
        """
        è®­ç»ƒå•ä¸ªæ¨¡å‹

        Parameters:
        - X_train: è®­ç»ƒç‰¹å¾
        - y_train: è®­ç»ƒæ ‡ç­¾
        - X_test: æµ‹è¯•ç‰¹å¾
        - y_test: æµ‹è¯•æ ‡ç­¾
        - model_name: æ¨¡å‹åç§°
        - tune_hyperparameters: æ˜¯å¦è°ƒå‚

        Returns:
        - æ¨¡å‹è®­ç»ƒç»“æœ
        """
        if tune_hyperparameters is None:
            tune_hyperparameters = self.config['enable_hyperparameter_tuning']

        problem_type = self.detect_problem_type(y_train)
        models_dict = self.available_models[problem_type]

        if model_name not in models_dict:
            raise ValueError(f"æ¨¡å‹ '{model_name}' ä¸é€‚ç”¨äº {problem_type} é—®é¢˜")

        print(f"ğŸš€ è®­ç»ƒæ¨¡å‹: {model_name}")

        model_info = models_dict[model_name]
        model = model_info['model']

        # è¶…å‚æ•°è°ƒä¼˜
        if tune_hyperparameters and model_info['param_grid']:
            print(f"   ğŸ”§ è¿›è¡Œè¶…å‚æ•°è°ƒä¼˜...")
            search = RandomizedSearchCV(
                model, model_info['param_grid'],
                n_iter=self.config['n_iter_search'],
                cv=self.config['cv_folds'],
                scoring=self.config['scoring_metric'],
                random_state=self.config['random_state'],
                n_jobs=-1
            )
            search.fit(X_train, y_train)
            best_model = search.best_estimator_
            best_params = search.best_params_
            print(f"   âœ“ æœ€ä½³å‚æ•°: {best_params}")
        else:
            model.fit(X_train, y_train)
            best_model = model
            best_params = {}

        # é¢„æµ‹
        if problem_type == 'classification':
            y_pred = best_model.predict(X_test)
            y_pred_proba = best_model.predict_proba(X_test)[:, 1] if hasattr(best_model, 'predict_proba') else None
            metrics = self._calculate_classification_metrics(y_test, y_pred, y_pred_proba)
        else:
            y_pred = best_model.predict(X_test)
            metrics = self._calculate_regression_metrics(y_test, y_pred)

        # äº¤å‰éªŒè¯
        cv_scores = cross_val_score(
            best_model, X_train, y_train,
            cv=self.config['cv_folds'],
            scoring=self.config['scoring_metric']
        )

        result = {
            'model': best_model,
            'model_name': model_name,
            'problem_type': problem_type,
            'best_params': best_params,
            'metrics': metrics,
            'cv_scores': cv_scores,
            'cv_mean': cv_scores.mean(),
            'cv_std': cv_scores.std(),
            'predictions': y_pred,
            'feature_names': X_train.columns.tolist()
        }

        if problem_type == 'classification' and y_pred_proba is not None:
            result['predictions_proba'] = y_pred_proba

        self.models[model_name] = result
        print(f"   âœ“ {model_name} è®­ç»ƒå®Œæˆ - {self.config['scoring_metric']}: {cv_scores.mean():.4f}")

        return result

    def _calculate_classification_metrics(self, y_true: np.ndarray, y_pred: np.ndarray,
                                        y_pred_proba: Optional[np.ndarray] = None) -> Dict:
        """è®¡ç®—åˆ†ç±»æŒ‡æ ‡"""
        metrics = {
            'accuracy': accuracy_score(y_true, y_pred),
            'precision': precision_score(y_true, y_pred, average='weighted', zero_division=0),
            'recall': recall_score(y_true, y_pred, average='weighted', zero_division=0),
            'f1': f1_score(y_true, y_pred, average='weighted', zero_division=0)
        }

        if y_pred_proba is not None and len(np.unique(y_true)) == 2:
            metrics['auc'] = roc_auc_score(y_true, y_pred_proba)

        # åˆ†ç±»æŠ¥å‘Š
        metrics['classification_report'] = classification_report(y_true, y_pred)

        return metrics

    def _calculate_regression_metrics(self, y_true: np.ndarray, y_pred: np.ndarray) -> Dict:
        """è®¡ç®—å›å½’æŒ‡æ ‡"""
        metrics = {
            'mse': mean_squared_error(y_true, y_pred),
            'rmse': np.sqrt(mean_squared_error(y_true, y_pred)),
            'mae': mean_absolute_error(y_true, y_pred),
            'r2': r2_score(y_true, y_pred)
        }

        return metrics

    def train_multiple_models(self, X_train: pd.DataFrame, y_train: pd.Series,
                            X_test: pd.DataFrame, y_test: pd.Series,
                            model_names: Optional[List[str]] = None) -> Dict:
        """
        è®­ç»ƒå¤šä¸ªæ¨¡å‹

        Parameters:
        - X_train: è®­ç»ƒç‰¹å¾
        - y_train: è®­ç»ƒæ ‡ç­¾
        - X_test: æµ‹è¯•ç‰¹å¾
        - y_test: æµ‹è¯•æ ‡ç­¾
        - model_names: è¦è®­ç»ƒçš„æ¨¡å‹åç§°åˆ—è¡¨

        Returns:
        - æ‰€æœ‰æ¨¡å‹çš„è®­ç»ƒç»“æœ
        """
        if model_names is None:
            problem_type = self.detect_problem_type(y_train)
            model_names = list(self.available_models[problem_type].keys())

        print(f"ğŸ¯ å¼€å§‹æ‰¹é‡æ¨¡å‹è®­ç»ƒ ({len(model_names)} ä¸ªæ¨¡å‹)...")
        results = {}

        for model_name in model_names:
            try:
                result = self.train_single_model(
                    X_train, y_train, X_test, y_test, model_name
                )
                results[model_name] = result
            except Exception as e:
                print(f"   âŒ æ¨¡å‹ {model_name} è®­ç»ƒå¤±è´¥: {str(e)}")
                continue

        # é€‰æ‹©æœ€ä½³æ¨¡å‹
        if results:
            best_model_name = max(
                results.keys(),
                key=lambda x: results[x]['cv_mean']
            )
            self.best_model = results[best_model_name]['model']
            self.best_model_name = best_model_name

            print(f"\nğŸ† æœ€ä½³æ¨¡å‹: {best_model_name}")
            print(f"   CV {self.config['scoring_metric']}: {results[best_model_name]['cv_mean']:.4f}")

        self.model_results = results
        return results

    def auto_modeling(self, data: pd.DataFrame, target_col: str,
                     model_names: Optional[List[str]] = None) -> Dict:
        """
        è‡ªåŠ¨åŒ–å»ºæ¨¡æµç¨‹

        Parameters:
        - data: æ•°æ®DataFrame
        - target_col: ç›®æ ‡åˆ—å
        - model_names: è¦è®­ç»ƒçš„æ¨¡å‹åç§°åˆ—è¡¨

        Returns:
        - è‡ªåŠ¨åŒ–å»ºæ¨¡ç»“æœ
        """
        print("ğŸ¤– å¼€å§‹è‡ªåŠ¨åŒ–å»ºæ¨¡...")

        # æ•°æ®åˆ†å‰²
        X = data.drop(columns=[target_col])
        y = data[target_col]

        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=self.config['test_size'],
            random_state=self.config['random_state'],
            stratify=y if self.detect_problem_type(y) == 'classification' else None
        )

        # ç‰¹å¾æ ‡å‡†åŒ–
        scaler = StandardScaler()
        X_train_scaled = pd.DataFrame(
            scaler.fit_transform(X_train),
            columns=X_train.columns,
            index=X_train.index
        )
        X_test_scaled = pd.DataFrame(
            scaler.transform(X_test),
            columns=X_test.columns,
            index=X_test.index
        )

        # è®­ç»ƒæ¨¡å‹
        model_results = self.train_multiple_models(
            X_train_scaled, y_train, X_test_scaled, y_test, model_names
        )

        # ç‰¹å¾é‡è¦æ€§åˆ†æ
        if self.config['enable_feature_importance']:
            print("\nğŸ“Š åˆ†æç‰¹å¾é‡è¦æ€§...")
            importance_results = self.analyze_feature_importance(X_train_scaled)
        else:
            importance_results = {}

        # SHAPè§£é‡Šï¼ˆä»…å¯¹æœ€ä½³æ¨¡å‹ï¼‰
        shap_results = {}
        if (self.config['enable_shap'] and
            self.best_model is not None and
            len(X_train.columns) <= 50):  # é™åˆ¶ç‰¹å¾æ•°é‡é¿å…è®¡ç®—è¿‡æ…¢

            print("\nğŸ” ç”ŸæˆSHAPè§£é‡Š...")
            try:
                shap_results = self.generate_shap_explanations(
                    self.best_model, X_train_scaled, X_test_scaled
                )
            except Exception as e:
                print(f"   âš ï¸ SHAPè§£é‡Šç”Ÿæˆå¤±è´¥: {str(e)}")

        # æ¨¡å‹æ¯”è¾ƒ
        comparison_results = self.compare_models(model_results)

        results = {
            'data_info': {
                'shape': data.shape,
                'target_col': target_col,
                'problem_type': self.detect_problem_type(y),
                'feature_count': X.shape[1]
            },
            'split_info': {
                'train_size': len(X_train),
                'test_size': len(X_test),
                'scaler': scaler
            },
            'model_results': model_results,
            'best_model': {
                'name': self.best_model_name,
                'model': self.best_model,
                'metrics': model_results[self.best_model_name]['metrics'] if self.best_model_name else None
            },
            'feature_importance': importance_results,
            'shap_explanations': shap_results,
            'model_comparison': comparison_results
        }

        print(f"\nğŸ‰ è‡ªåŠ¨åŒ–å»ºæ¨¡å®Œæˆï¼")
        print(f"   è®­ç»ƒäº† {len(model_results)} ä¸ªæ¨¡å‹")
        print(f"   æœ€ä½³æ¨¡å‹: {self.best_model_name}")

        return results

    def analyze_feature_importance(self, X: pd.DataFrame) -> Dict:
        """
        åˆ†æç‰¹å¾é‡è¦æ€§

        Parameters:
        - X: ç‰¹å¾æ•°æ®

        Returns:
        - ç‰¹å¾é‡è¦æ€§ç»“æœ
        """
        importance_results = {}

        for model_name, result in self.models.items():
            model = result['model']
            feature_names = result['feature_names']

            try:
                if hasattr(model, 'feature_importances_'):
                    # æ ‘æ¨¡å‹çš„ç‰¹å¾é‡è¦æ€§
                    importances = model.feature_importances_
                elif hasattr(model, 'coef_'):
                    # çº¿æ€§æ¨¡å‹çš„ç³»æ•°
                    importances = np.abs(model.coef_).flatten()
                else:
                    continue

                # åˆ›å»ºé‡è¦æ€§DataFrame
                importance_df = pd.DataFrame({
                    'feature': feature_names,
                    'importance': importances
                }).sort_values('importance', ascending=False)

                importance_results[model_name] = {
                    'importance_df': importance_df,
                    'top_features': importance_df.head(10).to_dict('records')
                }

                print(f"   âœ“ {model_name} ç‰¹å¾é‡è¦æ€§åˆ†æå®Œæˆ")

            except Exception as e:
                print(f"   âš ï¸ {model_name} ç‰¹å¾é‡è¦æ€§åˆ†æå¤±è´¥: {str(e)}")

        return importance_results

    def generate_shap_explanations(self, model: Any, X_train: pd.DataFrame,
                                 X_test: pd.DataFrame, sample_size: int = 100) -> Dict:
        """
        ç”ŸæˆSHAPè§£é‡Š

        Parameters:
        - model: è®­ç»ƒå¥½çš„æ¨¡å‹
        - X_train: è®­ç»ƒç‰¹å¾
        - X_test: æµ‹è¯•ç‰¹å¾
        - sample_size: é‡‡æ ·å¤§å°

        Returns:
        - SHAPè§£é‡Šç»“æœ
        """
        # é‡‡æ ·æ•°æ®å‡å°‘è®¡ç®—é‡
        if len(X_test) > sample_size:
            X_test_sample = X_test.sample(sample_size, random_state=self.config['random_state'])
        else:
            X_test_sample = X_test

        # åˆ›å»ºè§£é‡Šå™¨
        try:
            if hasattr(model, 'predict_proba'):
                explainer = shap.TreeExplainer(model)
            else:
                explainer = shap.KernelExplainer(model.predict, X_train.sample(50))

            # è®¡ç®—SHAPå€¼
            shap_values = explainer.shap_values(X_test_sample)

            # å¦‚æœæ˜¯å¤šåˆ†ç±»ï¼Œå–ç¬¬ä¸€ä¸ªç±»çš„SHAPå€¼
            if isinstance(shap_values, list):
                shap_values = shap_values[1] if len(shap_values) > 1 else shap_values[0]

            # å…¨å±€ç‰¹å¾é‡è¦æ€§
            feature_importance = np.abs(shap_values).mean(0)
            importance_df = pd.DataFrame({
                'feature': X_test_sample.columns,
                'shap_importance': feature_importance
            }).sort_values('shap_importance', ascending=False)

            results = {
                'explainer': explainer,
                'shap_values': shap_values,
                'feature_importance': importance_df,
                'sample_data': X_test_sample
            }

            print(f"   âœ“ SHAPè§£é‡Šç”Ÿæˆå®Œæˆ (æ ·æœ¬æ•°: {len(X_test_sample)})")
            return results

        except Exception as e:
            print(f"   âŒ SHAPè§£é‡Šç”Ÿæˆå¤±è´¥: {str(e)}")
            return {}

    def compare_models(self, model_results: Dict) -> Dict:
        """
        æ¯”è¾ƒæ¨¡å‹æ€§èƒ½

        Parameters:
        - model_results: æ¨¡å‹è®­ç»ƒç»“æœå­—å…¸

        Returns:
        - æ¨¡å‹æ¯”è¾ƒç»“æœ
        """
        if not model_results:
            return {}

        # åˆ›å»ºæ¯”è¾ƒDataFrame
        comparison_data = []

        for model_name, result in model_results.items():
            row = {
                'model': model_name,
                'cv_mean': result['cv_mean'],
                'cv_std': result['cv_std']
            }

            # æ·»åŠ ä¸»è¦æŒ‡æ ‡
            metrics = result['metrics']
            if 'accuracy' in metrics:
                row['accuracy'] = metrics['accuracy']
            if 'f1' in metrics:
                row['f1'] = metrics['f1']
            if 'auc' in metrics:
                row['auc'] = metrics['auc']
            if 'r2' in metrics:
                row['r2'] = metrics['r2']

            comparison_data.append(row)

        comparison_df = pd.DataFrame(comparison_data)

        # æ’åº
        if 'accuracy' in comparison_df.columns:
            comparison_df = comparison_df.sort_values('accuracy', ascending=False)
        elif 'r2' in comparison_df.columns:
            comparison_df = comparison_df.sort_values('r2', ascending=False)

        # æ’å
        comparison_df['rank'] = range(1, len(comparison_df) + 1)

        results = {
            'comparison_df': comparison_df,
            'best_model': comparison_df.iloc[0]['model'],
            'ranking': comparison_df[['model', 'rank']].to_dict('records')
        }

        return results

    def predict_new_data(self, new_data: pd.DataFrame, model_name: Optional[str] = None) -> Dict:
        """
        å¯¹æ–°æ•°æ®è¿›è¡Œé¢„æµ‹

        Parameters:
        - new_data: æ–°æ•°æ®
        - model_name: ä½¿ç”¨çš„æ¨¡å‹åç§°ï¼ˆé»˜è®¤ä½¿ç”¨æœ€ä½³æ¨¡å‹ï¼‰

        Returns:
        - é¢„æµ‹ç»“æœ
        """
        if model_name is None:
            if self.best_model is None:
                raise ValueError("æ²¡æœ‰å¯ç”¨çš„æ¨¡å‹ï¼Œè¯·å…ˆè®­ç»ƒæ¨¡å‹")
            model = self.best_model
            model_name = self.best_model_name
        else:
            if model_name not in self.models:
                raise ValueError(f"æ¨¡å‹ '{model_name}' ä¸å­˜åœ¨")
            model = self.models[model_name]['model']

        # ç¡®ä¿ç‰¹å¾é¡ºåºä¸€è‡´
        feature_names = self.models[model_name]['feature_names']
        if set(new_data.columns) != set(feature_names):
            missing_features = set(feature_names) - set(new_data.columns)
            extra_features = set(new_data.columns) - set(feature_names)

            if missing_features:
                raise ValueError(f"ç¼ºå°‘ç‰¹å¾: {missing_features}")
            if extra_features:
                print(f"âš ï¸ å¤šä½™ç‰¹å¾å°†è¢«å¿½ç•¥: {extra_features}")

        # é‡æ–°æ’åºåˆ—
        new_data_aligned = new_data[feature_names]

        # é¢„æµ‹
        predictions = model.predict(new_data_aligned)

        results = {
            'model_name': model_name,
            'predictions': predictions,
            'input_data': new_data_aligned
        }

        # æ·»åŠ æ¦‚ç‡é¢„æµ‹ï¼ˆå¦‚æœæ”¯æŒï¼‰
        if hasattr(model, 'predict_proba'):
            predictions_proba = model.predict_proba(new_data_aligned)
            results['predictions_proba'] = predictions_proba

        return results

    def generate_model_report(self, output_path: str, include_plots: bool = True):
        """
        ç”Ÿæˆæ¨¡å‹æŠ¥å‘Š

        Parameters:
        - output_path: è¾“å‡ºè·¯å¾„
        - include_plots: æ˜¯å¦åŒ…å«å›¾è¡¨
        """
        if not self.model_results:
            print("âŒ æ²¡æœ‰æ¨¡å‹ç»“æœå¯ä»¥ç”ŸæˆæŠ¥å‘Š")
            return

        report_data = {
            'timestamp': pd.Timestamp.now().isoformat(),
            'config': self.config,
            'best_model': self.best_model_name,
            'model_count': len(self.model_results),
            'model_results': {}
        }

        # è½¬æ¢æ¨¡å‹ç»“æœä¸ºå¯åºåˆ—åŒ–æ ¼å¼
        for model_name, result in self.model_results.items():
            model_data = {
                'cv_mean': float(result['cv_mean']),
                'cv_std': float(result['cv_std']),
                'best_params': result['best_params'],
                'metrics': {}
            }

            # è½¬æ¢æŒ‡æ ‡
            for metric, value in result['metrics'].items():
                if isinstance(value, (int, float, np.number)):
                    model_data['metrics'][metric] = float(value)
                elif isinstance(value, str):
                    model_data['metrics'][metric] = value

            report_data['model_results'][model_name] = model_data

        # ä¿å­˜æŠ¥å‘Š
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(report_data, f, ensure_ascii=False, indent=2)

        print(f"âœ… æ¨¡å‹æŠ¥å‘Šå·²ä¿å­˜åˆ° {output_path}")

        # ç”Ÿæˆå›¾è¡¨
        if include_plots:
            self._generate_model_plots(output_path.replace('.json', '_plots.png'))

    def _generate_model_plots(self, output_path: str):
        """ç”Ÿæˆæ¨¡å‹æ¯”è¾ƒå›¾è¡¨"""
        if not self.model_results:
            return

        # å‡†å¤‡æ•°æ®
        model_names = []
        cv_means = []
        cv_stds = []

        for model_name, result in self.model_results.items():
            model_names.append(model_name)
            cv_means.append(result['cv_mean'])
            cv_stds.append(result['cv_std'])

        # åˆ›å»ºå›¾è¡¨
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))

        # æ¨¡å‹æ€§èƒ½æ¯”è¾ƒ
        bars = ax1.bar(model_names, cv_means, yerr=cv_stds, capsize=5)
        ax1.set_title('æ¨¡å‹æ€§èƒ½æ¯”è¾ƒ (CV Score)', fontsize=14)
        ax1.set_ylabel('CV Score')
        ax1.tick_params(axis='x', rotation=45)

        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for bar, mean in zip(bars, cv_means):
            height = bar.get_height()
            ax1.text(bar.get_x() + bar.get_width()/2., height + max(cv_stds)*0.1,
                    f'{mean:.3f}', ha='center', va='bottom')

        # æ¨¡å‹ç¨³å®šæ€§æ¯”è¾ƒ
        ax2.bar(model_names, cv_stds, color='orange', alpha=0.7)
        ax2.set_title('æ¨¡å‹ç¨³å®šæ€§æ¯”è¾ƒ (CV Std)', fontsize=14)
        ax2.set_ylabel('CV Std')
        ax2.tick_params(axis='x', rotation=45)

        plt.tight_layout()
        plt.savefig(output_path, dpi=300, bbox_inches='tight')
        plt.close()

        print(f"âœ… æ¨¡å‹æ¯”è¾ƒå›¾è¡¨å·²ä¿å­˜åˆ° {output_path}")

    def save_models(self, output_dir: str):
        """
        ä¿å­˜æ‰€æœ‰æ¨¡å‹

        Parameters:
        - output_dir: è¾“å‡ºç›®å½•
        """
        output_path = Path(output_dir)
        output_path.mkdir(exist_ok=True)

        # ä¿å­˜æ¯ä¸ªæ¨¡å‹
        for model_name, result in self.model_results.items():
            model_path = output_path / f"{model_name}.pkl"
            joblib.dump(result['model'], model_path)

        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if self.best_model:
            joblib.dump(self.best_model, output_path / "best_model.pkl")

        # ä¿å­˜æ¨¡å‹å…ƒæ•°æ®
        metadata = {
            'best_model_name': self.best_model_name,
            'model_names': list(self.model_results.keys()),
            'config': self.config
        }

        with open(output_path / 'models_metadata.json', 'w', encoding='utf-8') as f:
            json.dump(metadata, f, ensure_ascii=False, indent=2)

        print(f"âœ… æ¨¡å‹å·²ä¿å­˜åˆ° {output_dir}")

    def load_models(self, input_dir: str):
        """
        åŠ è½½æ¨¡å‹

        Parameters:
        - input_dir: è¾“å…¥ç›®å½•
        """
        input_path = Path(input_dir)

        # åŠ è½½å…ƒæ•°æ®
        with open(input_path / 'models_metadata.json', 'r', encoding='utf-8') as f:
            metadata = json.load(f)

        self.best_model_name = metadata['best_model_name']

        # åŠ è½½æœ€ä½³æ¨¡å‹
        best_model_path = input_path / "best_model.pkl"
        if best_model_path.exists():
            self.best_model = joblib.load(best_model_path)

        print(f"âœ… æ¨¡å‹å·²ä» {input_dir} åŠ è½½")
        print(f"   æœ€ä½³æ¨¡å‹: {self.best_model_name}")

    def get_model_summary(self) -> Dict:
        """
        è·å–æ¨¡å‹æ‘˜è¦ä¿¡æ¯

        Returns:
        - æ¨¡å‹æ‘˜è¦
        """
        summary = {
            'trained_models': list(self.model_results.keys()),
            'best_model': self.best_model_name,
            'model_count': len(self.model_results),
            'config': self.config
        }

        if self.model_results:
            # æ€§èƒ½ç»Ÿè®¡
            cv_scores = [result['cv_mean'] for result in self.model_results.values()]
            summary['performance_stats'] = {
                'best_cv_score': max(cv_scores),
                'worst_cv_score': min(cv_scores),
                'mean_cv_score': np.mean(cv_scores),
                'std_cv_score': np.std(cv_scores)
            }

            # æœ€ä½³æ¨¡å‹è¯¦ç»†ä¿¡æ¯
            if self.best_model_name in self.model_results:
                best_result = self.model_results[self.best_model_name]
                summary['best_model_details'] = {
                    'cv_mean': float(best_result['cv_mean']),
                    'cv_std': float(best_result['cv_std']),
                    'metrics': best_result['metrics'],
                    'best_params': best_result['best_params']
                }

        return summary