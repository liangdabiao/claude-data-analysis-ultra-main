#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Core Regression Analysis Engine
Comprehensive regression modeling with multiple algorithms and automated feature engineering
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.preprocessing import StandardScaler, LabelEncoder, PolynomialFeatures
from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error, mean_absolute_percentage_error
import warnings
warnings.filterwarnings('ignore')

class RegressionAnalyzer:
    """Comprehensive regression analysis engine with multiple algorithms"""

    def __init__(self, random_state=42, chinese_font='SimHei'):
        """Initialize the regression analyzer"""
        self.random_state = random_state
        self.chinese_font = chinese_font
        self.models = {}
        self.scalers = {}
        self.feature_names = None
        self.target_name = None
        self.best_model = None
        self.best_model_name = None

    def load_and_validate_data(self, file_path, target_column, **kwargs):
        """
        Load and validate data for regression analysis

        Args:
            file_path (str): Path to the CSV file
            target_column (str): Name of the target variable column
            **kwargs: Additional parameters for pd.read_csv()

        Returns:
            tuple: (X, y) features and target
        """
        print("=== æ•°æ®åŠ è½½ä¸éªŒè¯ ===")

        # Load data with different encodings
        try:
            df = pd.read_csv(file_path, encoding='utf-8', **kwargs)
        except UnicodeDecodeError:
            try:
                df = pd.read_csv(file_path, encoding='gbk', **kwargs)
            except:
                df = pd.read_csv(file_path, encoding='latin-1', **kwargs)

        print(f"æ•°æ®é›†å½¢çŠ¶: {df.shape}")
        print(f"åˆ—å: {list(df.columns)}")

        # Check if target column exists
        if target_column not in df.columns:
            # Try to find similar column names
            possible_targets = [col for col in df.columns
                              if target_column.lower() in col.lower() or
                                 col.lower() in target_column.lower()]
            if possible_targets:
                target_column = possible_targets[0]
                print(f"è‡ªåŠ¨è¯†åˆ«ç›®æ ‡åˆ—: {target_column}")
            else:
                raise ValueError(f"æ‰¾ä¸åˆ°ç›®æ ‡åˆ—: {target_column}")

        # Separate features and target
        X = df.drop(columns=[target_column])
        y = df[target_column]

        self.feature_names = list(X.columns)
        self.target_name = target_column

        # Data validation
        print(f"\næ•°æ®è´¨é‡æ£€æŸ¥:")
        print(f"- ç¼ºå¤±å€¼: {X.isnull().sum().sum()}")
        print(f"- ç›®æ ‡å˜é‡ç¼ºå¤±å€¼: {y.isnull().sum()}")
        print(f"- ç‰¹å¾æ•°é‡: {X.shape[1]}")
        print(f"- æ ·æœ¬æ•°é‡: {X.shape[0]}")

        return X, y

    def preprocess_data(self, X, y, handle_missing='auto', handle_outliers='iqr'):
        """
        Preprocess data with missing value handling and outlier detection

        Args:
            X (pd.DataFrame): Features
            y (pd.Series): Target
            handle_missing (str): How to handle missing values
            handle_outliers (str): How to handle outliers

        Returns:
            tuple: (X_processed, y_processed)
        """
        print("\n=== æ•°æ®é¢„å¤„ç† ===")

        X_processed = X.copy()
        y_processed = y.copy()

        # Handle missing values
        if handle_missing == 'auto':
            # Numerical columns: median imputation
            numerical_cols = X_processed.select_dtypes(include=[np.number]).columns
            for col in numerical_cols:
                if X_processed[col].isnull().sum() > 0:
                    median_val = X_processed[col].median()
                    X_processed[col].fillna(median_val, inplace=True)
                    print(f"- {col}: ç”¨ä¸­ä½æ•° {median_val:.2f} å¡«å…… {X_processed[col].isnull().sum()} ä¸ªç¼ºå¤±å€¼")

            # Categorical columns: mode imputation
            categorical_cols = X_processed.select_dtypes(include=['object']).columns
            for col in categorical_cols:
                if X_processed[col].isnull().sum() > 0:
                    mode_val = X_processed[col].mode()[0] if len(X_processed[col].mode()) > 0 else 'Unknown'
                    X_processed[col].fillna(mode_val, inplace=True)
                    print(f"- {col}: ç”¨ä¼—æ•° '{mode_val}' å¡«å…… {X_processed[col].isnull().sum()} ä¸ªç¼ºå¤±å€¼")

        # Remove rows with missing target
        missing_target = y_processed.isnull().sum()
        if missing_target > 0:
            mask = ~y_processed.isnull()
            X_processed = X_processed[mask]
            y_processed = y_processed[mask]
            print(f"- ç§»é™¤ {missing_target} è¡Œç›®æ ‡å˜é‡ç¼ºå¤±å€¼")

        # Handle outliers in target variable
        if handle_outliers == 'iqr' and y_processed.dtype in [np.number, 'int64', 'float64']:
            Q1 = y_processed.quantile(0.25)
            Q3 = y_processed.quantile(0.75)
            IQR = Q3 - Q1
            lower_bound = Q1 - 1.5 * IQR
            upper_bound = Q3 + 1.5 * IQR

            outlier_mask = (y_processed >= lower_bound) & (y_processed <= upper_bound)
            outliers_removed = len(y_processed) - outlier_mask.sum()

            if outliers_removed > 0:
                X_processed = X_processed[outlier_mask]
                y_processed = y_processed[outlier_mask]
                print(f"- ç§»é™¤ {outliers_removed} ä¸ªå¼‚å¸¸å€¼ (IQRæ–¹æ³•)")

        print(f"é¢„å¤„ç†åæ•°æ®å½¢çŠ¶: {X_processed.shape}")
        return X_processed, y_processed

    def encode_categorical_features(self, X):
        """
        Encode categorical features to numerical

        Args:
            X (pd.DataFrame): Features with categorical columns

        Returns:
            pd.DataFrame: Features with encoded categorical variables
        """
        print("\n=== åˆ†ç±»ç‰¹å¾ç¼–ç  ===")

        X_encoded = X.copy()
        categorical_cols = X_encoded.select_dtypes(include=['object']).columns

        if len(categorical_cols) > 0:
            print(f"å‘ç° {len(categorical_cols)} ä¸ªåˆ†ç±»ç‰¹å¾:")

            label_encoders = {}

            for col in categorical_cols:
                # Use label encoding for high-cardinality features
                if X_encoded[col].nunique() > 10:
                    le = LabelEncoder()
                    X_encoded[col] = le.fit_transform(X_encoded[col].astype(str))
                    label_encoders[col] = le
                    print(f"- {col}: æ ‡ç­¾ç¼–ç  ({X_encoded[col].nunique()} ä¸ªç±»åˆ«)")
                else:
                    # Use one-hot encoding for low-cardinality features
                    dummies = pd.get_dummies(X_encoded[col], prefix=col, drop_first=True)
                    X_encoded = pd.concat([X_encoded.drop(columns=[col]), dummies], axis=1)
                    print(f"- {col}: ç‹¬çƒ­ç¼–ç  ({X_encoded[col].nunique()} ä¸ªç±»åˆ«)")

            self.label_encoders = label_encoders

        self.feature_names = list(X_encoded.columns)
        print(f"ç¼–ç åç‰¹å¾æ•°é‡: {len(self.feature_names)}")

        return X_encoded

    def create_interaction_features(self, X):
        """
        Create interaction features for improved model performance

        Args:
            X (pd.DataFrame): Original features

        Returns:
            pd.DataFrame: Features with interactions
        """
        print("\n=== äº¤äº’ç‰¹å¾ç”Ÿæˆ ===")

        X_interactions = X.copy()
        numerical_cols = X.select_dtypes(include=[np.number]).columns

        # Create pairwise interactions for top correlated features
        if len(numerical_cols) >= 2:
            # Calculate correlation matrix
            corr_matrix = X[numerical_cols].corr().abs()

            # Find top correlated pairs (excluding self-correlation)
            top_pairs = []
            for i in range(len(corr_matrix.columns)):
                for j in range(i+1, len(corr_matrix.columns)):
                    corr_val = corr_matrix.iloc[i, j]
                    if corr_val > 0.3:  # Threshold for creating interaction
                        col1, col2 = corr_matrix.columns[i], corr_matrix.columns[j]
                        top_pairs.append((col1, col2, corr_val))

            # Sort by correlation and take top 5
            top_pairs.sort(key=lambda x: x[2], reverse=True)
            top_pairs = top_pairs[:5]

            for col1, col2, corr in top_pairs:
                interaction_name = f"{col1}_x_{col2}"
                X_interactions[interaction_name] = X[col1] * X[col2]
                print(f"- åˆ›å»ºäº¤äº’ç‰¹å¾: {interaction_name} (ç›¸å…³ç³»æ•°: {corr:.3f})")

        # Create polynomial features for important numerical features
        if len(numerical_cols) > 0:
            # Select features with highest variance
            variances = X[numerical_cols].var()
            top_variance_features = variances.nlargest(3).index

            for col in top_variance_features:
                squared_name = f"{col}_squared"
                X_interactions[squared_name] = X[col] ** 2
                print(f"- åˆ›å»ºå¹³æ–¹ç‰¹å¾: {squared_name}")

        print(f"äº¤äº’ç‰¹å¾ç”Ÿæˆå®Œæˆï¼Œæ€»ç‰¹å¾æ•°: {X_interactions.shape[1]}")
        return X_interactions

    def train_models(self, X, y, test_size=0.2, cv_folds=5):
        """
        Train multiple regression models and compare performance

        Args:
            X (pd.DataFrame): Features
            y (pd.Series): Target
            test_size (float): Test set proportion
            cv_folds (int): Cross-validation folds

        Returns:
            dict: Model performance results
        """
        print("\n=== æ¨¡å‹è®­ç»ƒä¸è¯„ä¼° ===")

        # Split data
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=test_size, random_state=self.random_state
        )

        print(f"è®­ç»ƒé›†å¤§å°: {X_train.shape[0]}")
        print(f"æµ‹è¯•é›†å¤§å°: {X_test.shape[0]}")

        # Define models
        models = {
            'Linear Regression': LinearRegression(),
            'Ridge Regression': Ridge(alpha=1.0, random_state=self.random_state),
            'Lasso Regression': Lasso(alpha=1.0, random_state=self.random_state),
            'Decision Tree': DecisionTreeRegressor(random_state=self.random_state),
            'Random Forest': RandomForestRegressor(n_estimators=100, random_state=self.random_state),
            'Gradient Boosting': GradientBoostingRegressor(random_state=self.random_state)
        }

        # Scale features for linear models
        scaler = StandardScaler()
        X_train_scaled = scaler.fit_transform(X_train)
        X_test_scaled = scaler.transform(X_test)

        results = {}

        for name, model in models.items():
            print(f"\n--- {name} ---")

            # Choose appropriate data scaling
            if 'Linear' in name or 'Ridge' in name or 'Lasso' in name:
                X_tr, X_te = X_train_scaled, X_test_scaled
            else:
                X_tr, X_te = X_train, X_test

            # Train model
            model.fit(X_tr, y_train)

            # Make predictions
            y_train_pred = model.predict(X_tr)
            y_test_pred = model.predict(X_te)

            # Calculate metrics
            train_r2 = r2_score(y_train, y_train_pred)
            test_r2 = r2_score(y_test, y_test_pred)
            train_mae = mean_absolute_error(y_train, y_train_pred)
            test_mae = mean_absolute_error(y_test, y_test_pred)
            train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))
            test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))

            # Cross-validation
            cv_scores = cross_val_score(model, X_tr, y_train, cv=cv_folds, scoring='r2')

            # Store results
            results[name] = {
                'model': model,
                'scaler': scaler if 'Linear' in name or 'Ridge' in name or 'Lasso' in name else None,
                'train_r2': train_r2,
                'test_r2': test_r2,
                'train_mae': train_mae,
                'test_mae': test_mae,
                'train_rmse': train_rmse,
                'test_rmse': test_rmse,
                'cv_mean': cv_scores.mean(),
                'cv_std': cv_scores.std(),
                'predictions': y_test_pred,
                'y_test': y_test
            }

            # Store model and scaler
            self.models[name] = model
            if 'Linear' in name or 'Ridge' in name or 'Lasso' in name:
                self.scalers[name] = scaler

            # Print results
            print(f"è®­ç»ƒé›† RÂ²: {train_r2:.4f}")
            print(f"æµ‹è¯•é›† RÂ²: {test_r2:.4f}")
            print(f"æµ‹è¯•é›† MAE: {test_mae:.4f}")
            print(f"æµ‹è¯•é›† RMSE: {test_rmse:.4f}")
            print(f"äº¤å‰éªŒè¯ RÂ²: {cv_scores.mean():.4f} (Â±{cv_scores.std():.4f})")

        # Select best model based on test RÂ²
        best_model_name = max(results.keys(), key=lambda k: results[k]['test_r2'])
        self.best_model = results[best_model_name]['model']
        self.best_model_name = best_model_name

        print(f"\nğŸ† æœ€ä½³æ¨¡å‹: {best_model_name} (æµ‹è¯•é›† RÂ²: {results[best_model_name]['test_r2']:.4f})")

        self.results = results
        return results

    def get_feature_importance(self, model_name=None, top_n=10):
        """
        Get feature importance from trained models

        Args:
            model_name (str): Name of the model (use best model if None)
            top_n (int): Number of top features to return

        Returns:
            pd.DataFrame: Feature importance with rankings
        """
        if model_name is None:
            model_name = self.best_model_name

        if model_name not in self.models:
            raise ValueError(f"Model '{model_name}' not found in trained models")

        model = self.models[model_name]

        # Get feature importance based on model type
        if hasattr(model, 'feature_importances_'):
            # Tree-based models
            importance = model.feature_importances_
        elif hasattr(model, 'coef_'):
            # Linear models
            importance = np.abs(model.coef_)
        else:
            raise ValueError(f"Model '{model_name}' does not support feature importance")

        # Create importance DataFrame
        feature_importance = pd.DataFrame({
            'feature': self.feature_names,
            'importance': importance
        })

        # Sort and rank
        feature_importance = feature_importance.sort_values('importance', ascending=False)
        feature_importance['rank'] = range(1, len(feature_importance) + 1)
        feature_importance['importance_pct'] = (feature_importance['importance'] /
                                               feature_importance['importance'].sum() * 100)

        return feature_importance.head(top_n)

    def predict(self, X, model_name=None):
        """
        Make predictions using trained model

        Args:
            X (pd.DataFrame): Features for prediction
            model_name (str): Model to use (best model if None)

        Returns:
            np.array: Predictions
        """
        if model_name is None:
            model_name = self.best_model_name

        if model_name not in self.models:
            raise ValueError(f"Model '{model_name}' not found in trained models")

        model = self.models[model_name]

        # Apply scaling if needed
        if model_name in self.scalers:
            X_scaled = self.scalers[model_name].transform(X)
            return model.predict(X_scaled)
        else:
            return model.predict(X)

    def run_complete_analysis(self, file_path, target_column,
                            create_interactions=False, **kwargs):
        """
        Run complete regression analysis pipeline

        Args:
            file_path (str): Path to data file
            target_column (str): Target variable column
            create_interactions (bool): Whether to create interaction features
            **kwargs: Additional parameters

        Returns:
            dict: Complete analysis results
        """
        print("ğŸš€ å¼€å§‹å®Œæ•´å›å½’åˆ†æ")
        print("=" * 50)

        # 1. Load and validate data
        X, y = self.load_and_validate_data(file_path, target_column, **kwargs)

        # 2. Preprocess data
        X_processed, y_processed = self.preprocess_data(X, y)

        # 3. Handle categorical features
        X_encoded = self.encode_categorical_features(X_processed)

        # 4. Create interaction features if requested
        if create_interactions:
            X_final = self.create_interaction_features(X_encoded)
        else:
            X_final = X_encoded

        # 6. Train models
        results = self.train_models(X_final, y_processed)

        # 7. Get feature importance
        feature_importance = self.get_feature_importance()

        # 8. Create summary
        summary = {
            'data_shape': X_final.shape,
            'feature_count': X_final.shape[1],
            'sample_count': len(y_processed),
            'best_model': self.best_model_name,
            'best_r2': results[self.best_model_name]['test_r2'],
            'best_mae': results[self.best_model_name]['test_mae'],
            'feature_importance': feature_importance
        }

        print(f"\nâœ… åˆ†æå®Œæˆï¼")
        print(f"æœ€ä½³æ¨¡å‹: {self.best_model_name}")
        print(f"æµ‹è¯•é›† RÂ²: {summary['best_r2']:.4f}")
        print(f"æµ‹è¯•é›† MAE: {summary['best_mae']:.4f}")

        return {
            'results': results,
            'summary': summary,
            'X_final': X_final,
            'y_final': y_processed,
            'feature_importance': feature_importance
        }

def main():
    """Example usage"""
    analyzer = RegressionAnalyzer()

    # Example with housing price data
    file_path = "æˆ¿ä»·é¢„æµ‹æ•°æ®.csv"
    target_column = "æˆ¿ä»·"

    if pd.io.common.file_exists(file_path):
        analysis_results = analyzer.run_complete_analysis(
            file_path,
            target_column,
            create_interactions=True
        )

        print(f"\nç‰¹å¾é‡è¦æ€§æ’å:")
        print(analysis_results['feature_importance'])

    else:
        print(f"æ•°æ®æ–‡ä»¶æœªæ‰¾åˆ°: {file_path}")
        print("è¯·æä¾›æ­£ç¡®çš„CSVæ•°æ®æ–‡ä»¶è·¯å¾„")

if __name__ == "__main__":
    main()