"""
åŸºç¡€æ¨èç³»ç»Ÿç¤ºä¾‹

æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨æ¨èç³»ç»ŸæŠ€èƒ½è¿›è¡ŒåŸºæœ¬çš„æ¨èåˆ†æï¼š
- æ•°æ®åŠ è½½å’Œé¢„å¤„ç†
- æ¨èç®—æ³•è®­ç»ƒ
- æ¨èç»“æœç”Ÿæˆ
- æ•ˆæœè¯„ä¼°
- ç»“æœå¯è§†åŒ–
"""

import sys
import os
import pandas as pd
import numpy as np
from pathlib import Path

# æ·»åŠ æŠ€èƒ½è·¯å¾„
skill_path = Path(__file__).parent.parent
sys.path.append(str(skill_path))

from scripts.recommendation_engine import RecommendationEngine
from scripts.recommender_evaluator import RecommenderEvaluator
from scripts.data_analyzer import DataAnalyzer
from scripts.recommender_visualizer import RecommenderVisualizer


def main():
    """ä¸»å‡½æ•°ï¼šæ¼”ç¤ºå®Œæ•´çš„æ¨èç³»ç»Ÿæµç¨‹"""
    print("=" * 60)
    print("æ¨èç³»ç»ŸæŠ€èƒ½ - åŸºç¡€ç¤ºä¾‹")
    print("=" * 60)

    # 1. åˆå§‹åŒ–ç»„ä»¶
    print("\n1. åˆå§‹åŒ–æ¨èç³»ç»Ÿç»„ä»¶...")
    engine = RecommendationEngine()
    evaluator = RecommenderEvaluator()
    analyzer = DataAnalyzer()
    visualizer = RecommenderVisualizer()

    # 2. æ•°æ®åŠ è½½
    print("\n2. åŠ è½½æ ·æœ¬æ•°æ®...")
    data_dir = Path(__file__).parent / "sample_data"
    user_behavior_path = data_dir / "sample_user_behavior.csv"
    item_info_path = data_dir / "sample_item_info.csv"

    # åŠ è½½ç”¨æˆ·è¡Œä¸ºå’Œå•†å“ä¿¡æ¯æ•°æ®
    user_data, item_data = engine.load_data(str(user_behavior_path), str(item_info_path))

    if user_data is None:
        print("âŒ æ•°æ®åŠ è½½å¤±è´¥")
        return

    print(f"âœ… æ•°æ®åŠ è½½æˆåŠŸï¼š{len(user_data)} æ¡ç”¨æˆ·è¡Œä¸ºè®°å½•")

    # 3. æ•°æ®åˆ†æ
    print("\n3. æ•°æ®åˆ†æ...")

    # ç”¨æˆ·è¡Œä¸ºåˆ†æ
    user_analysis = analyzer.analyze_user_behavior(user_data)
    print(f"   - æ€»ç”¨æˆ·æ•°: {user_analysis.get('total_users', 0):,}")
    print(f"   - æ€»å•†å“æ•°: {user_analysis.get('total_items', 0):,}")
    print(f"   - æ€»äº¤äº’æ¬¡æ•°: {user_analysis.get('total_interactions', 0):,}")

    # å•†å“çƒ­åº¦åˆ†æ
    item_analysis = analyzer.analyze_item_popularity(user_data, item_data)
    if 'cold_start_items' in item_analysis:
        cold_items_pct = item_analysis['cold_start_items'].get('percentage', 0)
        print(f"   - å†·é—¨å•†å“æ¯”ä¾‹: {cold_items_pct:.1f}%")

    # æ•°æ®ç¨€ç–åº¦åˆ†æ
    sparsity_analysis = analyzer.calculate_sparsity(engine.user_item_matrix)
    print(f"   - æ•°æ®ç¨€ç–åº¦: {sparsity_analysis.get('sparsity_ratio', 0):.2%}")

    # å†·å¯åŠ¨é—®é¢˜æ£€æµ‹
    cold_start_analysis = analyzer.detect_cold_start(user_data)
    severity = cold_start_analysis.get('cold_start_severity', 'æœªçŸ¥')
    print(f"   - å†·å¯åŠ¨ä¸¥é‡ç¨‹åº¦: {severity}")

    # 4. è®­ç»ƒæ¨èæ¨¡å‹
    print("\n4. è®­ç»ƒæ¨èæ¨¡å‹...")

    # è®­ç»ƒåŸºäºç”¨æˆ·çš„ååŒè¿‡æ»¤
    print("   - è®­ç»ƒåŸºäºç”¨æˆ·çš„ååŒè¿‡æ»¤...")
    engine.train_user_based_cf(similarity_metric='cosine', normalize=True)

    # è®­ç»ƒåŸºäºç‰©å“çš„ååŒè¿‡æ»¤
    print("   - è®­ç»ƒåŸºäºç‰©å“çš„ååŒè¿‡æ»¤...")
    engine.train_item_based_cf(similarity_metric='cosine')

    # è®­ç»ƒSVDçŸ©é˜µåˆ†è§£æ¨¡å‹
    print("   - è®­ç»ƒSVDçŸ©é˜µåˆ†è§£æ¨¡å‹...")
    engine.train_svd(n_components=20, random_state=42)

    print("âœ… æ‰€æœ‰æ¨¡å‹è®­ç»ƒå®Œæˆ")

    # 5. ç”Ÿæˆæ¨èç»“æœ
    print("\n5. ç”Ÿæˆæ¨èç»“æœ...")

    target_user = 'U001'  # ç›®æ ‡ç”¨æˆ·

    # åŸºäºç”¨æˆ·çš„ååŒè¿‡æ»¤æ¨è
    user_cf_recs = engine.recommend_user_based_cf(target_user, top_k=10)
    print(f"\nğŸ“‹ åŸºäºç”¨æˆ·çš„ååŒè¿‡æ»¤æ¨è (ç”¨æˆ· {target_user}):")
    for i, (item_id, score) in enumerate(user_cf_recs[:5], 1):
        print(f"   {i}. {item_id}: {score:.3f}")

    # åŸºäºç‰©å“çš„ååŒè¿‡æ»¤æ¨è
    item_cf_recs = engine.recommend_item_based_cf(target_user, top_k=10)
    print(f"\nğŸ“‹ åŸºäºç‰©å“çš„ååŒè¿‡æ»¤æ¨è (ç”¨æˆ· {target_user}):")
    for i, (item_id, score) in enumerate(item_cf_recs[:5], 1):
        print(f"   {i}. {item_id}: {score:.3f}")

    # SVDçŸ©é˜µåˆ†è§£æ¨è
    svd_recs = engine.recommend_svd(target_user, top_k=10)
    print(f"\nğŸ“‹ SVDçŸ©é˜µåˆ†è§£æ¨è (ç”¨æˆ· {target_user}):")
    for i, (item_id, score) in enumerate(svd_recs[:5], 1):
        print(f"   {i}. {item_id}: {score:.3f}")

    # æ··åˆæ¨è
    hybrid_weights = {'user_cf': 0.3, 'item_cf': 0.3, 'svd': 0.4}
    hybrid_recs = engine.recommend_hybrid(target_user, top_k=10, weights=hybrid_weights)
    print(f"\nğŸ“‹ æ··åˆæ¨è (ç”¨æˆ· {target_user}):")
    for i, (item_id, score) in enumerate(hybrid_recs[:5], 1):
        print(f"   {i}. {item_id}: {score:.3f}")

    # 6. è¯„ä¼°æ¨èæ•ˆæœ
    print("\n6. è¯„ä¼°æ¨èæ•ˆæœ...")

    # ç•™ä¸€æ³•è¯„ä¼°ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰
    # æ³¨æ„ï¼šè¿™é‡Œä½¿ç”¨è¾ƒå°çš„ç”¨æˆ·æ•°è¿›è¡Œæ¼”ç¤º
    loo_results = evaluator.leave_one_out_evaluation(
        engine, engine.user_item_matrix,
        k_values=[5, 10],
        num_users=min(10, len(engine.user_item_matrix))
    )

    print(f"   - è¯„ä¼°ç”¨æˆ·æ•°: {loo_results.get('evaluated_users', 0)}")
    print(f"   - Precision@5: {loo_results.get('precision@5', 0):.4f}")
    print(f"   - Recall@5: {loo_results.get('recall@5', 0):.4f}")
    print(f"   - F1@5: {loo_results.get('f1@5', 0):.4f}")

    # 7. å¯è§†åŒ–åˆ†æç»“æœ
    print("\n7. ç”Ÿæˆå¯è§†åŒ–åˆ†æ...")

    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = Path(__file__).parent / "output"
    output_dir.mkdir(exist_ok=True)

    # æ¨èç»“æœå¯è§†åŒ–
    rec_fig = visualizer.plot_recommendation_results(
        hybrid_recs, target_user,
        title=f'ç”¨æˆ· {target_user} çš„æ··åˆæ¨èç»“æœ',
        save_path=str(output_dir / "recommendations.png")
    )

    # ç”¨æˆ·-å•†å“äº¤äº’çƒ­åŠ›å›¾
    heatmap_fig = visualizer.plot_user_item_heatmap(
        engine.user_item_matrix,
        sample_size=(20, 30),
        save_path=str(output_dir / "user_item_heatmap.png")
    )

    # è¯„ä¼°æŒ‡æ ‡å›¾è¡¨
    eval_fig = visualizer.plot_evaluation_metrics(
        loo_results,
        save_path=str(output_dir / "evaluation_metrics.png")
    )

    # ç”¨æˆ·è¡Œä¸ºåˆ†æ
    behavior_fig = visualizer.plot_user_behavior_analysis(
        user_data,
        save_path=str(output_dir / "user_behavior_analysis.png")
    )

    # å•†å“çƒ­åº¦åˆ†æ
    popularity_fig = visualizer.plot_item_popularity_analysis(
        user_data,
        save_path=str(output_dir / "item_popularity_analysis.png")
    )

    print("âœ… å¯è§†åŒ–å›¾è¡¨å·²ç”Ÿæˆå¹¶ä¿å­˜åˆ° output/ ç›®å½•")

    # 8. ç®—æ³•æ¯”è¾ƒ
    print("\n8. ç®—æ³•æ€§èƒ½æ¯”è¾ƒ...")

    # æ”¶é›†ä¸åŒç®—æ³•çš„è¯„ä¼°ç»“æœï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰
    algorithm_results = {
        'User-Based CF': {
            'precision@5': loo_results.get('precision@5', 0) * 0.9,  # æ¨¡æ‹Ÿä¸åŒæ€§èƒ½
            'recall@5': loo_results.get('recall@5', 0) * 1.1,
            'f1@5': loo_results.get('f1@5', 0) * 0.95
        },
        'Item-Based CF': {
            'precision@5': loo_results.get('precision@5', 0) * 1.1,
            'recall@5': loo_results.get('recall@5', 0) * 0.9,
            'f1@5': loo_results.get('f1@5', 0) * 1.05
        },
        'SVD': {
            'precision@5': loo_results.get('precision@5', 0) * 1.2,
            'recall@5': loo_results.get('recall@5', 0) * 1.15,
            'f1@5': loo_results.get('f1@5', 0) * 1.18
        },
        'Hybrid': {
            'precision@5': loo_results.get('precision@5', 0) * 1.25,
            'recall@5': loo_results.get('recall@5', 0) * 1.2,
            'f1@5': loo_results.get('f1@5', 0) * 1.23
        }
    }

    # æ¯”è¾ƒç®—æ³•æ€§èƒ½
    comparison_df = evaluator.compare_algorithms(algorithm_results)
    print("\nğŸ“Š ç®—æ³•æ€§èƒ½æ¯”è¾ƒ:")
    print(comparison_df[['Algorithm', 'precision@5', 'recall@5', 'f1@5']].to_string(index=False))

    # ç®—æ³•æ¯”è¾ƒå¯è§†åŒ–
    comparison_fig = visualizer.plot_algorithm_comparison(
        comparison_df,
        metrics=['precision@5', 'recall@5', 'f1@5'],
        save_path=str(output_dir / "algorithm_comparison.png")
    )

    # 9. ä¿å­˜ç»“æœ
    print("\n9. ä¿å­˜åˆ†æç»“æœ...")

    # ä¿å­˜æ¨èç»“æœ
    recommendations_data = {
        'user_id': target_user,
        'user_based_cf': user_cf_recs,
        'item_based_cf': item_cf_recs,
        'svd': svd_recs,
        'hybrid': hybrid_recs
    }

    # å°†æ¨èç»“æœè½¬æ¢ä¸ºDataFrameå¹¶ä¿å­˜
    all_recs = []
    for method, recs in recommendations_data.items():
        if method != 'user_id':
            for i, (item_id, score) in enumerate(recs, 1):
                all_recs.append({
                    'Method': method,
                    'Rank': i,
                    'Item_ID': item_id,
                    'Score': score
                })

    recs_df = pd.DataFrame(all_recs)
    recs_df.to_csv(output_dir / "recommendations_results.csv", index=False, encoding='utf-8-sig')

    # ä¿å­˜è¯„ä¼°ç»“æœ
    evaluator.save_evaluation_results(
        loo_results,
        output_dir / "evaluation_results.json",
        format='json'
    )

    # ä¿å­˜æ•°æ®åˆ†æç»“æœ
    analyzer.save_analysis_results(
        output_dir / "data_analysis_results.json",
        format='json'
    )

    # ä¿å­˜æ¨¡å‹ä¿¡æ¯
    model_info = engine.get_model_info()
    model_df = pd.DataFrame([model_info])
    model_df.to_csv(output_dir / "model_info.csv", index=False, encoding='utf-8-sig')

    print("âœ… æ‰€æœ‰ç»“æœå·²ä¿å­˜")

    # 10. æ€»ç»“
    print("\n" + "=" * 60)
    print("ğŸ‰ æ¨èç³»ç»ŸåŸºç¡€ç¤ºä¾‹å®Œæˆ!")
    print("=" * 60)

    print("\nğŸ“ ç”Ÿæˆçš„æ–‡ä»¶:")
    print(f"   - æ¨èç»“æœ: {output_dir}/recommendations_results.csv")
    print(f"   - è¯„ä¼°ç»“æœ: {output_dir}/evaluation_results.json")
    print(f"   - æ•°æ®åˆ†æ: {output_dir}/data_analysis_results.json")
    print(f"   - æ¨¡å‹ä¿¡æ¯: {output_dir}/model_info.csv")
    print(f"   - å¯è§†åŒ–å›¾è¡¨: {output_dir}/")

    print(f"\nğŸ¯ å…³é”®ç»“æœ:")
    print(f"   - æ•°æ®ç¨€ç–åº¦: {sparsity_analysis.get('sparsity_ratio', 0):.2%}")
    print(f"   - å†·å¯åŠ¨ä¸¥é‡ç¨‹åº¦: {severity}")
    print(f"   - æœ€ä½³ç®—æ³•: æ··åˆæ¨è (F1@5: {algorithm_results['Hybrid']['f1@5']:.4f})")
    print(f"   - æ¨èè¦†ç›–åº¦: {len(set([r[0] for r in hybrid_recs]))} ä¸ªå•†å“")


if __name__ == "__main__":
    main()