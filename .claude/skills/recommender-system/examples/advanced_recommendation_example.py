"""
é«˜çº§æ¨èç³»ç»Ÿç¤ºä¾‹

æ¼”ç¤ºæ¨èç³»ç»ŸæŠ€èƒ½çš„é«˜çº§åŠŸèƒ½ï¼š
- å¤šç§è¯„ä¼°æ–¹æ³•å¯¹æ¯”
- äº¤äº’å¼å¯è§†åŒ–
- ç”¨æˆ·ç”»åƒåˆ†æ
- å†·å¯åŠ¨ç­–ç•¥
- å®æ—¶æ¨èæ¼”ç¤º
"""

import sys
import os
import pandas as pd
import numpy as np
from pathlib import Path
import time
from datetime import datetime, timedelta

# æ·»åŠ æŠ€èƒ½è·¯å¾„
skill_path = Path(__file__).parent.parent
sys.path.append(str(skill_path))

from scripts.recommendation_engine import RecommendationEngine
from scripts.recommender_evaluator import RecommenderEvaluator
from scripts.data_analyzer import DataAnalyzer
from scripts.recommender_visualizer import RecommenderVisualizer


def simulate_real_time_recommendation(engine, user_id, new_ratings):
    """
    æ¨¡æ‹Ÿå®æ—¶æ¨èåœºæ™¯

    Args:
        engine: æ¨èå¼•æ“
        user_id: ç”¨æˆ·ID
        new_ratings: æ–°è¯„åˆ†åˆ—è¡¨ [(å•†å“ID, è¯„åˆ†), ...]
    """
    print(f"\nğŸ”„ æ¨¡æ‹Ÿå®æ—¶æ¨èåœºæ™¯ - ç”¨æˆ· {user_id}")

    # æ˜¾ç¤ºåŸå§‹æ¨è
    original_recs = engine.recommend_hybrid(user_id, top_k=5)
    print("åŸå§‹æ¨è:")
    for i, (item_id, score) in enumerate(original_recs, 1):
        print(f"   {i}. {item_id}: {score:.3f}")

    # æ¨¡æ‹Ÿç”¨æˆ·æ·»åŠ æ–°è¯„åˆ†
    print(f"\nğŸ“ ç”¨æˆ·æ·»åŠ äº† {len(new_ratings)} ä¸ªæ–°è¯„åˆ†:")
    for item_id, rating in new_ratings:
        print(f"   - {item_id}: {rating} åˆ†")

    # åœ¨å®é™…åº”ç”¨ä¸­ï¼Œè¿™é‡Œåº”è¯¥æ›´æ–°ç”¨æˆ·-å•†å“çŸ©é˜µå¹¶é‡æ–°è®¡ç®—ç›¸ä¼¼åº¦
    # ä¸ºäº†æ¼”ç¤ºï¼Œæˆ‘ä»¬ç®€å•åœ°æ¨¡æ‹Ÿæ¨èçš„å˜åŒ–
    print("\nğŸ”„ æ›´æ–°åçš„æ¨è:")
    updated_recs = engine.recommend_hybrid(user_id, top_k=5)
    for i, (item_id, score) in enumerate(updated_recs, 1):
        # æ¨¡æ‹Ÿæ¨èåˆ†æ•°çš„å˜åŒ–
        new_score = score + np.random.uniform(-0.1, 0.2)
        print(f"   {i}. {item_id}: {new_score:.3f}")


def demonstrate_cold_start_strategies(engine, evaluator):
    """
    æ¼”ç¤ºå†·å¯åŠ¨è§£å†³ç­–ç•¥

    Args:
        engine: æ¨èå¼•æ“
        evaluator: è¯„ä¼°å™¨
    """
    print("\nâ„ï¸ å†·å¯åŠ¨é—®é¢˜è§£å†³ç­–ç•¥æ¼”ç¤º")

    # 1. æ–°ç”¨æˆ·å†·å¯åŠ¨
    print("\n1. æ–°ç”¨æˆ·å†·å¯åŠ¨:")
    new_user_id = "NEW_U001"

    try:
        # å°è¯•ä¸ºæ–°ç”¨æˆ·ç”Ÿæˆæ¨è
        recommendations = engine.recommend_hybrid(new_user_id, top_k=5)
        if recommendations:
            print("   âœ… ä½¿ç”¨çƒ­é—¨å•†å“æ¨èç­–ç•¥æˆåŠŸ")
            for i, (item_id, score) in enumerate(recommendations, 1):
                print(f"      {i}. {item_id}: {score:.3f}")
    except Exception as e:
        print(f"   âŒ æ–°ç”¨æˆ·æ¨èå¤±è´¥: {str(e)}")

    # 2. æ–°å•†å“å†·å¯åŠ¨
    print("\n2. æ–°å•†å“å†·å¯åŠ¨:")
    new_item_id = "NEW_P001"
    print(f"   - æ–°å•†å“ {new_item_id} éœ€è¦é€šè¿‡åŸºäºå†…å®¹çš„æ¨èæˆ–æ¨å¹¿ç­–ç•¥")

    # 3. å†·å¯åŠ¨ç­–ç•¥å»ºè®®
    print("\n3. å†·å¯åŠ¨è§£å†³ç­–ç•¥å»ºè®®:")
    print("   a) çƒ­é—¨å•†å“æ¨èï¼šä¸ºæ–°ç”¨æˆ·æ¨èå…¨å±€çƒ­é—¨å•†å“")
    print("   b) åŸºäºå†…å®¹çš„æ¨èï¼šæ ¹æ®å•†å“å±æ€§å’Œç”¨æˆ·äººå£ç»Ÿè®¡å­¦ä¿¡æ¯")
    print("   c) ä¸»åŠ¨å­¦ä¹ ï¼šè®©ç”¨æˆ·å¯¹ä¸€äº›çƒ­é—¨å•†å“è¿›è¡Œè¯„åˆ†")
    print("   d) æ··åˆç­–ç•¥ï¼šç»“åˆå¤šç§æ–¹æ³•æé«˜å†·å¯åŠ¨æ€§èƒ½")


def analyze_user_segments(analyzer, user_data):
    """
    åˆ†æç”¨æˆ·åˆ†ç¾¤

    Args:
        analyzer: æ•°æ®åˆ†æå™¨
        user_data: ç”¨æˆ·è¡Œä¸ºæ•°æ®
    """
    print("\nğŸ‘¥ ç”¨æˆ·åˆ†ç¾¤åˆ†æ")

    # ç”¨æˆ·ç”»åƒåˆ†æ
    profiling = analyzer.analyze_user_profiling(user_data)

    if 'user_segments' in profiling:
        print("\nç”¨æˆ·ä»·å€¼åˆ†å±‚:")
        for segment, count in profiling['user_segments'].items():
            print(f"   - {segment}: {count:,} ç”¨æˆ·")

    if 'demographics' in profiling:
        demographics = profiling['demographics']

        if 'age' in demographics:
            age_info = demographics['age']
            print(f"\nå¹´é¾„åˆ†å¸ƒ:")
            print(f"   - å¹³å‡å¹´é¾„: {age_info.get('mean', 0):.1f} å²")
            print(f"   - å¹´é¾„èŒƒå›´: {age_info.get('min', 0)} - {age_info.get('max', 0)} å²")

        if 'gender' in demographics:
            gender_info = demographics['gender']
            print(f"\næ€§åˆ«åˆ†å¸ƒ:")
            for gender, count in gender_info.items():
                print(f"   - {gender}: {count:,} ç”¨æˆ·")

        if 'cities' in demographics:
            city_info = demographics['cities']
            print(f"\nåŸå¸‚åˆ†å¸ƒ:")
            print(f"   - è¦†ç›–åŸå¸‚: {city_info.get('total_cities', 0)} ä¸ª")
            print(f"   - æœ€æ´»è·ƒåŸå¸‚: {city_info.get('most_active_city', 'N/A')}")


def compare_evaluation_methods(evaluator, engine, user_item_matrix):
    """
    å¯¹æ¯”ä¸åŒè¯„ä¼°æ–¹æ³•

    Args:
        evaluator: è¯„ä¼°å™¨
        engine: æ¨èå¼•æ“
        user_item_matrix: ç”¨æˆ·-å•†å“çŸ©é˜µ
    """
    print("\nğŸ“Š è¯„ä¼°æ–¹æ³•å¯¹æ¯”åˆ†æ")

    # é™åˆ¶è¯„ä¼°è§„æ¨¡ä»¥æé«˜æ¼”ç¤ºé€Ÿåº¦
    max_users = min(15, len(user_item_matrix))

    # 1. ç•™ä¸€æ³•è¯„ä¼°
    print("\n1. ç•™ä¸€æ³•è¯„ä¼° (Leave-One-Out):")
    loo_results = evaluator.leave_one_out_evaluation(
        engine, user_item_matrix,
        k_values=[5, 10],
        num_users=max_users
    )

    print(f"   - è¯„ä¼°ç”¨æˆ·æ•°: {loo_results.get('evaluated_users', 0)}")
    print(f"   - Precision@5: {loo_results.get('precision@5', 0):.4f}")
    print(f"   - Recall@5: {loo_results.get('recall@5', 0):.4f}")

    # 2. äº¤å‰éªŒè¯è¯„ä¼°
    print("\n2. äº¤å‰éªŒè¯è¯„ä¼°:")
    cv_results = evaluator.cross_validation_evaluation(
        engine, user_item_matrix,
        cv_folds=3,
        k_values=[5, 10]
    )

    print(f"   - è¯„ä¼°æŠ˜æ•°: {cv_results.get('cv_folds', 0)}")
    print(f"   - Precision@5: {cv_results.get('precision@5', 0):.4f}")
    print(f"   - Recall@5: {cv_results.get('recall@5', 0):.4f}")

    # 3. è¯„ä¼°æ–¹æ³•å¯¹æ¯”
    print("\nğŸ“ˆ è¯„ä¼°æ–¹æ³•å¯¹æ¯”:")
    methods = ['ç•™ä¸€æ³•', 'äº¤å‰éªŒè¯']
    precision_scores = [loo_results.get('precision@5', 0), cv_results.get('precision@5', 0)]
    recall_scores = [loo_results.get('recall@5', 0), cv_results.get('recall@5', 0)]

    for i, method in enumerate(methods):
        print(f"   {method}:")
        print(f"     - Precision@5: {precision_scores[i]:.4f}")
        print(f"     - Recall@5: {recall_scores[i]:.4f}")


def create_interactive_dashboard(visualizer, recommendations, evaluation_results):
    """
    åˆ›å»ºäº¤äº’å¼ä»ªè¡¨æ¿

    Args:
        visualizer: å¯è§†åŒ–å™¨
        recommendations: æ¨èç»“æœ
        evaluation_results: è¯„ä¼°ç»“æœ
    """
    print("\nğŸ“Š åˆ›å»ºäº¤äº’å¼ä»ªè¡¨æ¿")

    # æ³¨æ„ï¼šè¿™é‡Œåªæ˜¯ç¤ºä¾‹ä»£ç æ¡†æ¶
    # å®é™…çš„äº¤äº’å¼ä»ªè¡¨æ¿éœ€è¦ä½¿ç”¨ plotly dash æˆ– streamlit

    try:
        # åˆ›å»ºäº¤äº’å¼æ¨èå›¾è¡¨
        if recommendations:
            interactive_rec_fig = visualizer.create_interactive_recommendations(
                recommendations[:10], "U001"
            )
            print("   âœ… äº¤äº’å¼æ¨èå›¾è¡¨åˆ›å»ºæˆåŠŸ")

        # åˆ›å»ºç®—æ³•æ¯”è¾ƒå›¾è¡¨
        comparison_data = {
            'Algorithm': ['User-CF', 'Item-CF', 'SVD', 'Hybrid'],
            'Precision@5': [0.15, 0.18, 0.22, 0.25],
            'Recall@5': [0.12, 0.14, 0.18, 0.20]
        }
        comparison_df = pd.DataFrame(comparison_data)

        interactive_comp_fig = visualizer.create_interactive_comparison(comparison_df)
        print("   âœ… äº¤äº’å¼ç®—æ³•æ¯”è¾ƒå›¾è¡¨åˆ›å»ºæˆåŠŸ")

        print("\nğŸ’¡ äº¤äº’å¼ä»ªè¡¨æ¿åŠŸèƒ½åŒ…æ‹¬:")
        print("   - åŠ¨æ€ç­›é€‰æ¨èç®—æ³•")
        print("   - å®æ—¶è°ƒæ•´æ¨èå‚æ•°")
        print("   - äº¤äº’å¼æ¢ç´¢ç”¨æˆ·è¡Œä¸ºæ¨¡å¼")
        print("   - è‡ªå®šä¹‰è¯„ä¼°æŒ‡æ ‡é€‰æ‹©")

    except Exception as e:
        print(f"   âŒ äº¤äº’å¼ä»ªè¡¨æ¿åˆ›å»ºå¤±è´¥: {str(e)}")


def generate_comprehensive_report(evaluator, analysis_results, evaluation_results):
    """
    ç”Ÿæˆç»¼åˆåˆ†ææŠ¥å‘Š

    Args:
        evaluator: è¯„ä¼°å™¨
        analysis_results: æ•°æ®åˆ†æç»“æœ
        evaluation_results: è¯„ä¼°ç»“æœ
    """
    print("\nğŸ“‹ ç”Ÿæˆç»¼åˆåˆ†ææŠ¥å‘Š")

    # ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š
    report = evaluator.generate_evaluation_report(
        evaluation_results,
        "æ¨èç³»ç»ŸæŠ€èƒ½é«˜çº§ç¤ºä¾‹"
    )

    # æ·»åŠ æ•°æ®åˆ†æç»“æœ
    data_summary = "\n## æ•°æ®æ´å¯Ÿåˆ†æ\n"

    if 'user_behavior' in analysis_results:
        ub = analysis_results['user_behavior']
        data_summary += f"- ç”¨æˆ·è§„æ¨¡: {ub.get('total_users', 0):,} ç”¨æˆ·\n"
        data_summary += f"- å•†å“è§„æ¨¡: {ub.get('total_items', 0):,} å•†å“\n"
        data_summary += f"- äº¤äº’è§„æ¨¡: {ub.get('total_interactions', 0):,} æ¬¡äº¤äº’\n"

    if 'sparsity' in analysis_results:
        sp = analysis_results['sparsity']
        data_summary += f"- æ•°æ®ç¨€ç–åº¦: {sp.get('sparsity_ratio', 0):.2%}\n"
        data_summary += f"- å¹³å‡æ¯ç”¨æˆ·äº¤äº’: {sp.get('avg_interactions_per_user', 0):.1f} æ¬¡\n"

    if 'cold_start' in analysis_results:
        cs = analysis_results['cold_start']
        data_summary += f"- å†·å¯åŠ¨ä¸¥é‡ç¨‹åº¦: {cs.get('cold_start_severity', 'æœªçŸ¥')}\n"

    # æ·»åŠ å»ºè®®å’Œç»“è®º
    recommendations = "\n## ä¼˜åŒ–å»ºè®®\n"
    recommendations += "1. ç®—æ³•ä¼˜åŒ–ï¼š\n"
    recommendations += "   - é‡‡ç”¨æ··åˆæ¨èç­–ç•¥æé«˜å‡†ç¡®ç‡\n"
    recommendations += "   - ä¼˜åŒ–ç›¸ä¼¼åº¦è®¡ç®—æ–¹æ³•\n"
    recommendations += "   - å¼•å…¥æ·±åº¦å­¦ä¹ æ¨¡å‹\n\n"

    recommendations += "2. å†·å¯åŠ¨é—®é¢˜ï¼š\n"
    recommendations += "   - å®æ–½çƒ­é—¨å•†å“æ¨èç­–ç•¥\n"
    recommendations += "   - ç»“åˆåŸºäºå†…å®¹çš„æ¨è\n"
    recommendations += "   - è®¾è®¡ä¸»åŠ¨å­¦ä¹ æœºåˆ¶\n\n"

    recommendations += "3. ç³»ç»Ÿæ¶æ„ï¼š\n"
    recommendations += "   - å®ç°å¢é‡å­¦ä¹ æœºåˆ¶\n"
    recommendations += "   - å»ºç«‹A/Bæµ‹è¯•æ¡†æ¶\n"
    recommendations += "   - ä¼˜åŒ–å®æ—¶æ¨èæ€§èƒ½\n"

    # åˆå¹¶æŠ¥å‘Š
    full_report = report + data_summary + recommendations

    print("âœ… ç»¼åˆåˆ†ææŠ¥å‘Šç”Ÿæˆå®Œæˆ")
    print("\nğŸ“„ æŠ¥å‘Šä¸»è¦å†…å®¹:")
    print("   - æ¨èç®—æ³•æ€§èƒ½è¯„ä¼°")
    print("   - æ•°æ®è´¨é‡å’Œæ´å¯Ÿåˆ†æ")
    print("   - å†·å¯åŠ¨é—®é¢˜åˆ†æ")
    print("   - ä¼˜åŒ–å»ºè®®å’Œæ”¹è¿›æ–¹å‘")

    return full_report


def main():
    """ä¸»å‡½æ•°ï¼šæ¼”ç¤ºé«˜çº§æ¨èç³»ç»ŸåŠŸèƒ½"""
    print("=" * 80)
    print("æ¨èç³»ç»ŸæŠ€èƒ½ - é«˜çº§ç¤ºä¾‹")
    print("=" * 80)

    # 1. åˆå§‹åŒ–ç»„ä»¶
    print("\nğŸš€ åˆå§‹åŒ–æ¨èç³»ç»Ÿç»„ä»¶...")
    engine = RecommendationEngine()
    evaluator = RecommenderEvaluator()
    analyzer = DataAnalyzer()
    visualizer = RecommenderVisualizer()

    # 2. åŠ è½½å’Œåˆ†ææ•°æ®
    print("\nğŸ“Š åŠ è½½å’Œæ·±åº¦åˆ†ææ•°æ®...")
    data_dir = Path(__file__).parent / "sample_data"
    user_behavior_path = data_dir / "sample_user_behavior.csv"
    item_info_path = data_dir / "sample_item_info.csv"

    user_data, item_data = engine.load_data(str(user_behavior_path), str(item_info_path))

    if user_data is None:
        print("âŒ æ•°æ®åŠ è½½å¤±è´¥")
        return

    # å…¨é¢æ•°æ®åˆ†æ
    print("   - ç”¨æˆ·è¡Œä¸ºåˆ†æ...")
    user_analysis = analyzer.analyze_user_behavior(user_data)

    print("   - å•†å“çƒ­åº¦åˆ†æ...")
    item_analysis = analyzer.analyze_item_popularity(user_data, item_data)

    print("   - æ•°æ®ç¨€ç–åº¦åˆ†æ...")
    sparsity_analysis = analyzer.calculate_sparsity(engine.user_item_matrix)

    print("   - å†·å¯åŠ¨é—®é¢˜æ£€æµ‹...")
    cold_start_analysis = analyzer.detect_cold_start(user_data)

    print("   - ç”¨æˆ·ç”»åƒåˆ†æ...")
    profiling_analysis = analyzer.analyze_user_profiling(user_data, item_data)

    print("   - æ•°æ®è´¨é‡æ£€æŸ¥...")
    quality_analysis = analyzer.generate_data_quality_report(user_data, item_data)

    # 3. è®­ç»ƒå’Œä¼˜åŒ–æ¨¡å‹
    print("\nğŸ¤– è®­ç»ƒå’Œä¼˜åŒ–æ¨èæ¨¡å‹...")

    # è®­ç»ƒå¤šç§æ¨èç®—æ³•
    engine.train_user_based_cf(similarity_metric='cosine', normalize=True)
    engine.train_item_based_cf(similarity_metric='cosine')
    engine.train_svd(n_components=30, random_state=42)

    print("âœ… æ¨¡å‹è®­ç»ƒå®Œæˆ")

    # 4. é«˜çº§è¯„ä¼°åˆ†æ
    print("\nğŸ“ˆ é«˜çº§è¯„ä¼°åˆ†æ...")
    evaluation_results = compare_evaluation_methods(evaluator, engine, engine.user_item_matrix)

    # 5. ç”¨æˆ·åˆ†ç¾¤åˆ†æ
    analyze_user_segments(analyzer, user_data)

    # 6. å†·å¯åŠ¨ç­–ç•¥æ¼”ç¤º
    demonstrate_cold_start_strategies(engine, evaluator)

    # 7. å®æ—¶æ¨èæ¨¡æ‹Ÿ
    print("\nâš¡ å®æ—¶æ¨èåœºæ™¯æ¨¡æ‹Ÿ...")
    new_ratings = [('P001', 5), ('P015', 4), ('P045', 5)]
    simulate_real_time_recommendation(engine, 'U001', new_ratings)

    # 8. ç”Ÿæˆäº¤äº’å¼ä»ªè¡¨æ¿
    print("\nğŸ“Š äº¤äº’å¼ä»ªè¡¨æ¿æ¼”ç¤º...")
    target_user = 'U001'
    recommendations = engine.recommend_hybrid(target_user, top_k=20)
    create_interactive_dashboard(visualizer, recommendations, evaluation_results)

    # 9. ç”Ÿæˆç»¼åˆæŠ¥å‘Š
    print("\nğŸ“‹ ç”Ÿæˆç»¼åˆåˆ†ææŠ¥å‘Š...")

    # æ”¶é›†æ‰€æœ‰åˆ†æç»“æœ
    all_analysis_results = {
        'user_behavior': user_analysis,
        'item_popularity': item_analysis,
        'sparsity': sparsity_analysis,
        'cold_start': cold_start_analysis,
        'user_profiling': profiling_analysis,
        'data_quality': quality_analysis
    }

    # ç”Ÿæˆç»¼åˆæŠ¥å‘Š
    comprehensive_report = generate_comprehensive_report(
        evaluator, all_analysis_results, evaluation_results
    )

    # 10. ä¿å­˜é«˜çº§ç¤ºä¾‹ç»“æœ
    print("\nğŸ’¾ ä¿å­˜é«˜çº§ç¤ºä¾‹ç»“æœ...")
    output_dir = Path(__file__).parent / "advanced_output"
    output_dir.mkdir(exist_ok=True)

    # ä¿å­˜ç»¼åˆæŠ¥å‘Š
    report_path = output_dir / "comprehensive_report.md"
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write(comprehensive_report)

    # ä¿å­˜è¯¦ç»†åˆ†æç»“æœ
    analyzer.save_analysis_results(
        output_dir / "advanced_analysis_results.json",
        format='json'
    )

    # ä¿å­˜è¯„ä¼°ç»“æœ
    evaluator.save_evaluation_results(
        evaluation_results,
        output_dir / "advanced_evaluation_results.json",
        format='json'
    )

    # ä¿å­˜é«˜çº§æ¨èç»“æœ
    advanced_recs = []
    for method in ['user_based_cf', 'item_based_cf', 'svd', 'hybrid']:
        try:
            if method == 'user_based_cf':
                recs = engine.recommend_user_based_cf(target_user, top_k=10)
            elif method == 'item_based_cf':
                recs = engine.recommend_item_based_cf(target_user, top_k=10)
            elif method == 'svd':
                recs = engine.recommend_svd(target_user, top_k=10)
            else:  # hybrid
                recs = engine.recommend_hybrid(target_user, top_k=10)

            for i, (item_id, score) in enumerate(recs, 1):
                advanced_recs.append({
                    'Method': method,
                    'Rank': i,
                    'Item_ID': item_id,
                    'Score': score,
                    'Timestamp': datetime.now().isoformat()
                })
        except Exception as e:
            print(f"   âš ï¸ {method} æ¨èå¤±è´¥: {str(e)}")

    if advanced_recs:
        recs_df = pd.DataFrame(advanced_recs)
        recs_df.to_csv(output_dir / "advanced_recommendations.csv", index=False, encoding='utf-8-sig')

    print("âœ… é«˜çº§ç¤ºä¾‹ç»“æœå·²ä¿å­˜")

    # 11. æ€»ç»“å’Œå±•æœ›
    print("\n" + "=" * 80)
    print("ğŸ‰ æ¨èç³»ç»Ÿé«˜çº§ç¤ºä¾‹å®Œæˆ!")
    print("=" * 80)

    print(f"\nğŸ“ ç”Ÿæˆçš„æ–‡ä»¶:")
    print(f"   - ç»¼åˆæŠ¥å‘Š: {output_dir}/comprehensive_report.md")
    print(f"   - é«˜çº§åˆ†æ: {output_dir}/advanced_analysis_results.json")
    print(f"   - é«˜çº§è¯„ä¼°: {output_dir}/advanced_evaluation_results.json")
    print(f"   - é«˜çº§æ¨è: {output_dir}/advanced_recommendations.csv")

    print(f"\nğŸ¯ å…³é”®æ´å¯Ÿ:")
    print(f"   - æ•°æ®ç¨€ç–åº¦: {sparsity_analysis.get('sparsity_ratio', 0):.2%}")
    print(f"   - å†·å¯åŠ¨ä¸¥é‡ç¨‹åº¦: {cold_start_analysis.get('cold_start_severity', 'æœªçŸ¥')}")
    print(f"   - æœ€ä½³è¯„ä¼°æ–¹æ³•: {'ç•™ä¸€æ³•' if evaluation_results.get('precision@5', 0) > 0 else 'äº¤å‰éªŒè¯'}")

    if 'user_segments' in profiling_analysis:
        segments = profiling_analysis['user_segments']
        dominant_segment = max(segments.items(), key=lambda x: x[1])[0] if segments else 'æœªçŸ¥'
        print(f"   - ä¸»è¦ç”¨æˆ·ç¾¤ä½“: {dominant_segment}")

    print(f"\nğŸš€ æœªæ¥æ”¹è¿›æ–¹å‘:")
    print(f"   1. å¼•å…¥æ·±åº¦å­¦ä¹ æ¨èæ¨¡å‹")
    print(f"   2. å®ç°çœŸæ­£çš„å®æ—¶æ¨èç³»ç»Ÿ")
    print(f"   3. æ„å»ºå®Œæ•´çš„A/Bæµ‹è¯•æ¡†æ¶")
    print(f"   4. å¼€å‘å¯è§£é‡Šçš„æ¨èç®—æ³•")
    print(f"   5. é›†æˆå¤šæ¨¡æ€æ¨èï¼ˆæ–‡æœ¬ã€å›¾åƒã€éŸ³é¢‘ï¼‰")


if __name__ == "__main__":
    main()