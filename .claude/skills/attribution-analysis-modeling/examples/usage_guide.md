# å½’å› åˆ†ææŠ€èƒ½ä½¿ç”¨æŒ‡å—
# Attribution Analysis Skill Usage Guide

## ğŸ“‹ ç›®å½•
- [å¿«é€Ÿå¼€å§‹](#å¿«é€Ÿå¼€å§‹)
- [åŸºç¡€å½’å› åˆ†æ](#åŸºç¡€å½’å› åˆ†æ)
- [é«˜çº§å½’å› åˆ†æ](#é«˜çº§å½’å› åˆ†æ)
- [æ•°æ®å‡†å¤‡æŒ‡å—](#æ•°æ®å‡†å¤‡æŒ‡å—)
- [ç»“æœè§£è¯»](#ç»“æœè§£è¯»)
- [æœ€ä½³å®è·µ](#æœ€ä½³å®è·µ)
- [å¸¸è§é—®é¢˜](#å¸¸è§é—®é¢˜)

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. ç¯å¢ƒå‡†å¤‡

```bash
# å®‰è£…ä¾èµ–
pip install -r requirements.txt

# éªŒè¯å®‰è£…
python -c "import pandas, numpy, matplotlib, seaborn, scipy, networkx; print('âœ… ä¾èµ–å®‰è£…æˆåŠŸ')"
```

### 2. åŸºç¡€ä½¿ç”¨ç¤ºä¾‹

```python
from core_attribution import AttributionAnalyzer
from attribution_visualizer import AttributionVisualizer

# 1. åˆå§‹åŒ–åˆ†æå™¨
analyzer = AttributionAnalyzer()

# 2. åŠ è½½æ•°æ®
df = analyzer.load_and_validate_data('your_data.csv')

# 3. æ„å»ºå®¢æˆ·è·¯å¾„
paths_df = analyzer.build_customer_paths(df)

# 4. è¿è¡ŒåŸºç¡€å½’å› åˆ†æ
results = analyzer.run_basic_attribution_analysis(paths_df)

# 5. ç”Ÿæˆå¯è§†åŒ–
visualizer = AttributionVisualizer()
visualizer.create_attribution_dashboard(results)
```

## ğŸ“Š åŸºç¡€å½’å› åˆ†æ

### æ•°æ®æ ¼å¼è¦æ±‚

```csv
user_id,timestamp,channel,conversion_status,conversion_value,cost
USER001,2024-01-15T10:30:00Z,paid_search,0,0,50
USER001,2024-01-16T14:20:00Z,social_media,0,0,30
USER001,2024-01-18T09:15:00Z,email,1,1000,10
```

### å¿…éœ€å­—æ®µè¯´æ˜

| å­—æ®µå | ç±»å‹ | è¯´æ˜ | ç¤ºä¾‹ |
|--------|------|------|------|
| user_id | å­—ç¬¦ä¸² | å”¯ä¸€å®¢æˆ·æ ‡è¯† | "USER001" |
| timestamp | æ—¶é—´æˆ³ | è§¦ç‚¹æ—¶é—´ | "2024-01-15T10:30:00Z" |
| channel | å­—ç¬¦ä¸² | è¥é”€æ¸ é“ | "paid_search" |
| conversion_status | æ•´æ•° | è½¬åŒ–çŠ¶æ€(0/1) | 1 |
| conversion_value | æ•°å€¼ | è½¬åŒ–ä»·å€¼ | 1000 |
| cost | æ•°å€¼ | è¥é”€æˆæœ¬ | 50 |

### è¿è¡ŒåŸºç¡€å½’å› åˆ†æ

```python
# åŸºç¡€å½’å› åˆ†æç¤ºä¾‹
from core_attribution import AttributionAnalyzer

analyzer = AttributionAnalyzer()

# åŠ è½½å’Œé¢„å¤„ç†æ•°æ®
df = analyzer.load_and_validate_data('marketing_data.csv')
paths_df = analyzer.build_customer_paths(df)

# è¿è¡Œæ‰€æœ‰åŸºç¡€å½’å› æ¨¡å‹
results = analyzer.run_basic_attribution_analysis(paths_df)

# æŸ¥çœ‹ç»“æœ
for model_name, weights in results.items():
    print(f"\n{model_name}:")
    for channel, weight in sorted(weights.items(), key=lambda x: x[1], reverse=True):
        print(f"  {channel}: {weight:.4f} ({weight*100:.1f}%)")
```

### è¾“å‡ºè¯´æ˜

åŸºç¡€å½’å› åˆ†æåŒ…å«ä»¥ä¸‹æ¨¡å‹ï¼š

1. **é¦–æ¬¡æ¥è§¦å½’å› **ï¼š100%åŠŸåŠ³ç»™ç¬¬ä¸€ä¸ªæ¸ é“
2. **æœ€åæ¥è§¦å½’å› **ï¼š100%åŠŸåŠ³ç»™è½¬åŒ–å‰æœ€åä¸€ä¸ªæ¸ é“
3. **çº¿æ€§å½’å› **ï¼šå¹³å‡åˆ†é…ç»™æ‰€æœ‰æ¸ é“
4. **æ—¶é—´è¡°å‡å½’å› **ï¼šæ—¶é—´è¶Šè¿‘æƒé‡è¶Šé«˜
5. **ä½ç½®å½’å› **ï¼šé¦–å°¾40%ï¼Œä¸­é—´å¹³å‡åˆ†é…

## ğŸ® é«˜çº§å½’å› åˆ†æ

### é©¬å°”å¯å¤«é“¾å½’å› 

```python
from markov_chains import MarkovChainAttributor

# åˆå§‹åŒ–é©¬å°”å¯å¤«é“¾å½’å› å™¨
markov_attributor = MarkovChainAttributor()

# æ„å»ºè½¬ç§»çŸ©é˜µ
transition_matrix = markov_attributor.build_transition_matrix(paths_df)

# è®¡ç®—å½’å› æƒé‡
markov_weights = markov_attributor.calculate_attribution_weights()

# åˆ†ææ¸ é“è½¬æ¢
transition_analysis = markov_attributor.analyze_channel_transitions(transition_matrix)

# æ„å»ºæ¸ é“ç½‘ç»œå›¾
channel_graph = markov_attributor.build_channel_graph(transition_matrix)
```

### Shapleyå€¼å½’å› 

```python
from shapley_values import ShapleyValueAttributor

# åˆå§‹åŒ–Shapleyå€¼å½’å› å™¨
shapley_attributor = ShapleyValueAttributor()

# è¿è¡Œå®Œæ•´åˆ†æ
shapley_results = shapley_attributor.run_complete_shapley_analysis(paths_df)

# è·å–å½’å› æƒé‡
attribution_weights = shapley_results['attribution_weights']

# åˆ†ææ¸ é“ååŒæ•ˆåº”
synergy_analysis = shapley_results['channel_synergy']

# è·å–ä¼˜åŒ–å»ºè®®
optimization = shapley_results['optimization']
```

### é«˜çº§å¯è§†åŒ–

```python
from attribution_visualizer import AttributionVisualizer

visualizer = AttributionVisualizer()

# åˆ›å»ºé©¬å°”å¯å¤«é“¾å¯è§†åŒ–
markov_viz = visualizer.create_markov_visualization(markov_results)

# åˆ›å»ºShapleyå€¼å¯è§†åŒ–
shapley_viz = visualizer.create_shapley_visualization(shapley_results)

# åˆ›å»ºç»¼åˆå¯¹æ¯”ä»ªè¡¨æ¿
dashboard = visualizer.create_attribution_dashboard(all_results)
```

## ğŸ“ æ•°æ®å‡†å¤‡æŒ‡å—

### æ•°æ®æ”¶é›†æœ€ä½³å®è·µ

1. **ç»Ÿä¸€ç”¨æˆ·æ ‡è¯†**
   ```python
   # æ ‡å‡†åŒ–ç”¨æˆ·IDæ ¼å¼
   df['user_id'] = df['user_id'].astype(str).str.lower()
   ```

2. **æ—¶é—´æˆ³æ ‡å‡†åŒ–**
   ```python
   # è½¬æ¢ä¸ºç»Ÿä¸€æ—¶é—´æ ¼å¼
   df['timestamp'] = pd.to_datetime(df['timestamp'], utc=True)
   ```

3. **æ¸ é“å‘½åè§„èŒƒ**
   ```python
   # ç»Ÿä¸€æ¸ é“åç§°
   channel_mapping = {
       'google_ads': 'paid_search',
       'facebook': 'social_media',
       'email_newsletter': 'email'
   }
   df['channel'] = df['channel'].map(channel_mapping).fillna(df['channel'])
   ```

### æ•°æ®è´¨é‡æ£€æŸ¥

```python
def validate_attribution_data(df):
    """éªŒè¯å½’å› åˆ†ææ•°æ®è´¨é‡"""
    issues = []

    # æ£€æŸ¥å¿…éœ€å­—æ®µ
    required_columns = ['user_id', 'timestamp', 'channel', 'conversion_status']
    missing_columns = [col for col in required_columns if col not in df.columns]
    if missing_columns:
        issues.append(f"ç¼ºå°‘å¿…éœ€å­—æ®µ: {missing_columns}")

    # æ£€æŸ¥æ•°æ®å®Œæ•´æ€§
    if df['user_id'].isnull().any():
        issues.append("å­˜åœ¨ç©ºçš„ç”¨æˆ·ID")

    if df['timestamp'].isnull().any():
        issues.append("å­˜åœ¨ç©ºçš„æ—¶é—´æˆ³")

    # æ£€æŸ¥è½¬åŒ–æ•°æ®
    if df['conversion_status'].notna().sum() == 0:
        issues.append("æ²¡æœ‰è½¬åŒ–æ•°æ®")

    return issues

# ä½¿ç”¨ç¤ºä¾‹
issues = validate_attribution_data(df)
if issues:
    for issue in issues:
        print(f"âš ï¸ {issue}")
else:
    print("âœ… æ•°æ®è´¨é‡æ£€æŸ¥é€šè¿‡")
```

### æ•°æ®é¢„å¤„ç†

```python
def preprocess_attribution_data(df):
    """é¢„å¤„ç†å½’å› åˆ†ææ•°æ®"""

    # 1. æ•°æ®ç±»å‹è½¬æ¢
    df['user_id'] = df['user_id'].astype(str)
    df['timestamp'] = pd.to_datetime(df['timestamp'])
    df['conversion_status'] = pd.to_numeric(df['conversion_status'], errors='coerce').fillna(0)
    df['conversion_value'] = pd.to_numeric(df.get('conversion_value', 0), errors='coerce').fillna(0)
    df['cost'] = pd.to_numeric(df.get('cost', 0), errors='coerce').fillna(0)

    # 2. å¼‚å¸¸å€¼å¤„ç†
    df['conversion_value'] = df['conversion_value'].clip(lower=0)
    df['cost'] = df['cost'].clip(lower=0)

    # 3. æ—¶é—´æ’åº
    df = df.sort_values(['user_id', 'timestamp'])

    # 4. å»é™¤é‡å¤è®°å½•
    df = df.drop_duplicates(subset=['user_id', 'timestamp', 'channel'], keep='first')

    return df

# ä½¿ç”¨ç¤ºä¾‹
df_clean = preprocess_attribution_data(df)
print(f"é¢„å¤„ç†åæ•°æ®: {len(df_clean)} æ¡è®°å½•")
```

## ğŸ“ˆ ç»“æœè§£è¯»

### å½’å› æƒé‡è§£è¯»

```python
def interpret_attribution_weights(weights):
    """è§£è¯»å½’å› æƒé‡ç»“æœ"""

    sorted_weights = sorted(weights.items(), key=lambda x: x[1], reverse=True)

    print("ğŸ“Š å½’å› æƒé‡è§£è¯»:")
    print("-" * 40)

    # æƒé‡åˆ†ç±»
    for i, (channel, weight) in enumerate(sorted_weights):
        percentage = weight * 100

        if percentage >= 25:
            tier = "ğŸ¥‡ æ ¸å¿ƒæ¸ é“"
            insight = "ä¸»è¦è´¡çŒ®è€…ï¼Œé‡ç‚¹æŠ•å…¥"
        elif percentage >= 15:
            tier = "ğŸ¥ˆ é‡è¦æ¸ é“"
            insight = "é‡è¦è´¡çŒ®è€…ï¼Œä¿æŒæŠ•å…¥"
        elif percentage >= 5:
            tier = "ğŸ¥‰ è¾…åŠ©æ¸ é“"
            insight = "è¾…åŠ©ä½œç”¨ï¼Œé€‚åº¦æŠ•å…¥"
        else:
            tier = "ğŸ’« é•¿å°¾æ¸ é“"
            insight = "å¾®å°è´¡çŒ®ï¼Œè€ƒè™‘ä¼˜åŒ–"

        print(f"{i+1:2d}. {channel:<15} {percentage:>5.1f}% - {tier}")
        print(f"     {insight}")

# ä½¿ç”¨ç¤ºä¾‹
interpret_attribution_weights(results['æœ€åæ¥è§¦å½’å› '])
```

### æ¸ é“ååŒæ•ˆåº”è§£è¯»

```python
def interpret_synergy_effects(synergy_analysis):
    """è§£è¯»æ¸ é“ååŒæ•ˆåº”"""

    print("\nğŸ¤ æ¸ é“ååŒæ•ˆåº”è§£è¯»:")
    print("-" * 50)

    # åˆ†ç±»ååŒæ•ˆåº”
    positive_synergy = []
    neutral_synergy = []
    negative_synergy = []

    for pair_key, synergy in synergy_analysis.items():
        if synergy['synergy_ratio'] > 1.1:
            positive_synergy.append(synergy)
        elif synergy['synergy_ratio'] < 0.9:
            negative_synergy.append(synergy)
        else:
            neutral_synergy.append(synergy)

    if positive_synergy:
        print("ğŸŒŸ æ­£ååŒæ•ˆåº” (1+1>2):")
        for synergy in sorted(positive_synergy, key=lambda x: x['synergy_ratio'], reverse=True)[:3]:
            print(f"  â€¢ {synergy['channel1']} + {synergy['channel2']}: "
                  f"ååŒæ¯” {synergy['synergy_ratio']:.2f}")
            print(f"    å»ºè®®: åŠ å¼ºè¿™ä¸¤ä¸ªæ¸ é“çš„è”åˆè¥é”€")

    if negative_synergy:
        print("âš ï¸ è´ŸååŒæ•ˆåº” (1+1<2):")
        for synergy in sorted(negative_synergy, key=lambda x: x['synergy_ratio'])[:3]:
            print(f"  â€¢ {synergy['channel1']} + {synergy['channel2']}: "
                  f"ååŒæ¯” {synergy['synergy_ratio']:.2f}")
            print(f"    å»ºè®®: é¿å…åŒæ—¶æŠ•æ”¾æˆ–è°ƒæ•´æ—¶é—´é—´éš”")

    if neutral_synergy:
        print("ğŸ“Š ä¸­æ€§ååŒæ•ˆåº”:")
        print(f"  â€¢ {len(neutral_synergy)} ä¸ªæ¸ é“ç»„åˆè¡¨ç°æ­£å¸¸")

# ä½¿ç”¨ç¤ºä¾‹
if 'channel_synergy' in shapley_results:
    interpret_synergy_effects(shapley_results['channel_synergy'])
```

## ğŸ’¡ æœ€ä½³å®è·µ

### 1. æ¨¡å‹é€‰æ‹©ç­–ç•¥

```python
def select_attribution_model(business_context):
    """æ ¹æ®ä¸šåŠ¡åœºæ™¯é€‰æ‹©å½’å› æ¨¡å‹"""

    recommendations = {
        'new_product_launch': {
            'recommended': ['é¦–æ¬¡æ¥è§¦å½’å› ', 'çº¿æ€§å½’å› '],
            'reason': 'æ–°äº§å“éœ€è¦å…³æ³¨è®¤çŸ¥å»ºç«‹ï¼Œé¦–æ¬¡è§¦è¾¾å¾ˆé‡è¦'
        },
        'mature_business': {
            'recommended': ['æœ€åæ¥è§¦å½’å› ', 'ä½ç½®å½’å› '],
            'reason': 'æˆç†Ÿä¸šåŠ¡å…³æ³¨æœ€ç»ˆè½¬åŒ–å†³ç­–ç¯èŠ‚'
        },
        'complex_journey': {
            'recommended': ['é©¬å°”å¯å¤«é“¾å½’å› ', 'Shapleyå€¼å½’å› '],
            'reason': 'å¤æ‚å®¢æˆ·æ—…ç¨‹éœ€è¦è€ƒè™‘è·¯å¾„ä¾èµ–å’ŒååŒæ•ˆåº”'
        },
        'budget_optimization': {
            'recommended': ['Shapleyå€¼å½’å› '],
            'reason': 'é¢„ç®—ä¼˜åŒ–éœ€è¦ç²¾ç¡®çš„è¾¹é™…è´¡çŒ®åˆ†æ'
        }
    }

    context_key = business_context.lower()
    if context_key in recommendations:
        rec = recommendations[context_key]
        print(f"æ¨èå½’å› æ¨¡å‹: {', '.join(rec['recommended'])}")
        print(f"é€‰æ‹©ç†ç”±: {rec['reason']}")
    else:
        print("å»ºè®®ä½¿ç”¨å¤šç§æ¨¡å‹è¿›è¡Œå¯¹æ¯”åˆ†æ")

# ä½¿ç”¨ç¤ºä¾‹
select_attribution_model('complex_journey')
```

### 2. æ€§èƒ½ç›‘æ§å»ºè®®

```python
def monitor_attribution_performance(current_results, benchmark_results=None):
    """ç›‘æ§å½’å› åˆ†ææ€§èƒ½"""

    print("ğŸ“Š å½’å› åˆ†ææ€§èƒ½ç›‘æ§:")
    print("-" * 40)

    # æ¨¡å‹ç¨³å®šæ€§æ£€æŸ¥
    if benchmark_results:
        print("ğŸ” æ¨¡å‹ç¨³å®šæ€§åˆ†æ:")
        for model_name in current_results.keys():
            if model_name in benchmark_results:
                current_weights = current_results[model_name]
                benchmark_weights = benchmark_results[model_name]

                # è®¡ç®—æƒé‡å˜åŒ–
                channels = set(current_weights.keys()) | set(benchmark_weights.keys())
                max_change = 0
                for channel in channels:
                    current_w = current_weights.get(channel, 0)
                    benchmark_w = benchmark_weights.get(channel, 0)
                    change = abs(current_w - benchmark_w)
                    max_change = max(max_change, change)

                if max_change > 0.1:
                    stability = "âš ï¸ ä¸ç¨³å®š"
                elif max_change > 0.05:
                    stability = "ğŸŸ¡ ä¸­ç­‰ç¨³å®š"
                else:
                    stability = "âœ… ç¨³å®š"

                print(f"  {model_name}: {stability} (æœ€å¤§å˜åŒ–: {max_change:.3f})")

    # æ¸ é“é›†ä¸­åº¦åˆ†æ
    for model_name, weights in current_results.items():
        # è®¡ç®—èµ«èŠ¬è¾¾å°”æŒ‡æ•°ï¼ˆHHIï¼‰
        hhi = sum(w**2 for w in weights.values())

        if hhi > 0.25:
            concentration = "é«˜åº¦é›†ä¸­"
            risk = "é«˜é£é™©"
        elif hhi > 0.15:
            concentration = "ä¸­åº¦é›†ä¸­"
            risk = "ä¸­ç­‰é£é™©"
        else:
            concentration = "åˆ†æ•£"
            risk = "ä½é£é™©"

        print(f"  {model_name}: {concentration} (HHI: {hhi:.3f}, {risk})")

# ä½¿ç”¨ç¤ºä¾‹
monitor_attribution_performance(results)
```

### 3. æŠ¥å‘Šç”Ÿæˆæ¨¡æ¿

```python
def generate_attribution_report(results, business_context):
    """ç”Ÿæˆå½’å› åˆ†ææŠ¥å‘Š"""

    report = f"""
# å½’å› åˆ†ææŠ¥å‘Š
# Attribution Analysis Report

## ğŸ“Š æ‰§è¡Œæ‘˜è¦
åˆ†ææ—¶é—´: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}
ä¸šåŠ¡åœºæ™¯: {business_context}
åˆ†ææ¨¡å‹: {', '.join(results.keys())}

## ğŸ¯ å…³é”®å‘ç°

### æ ¸å¿ƒæ¸ é“è¯†åˆ«
"""

    # æ·»åŠ å„æ¨¡å‹çš„æ ¸å¿ƒæ¸ é“
    for model_name, weights in results.items():
        sorted_weights = sorted(weights.items(), key=lambda x: x[1], reverse=True)
        top_channels = sorted_weights[:3]

        report += f"\n#### {model_name}\n"
        for i, (channel, weight) in enumerate(top_channels):
            report += f"{i+1}. **{channel}**: {weight*100:.1f}%\n"

    # æ·»åŠ ä¸šåŠ¡å»ºè®®
    report += """
## ğŸ’¡ ä¸šåŠ¡å»ºè®®

### çŸ­æœŸè¡ŒåŠ¨ (1-3ä¸ªæœˆ)
1. å¢åŠ å¯¹é«˜æƒé‡ã€é«˜ROIæ¸ é“çš„æŠ•å…¥
2. ä¼˜åŒ–ä½æ•ˆæ¸ é“çš„æŠ•æ”¾ç­–ç•¥
3. åŠ å¼ºæ­£ååŒæ•ˆåº”æ¸ é“çš„è”åˆæŠ•æ”¾

### ä¸­æœŸè§„åˆ’ (3-12ä¸ªæœˆ)
1. å»ºç«‹å¤šæ¨¡å‹å½’å› ç›‘æ§ä½“ç³»
2. ä¼˜åŒ–å®¢æˆ·æ—…ç¨‹è®¾è®¡
3. å®æ–½åŠ¨æ€é¢„ç®—åˆ†é…æœºåˆ¶

### é•¿æœŸæˆ˜ç•¥ (1å¹´ä»¥ä¸Š)
1. æ„å»ºé¢„æµ‹æ€§å½’å› æ¨¡å‹
2. å»ºç«‹æ¸ é“ååŒæ•ˆåº”è¯„ä¼°æ¡†æ¶
3. å®æ–½AIé©±åŠ¨çš„å½’å› ä¼˜åŒ–

---
*æŠ¥å‘Šç”±å½’å› åˆ†ææŠ€èƒ½è‡ªåŠ¨ç”Ÿæˆ*
"""

    return report

# ä½¿ç”¨ç¤ºä¾‹
report = generate_attribution_report(results, "ç”µå•†è½¬åŒ–åˆ†æ")
print(report)
```

## â“ å¸¸è§é—®é¢˜

### Q1: å¦‚ä½•å¤„ç†ç¼ºå¤±çš„æˆæœ¬æ•°æ®ï¼Ÿ

```python
# æ–¹æ³•1: ä½¿ç”¨è¡Œä¸šå¹³å‡æ°´å¹³
def estimate_missing_costs(df, industry_benchmarks):
    """ä¼°ç®—ç¼ºå¤±çš„æˆæœ¬æ•°æ®"""
    for channel, benchmark_cpa in industry_benchmarks.items():
        mask = (df['channel'] == channel) & (df['cost'].isna() | (df['cost'] == 0))
        if mask.any():
            # åŸºäºè½¬åŒ–æ•°é‡ä¼°ç®—æˆæœ¬
            conversions = df[mask & (df['conversion_status'] == 1)]['conversion_status'].sum()
            estimated_cost = conversions * benchmark_cpa
            df.loc[mask, 'cost'] = estimated_cost / mask.sum()
    return df

# æ–¹æ³•2: ä½¿ç”¨æ¸ é“å†…éƒ¨å¹³å‡å€¼
def fill_missing_costs_with_average(df):
    """ä½¿ç”¨æ¸ é“å¹³å‡å€¼å¡«å……ç¼ºå¤±æˆæœ¬"""
    for channel in df['channel'].unique():
        channel_mask = df['channel'] == channel
        avg_cost = df.loc[channel_mask, 'cost'].mean()
        df.loc[channel_mask & df['cost'].isna(), 'cost'] = avg_cost
    return df
```

### Q2: å¦‚ä½•å¤„ç†éå¸¸é•¿çš„å®¢æˆ·è·¯å¾„ï¼Ÿ

```python
def handle_long_paths(paths_df, max_path_length=10):
    """å¤„ç†è¿‡é•¿çš„å®¢æˆ·è·¯å¾„"""

    # ç»Ÿè®¡è·¯å¾„é•¿åº¦åˆ†å¸ƒ
    path_lengths = paths_df['path'].apply(len)
    print(f"è·¯å¾„é•¿åº¦ç»Ÿè®¡:")
    print(f"  å¹³å‡é•¿åº¦: {path_lengths.mean():.1f}")
    print(f"  ä¸­ä½æ•°: {path_lengths.median()}")
    print(f"  95%åˆ†ä½æ•°: {path_lengths.quantile(0.95)}")

    # æˆªæ–­è¿‡é•¿è·¯å¾„
    def truncate_path(path):
        if len(path) <= max_path_length:
            return path
        else:
            # ä¿ç•™é¦–å°¾é‡è¦è§¦ç‚¹
            important_count = 4  # å¼€å§‹2ä¸ª + ç»“æŸ2ä¸ª
            middle_count = max_path_length - important_count

            return (path[:2] +
                   path[2:2+middle_count] +
                   path[-2:])

    paths_df['path'] = paths_df['path'].apply(truncate_path)
    paths_df['path_length'] = paths_df['path'].apply(len)

    return paths_df
```

### Q3: å¦‚ä½•éªŒè¯å½’å› ç»“æœçš„å‡†ç¡®æ€§ï¼Ÿ

```python
def validate_attribution_results(attribution_results, holdout_data):
    """ä½¿ç”¨ç•™ç½®æ•°æ®éªŒè¯å½’å› ç»“æœ"""

    validation_scores = {}

    for model_name, weights in attribution_results.items():
        # åœ¨ç•™ç½®æ•°æ®ä¸Šæµ‹è¯•å½’å› æƒé‡
        predicted_performance = 0
        actual_performance = 0

        for channel, weight in weights.items():
            channel_mask = holdout_data['channel'] == channel
            predicted_contribution = weight * holdout_data.loc[channel_mask, 'conversion_value'].sum()

            # ç®€åŒ–çš„éªŒè¯é€»è¾‘ï¼ˆå®é™…åº”ç”¨ä¸­éœ€è¦æ›´å¤æ‚çš„æ–¹æ³•ï¼‰
            actual_contribution = holdout_data.loc[
                (holdout_data['channel'] == channel) &
                (holdout_data['conversion_status'] == 1),
                'conversion_value'
            ].sum()

            predicted_performance += predicted_contribution
            actual_performance += actual_contribution

        # è®¡ç®—é¢„æµ‹å‡†ç¡®åº¦
        if actual_performance > 0:
            accuracy = 1 - abs(predicted_performance - actual_performance) / actual_performance
            validation_scores[model_name] = accuracy

    print("ğŸ” å½’å› æ¨¡å‹éªŒè¯ç»“æœ:")
    for model_name, score in sorted(validation_scores.items(), key=lambda x: x[1], reverse=True):
        print(f"  {model_name}: {score:.3f}")

    return validation_scores
```

### Q4: å¦‚ä½•å¤„ç†å°æ ·æœ¬æ•°æ®ï¼Ÿ

```python
def handle_small_sample_data(paths_df, min_conversions=50):
    """å¤„ç†å°æ ·æœ¬æ•°æ®çš„ç­–ç•¥"""

    total_conversions = paths_df['converted'].sum()

    if total_conversions < min_conversions:
        print(f"âš ï¸ æ ·æœ¬é‡è¾ƒå° (æ€»è½¬åŒ–æ•°: {total_conversions})")
        print("å»ºè®®é‡‡ç”¨ä»¥ä¸‹ç­–ç•¥:")

        # ç­–ç•¥1: ä½¿ç”¨ç®€å•çš„å½’å› æ¨¡å‹
        print("1. ä½¿ç”¨ç®€å•å½’å› æ¨¡å‹ (é¦–æ¬¡/æœ€åæ¥è§¦)")

        # ç­–ç•¥2: å¢åŠ æ•°æ®æ—¶é—´èŒƒå›´
        print("2. å»¶é•¿æ•°æ®æ”¶é›†æ—¶é—´èŒƒå›´")

        # ç­–ç•¥3: åˆå¹¶ç›¸ä¼¼æ¸ é“
        print("3. åˆå¹¶ç›¸ä¼¼æ¸ é“å‡å°‘å¤æ‚æ€§")

        # ç­–ç•¥4: ä½¿ç”¨è´å¶æ–¯æ–¹æ³•
        print("4. è€ƒè™‘ä½¿ç”¨è´å¶æ–¯å½’å› æ–¹æ³•")

        # è‡ªåŠ¨åˆå¹¶ç›¸ä¼¼æ¸ é“ç¤ºä¾‹
        def merge_similar_channels(channel):
            mapping = {
                'google_search': 'paid_search',
                'bing_search': 'paid_search',
                'facebook_ads': 'social_media',
                'instagram_ads': 'social_media',
                'newsletter': 'email',
                'marketing_email': 'email'
            }
            return mapping.get(channel, channel)

        paths_df['path'] = paths_df['path'].apply(
            lambda path: [merge_similar_channels(touch) for touch in path]
        )

        print(f"âœ… æ¸ é“åˆå¹¶åè·¯å¾„æ•°é‡: {len(paths_df)}")

    return paths_df
```

---

ğŸ‰ **æ­å–œï¼** æ‚¨å·²å®Œæˆå½’å› åˆ†ææŠ€èƒ½ä½¿ç”¨æŒ‡å—çš„å­¦ä¹ ã€‚

å¦‚éœ€æ›´å¤šå¸®åŠ©ï¼Œè¯·å‚è€ƒç¤ºä¾‹ä»£ç æˆ–æŸ¥çœ‹æŠ€æœ¯æ–‡æ¡£ã€‚