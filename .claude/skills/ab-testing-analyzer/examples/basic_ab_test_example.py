#!/usr/bin/env python3
"""
åŸºç¡€ABæµ‹è¯•ç¤ºä¾‹

æ¼”ç¤ºABæµ‹è¯•åˆ†æžæŠ€èƒ½çš„æ ¸å¿ƒåŠŸèƒ½ï¼š
- æ•°æ®åŠ è½½å’Œé¢„å¤„ç†
- è½¬åŒ–çŽ‡åˆ†æž
- ç»Ÿè®¡æ˜¾è‘—æ€§æ£€éªŒ
- åŸºç¡€å¯è§†åŒ–
"""

import sys
import os
import pandas as pd
import numpy as np
from pathlib import Path

# æ·»åŠ æŠ€èƒ½è·¯å¾„
skill_path = Path(__file__).parent.parent
sys.path.append(str(skill_path))

from scripts.ab_test_analyzer import ABTestAnalyzer
from scripts.statistical_tests import StatisticalTests
from scripts.visualizer import ABTestVisualizer


def main():
    """ä¸»å‡½æ•°ï¼šæ¼”ç¤ºåŸºç¡€ABæµ‹è¯•åˆ†æžæµç¨‹"""
    print("=" * 60)
    print("ABæµ‹è¯•åˆ†æžæŠ€èƒ½ - åŸºç¡€ç¤ºä¾‹")
    print("=" * 60)

    # 1. åˆå§‹åŒ–ç»„ä»¶
    print("\n1. åˆå§‹åŒ–ABæµ‹è¯•åˆ†æžç»„ä»¶...")
    analyzer = ABTestAnalyzer()
    stats_tests = StatisticalTests()
    visualizer = ABTestVisualizer()

    # 2. åŠ è½½æ ·æœ¬æ•°æ®
    print("\n2. åŠ è½½ABæµ‹è¯•æ ·æœ¬æ•°æ®...")
    data_dir = Path(__file__).parent / "sample_data"
    data_file = data_dir / "sample_ab_test_data.csv"

    if not data_file.exists():
        print(f"âŒ æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {data_file}")
        return False

    data = analyzer.load_data(str(data_file))
    if data is None:
        return False

    print(f"âœ… æ•°æ®åŠ è½½æˆåŠŸï¼š{len(data)} æ¡ç”¨æˆ·è®°å½•")
    print(f"   - å®žéªŒç»„åˆ«: {data['å®žéªŒç»„åˆ«'].unique()}")
    print(f"   - æ€»è½¬åŒ–çŽ‡: {data['è½¬åŒ–çŠ¶æ€'].apply(lambda x: 1 if str(x).strip() in ['æ˜¯', 'True', 'true', '1', 'yes'] else 0).mean():.2%}")

    # 3. æ•°æ®è´¨é‡æ£€æŸ¥
    print("\n3. æ•°æ®è´¨é‡æ£€æŸ¥...")
    quality_report = analyzer.data_quality_check(
        data,
        required_cols=['ç”¨æˆ·ID', 'å®žéªŒç»„åˆ«', 'è½¬åŒ–çŠ¶æ€']
    )

    print(f"   - æ€»è¡Œæ•°: {quality_report['total_rows']:,}")
    print(f"   - æ€»åˆ—æ•°: {quality_report['total_columns']}")
    print(f"   - é‡å¤è¡Œæ•°: {quality_report['duplicate_rows']}")
    print(f"   - å¿…éœ€åˆ—æ£€æŸ¥: {quality_report['required_columns_check']}")

    if quality_report['missing_values']:
        print("   - ç¼ºå¤±å€¼ç»Ÿè®¡:")
        for col, info in quality_report['missing_values'].items():
            print(f"     * {col}: {info['count']} ({info['percentage']:.1f}%)")

    # 4. åŸºç¡€è½¬åŒ–çŽ‡åˆ†æž
    print("\n4. åŸºç¡€è½¬åŒ–çŽ‡åˆ†æž...")
    conversion_results = analyzer.analyze_conversion(
        data=data,
        group_col='å®žéªŒç»„åˆ«',
        conversion_col='è½¬åŒ–çŠ¶æ€',
        control_group='å¯¹ç…§ç»„'
    )

    print("   - å„ç»„è½¬åŒ–çŽ‡:")
    for group, stats in conversion_results['conversion_analysis'].items():
        print(f"     * {group}: {stats['conversion_rate']:.2%} "
              f"({stats['conversions']}/{stats['sample_size']})")

    # 5. æå‡çŽ‡åˆ†æž
    print("\n5. æå‡çŽ‡åˆ†æž...")
    if conversion_results['lift_analysis']:
        for group, lift in conversion_results['lift_analysis'].items():
            print(f"   - {group} vs å¯¹ç…§ç»„:")
            print(f"     * ç»å¯¹æå‡: {lift['absolute_lift']:.2%}")
            print(f"     * ç›¸å¯¹æå‡: {lift['relative_lift']:.2%}")
            print(f"     * æå‡ç™¾åˆ†æ¯”: {lift['lift_percentage']:.1f}%")
            ci_lower, ci_upper = lift['lift_confidence_interval']
            print(f"     * ç½®ä¿¡åŒºé—´: [{ci_lower:.2%}, {ci_upper:.2%}]")

    # 6. ç»Ÿè®¡æ˜¾è‘—æ€§æ£€éªŒ
    print("\n6. ç»Ÿè®¡æ˜¾è‘—æ€§æ£€éªŒ...")

    # è½¬åŒ–çŠ¶æ€æ•°å€¼åŒ–
    data_converted = data.copy()
    data_converted['è½¬åŒ–çŠ¶æ€_æ•°å€¼'] = data_converted['è½¬åŒ–çŠ¶æ€'].apply(
        lambda x: 1 if str(x).strip() in ['æ˜¯', 'True', 'true', '1', 'yes'] else 0
    )

    # tæ£€éªŒ
    t_test_result = stats_tests.t_test(
        data=data_converted,
        group_col='å®žéªŒç»„åˆ«',
        metric_col='è½¬åŒ–çŠ¶æ€_æ•°å€¼',
        test_type='independent'
    )

    print("   - tæ£€éªŒç»“æžœ:")
    print(f"     * ç»Ÿè®¡é‡: {t_test_result['statistic']:.4f}")
    print(f"     * På€¼: {t_test_result['p_value']:.4f}")
    print(f"     * æ•ˆåº”é‡ (Cohen's d): {t_test_result['effect_size']:.4f}")
    print(f"     * æ•ˆåº”é‡è§£é‡Š: {t_test_result['effect_size_interpretation']}")
    print(f"     * æ˜¾è‘—æ€§: {'æ˜¯' if t_test_result['is_significant'] else 'å¦'}")

    # å¡æ–¹æ£€éªŒ
    chi2_result = stats_tests.chi_square_test(
        data=data,
        group_col='å®žéªŒç»„åˆ«',
        outcome_col='è½¬åŒ–çŠ¶æ€',
        test_type='independence'
    )

    print("   - å¡æ–¹æ£€éªŒç»“æžœ:")
    print(f"     * å¡æ–¹ç»Ÿè®¡é‡: {chi2_result['statistic']:.4f}")
    print(f"     * På€¼: {chi2_result['p_value']:.4f}")
    print(f"     * æ•ˆåº”é‡ (Cramer's V): {chi2_result['effect_size']:.4f}")
    print(f"     * æ•ˆåº”é‡è§£é‡Š: {chi2_result['effect_size_interpretation']}")
    print(f"     * æ˜¾è‘—æ€§: {'æ˜¯' if chi2_result['is_significant'] else 'å¦'}")

    # 7. ç•™å­˜çŽ‡åˆ†æž
    print("\n7. ç•™å­˜çŽ‡åˆ†æž...")
    retention_results = analyzer.analyze_retention(
        data=data,
        group_col='å®žéªŒç»„åˆ«',
        retention_col='ç•™å­˜çŠ¶æ€'
    )

    print("   - å„ç»„ç•™å­˜çŽ‡:")
    for group, stats in retention_results['retention_analysis'].items():
        print(f"     * {group}: {stats['retention_rate']:.2%} "
              f"({stats['retained_users']}/{stats['sample_size']})")

    # 8. éšæœºåˆ†ç»„éªŒè¯
    print("\n8. éšæœºåˆ†ç»„éªŒè¯...")
    numeric_features = ['ç´¯è®¡æ¶ˆè´¹é‡‘é¢', 'å¹´é¾„']
    randomization_check = analyzer.check_randomization(
        data=data,
        group_col='å®žéªŒç»„åˆ«',
        feature_cols=numeric_features
    )

    print("   - åˆ†ç»„å‡è¡¡æ€§æ£€æŸ¥:")
    for feature, result in randomization_check.items():
        print(f"     * {feature}:")
        print(f"       - æ£€éªŒæ–¹æ³•: {result['test']}")
        print(f"       - På€¼: {result['p_value']:.4f}")
        print(f"       - åˆ†ç»„å‡è¡¡: {'æ˜¯' if result['is_balanced'] else 'å¦'}")

    # 9. æ ·æœ¬é‡è®¡ç®—
    print("\n9. æ ·æœ¬é‡è®¡ç®—...")
    baseline_rate = conversion_results['conversion_analysis']['å¯¹ç…§ç»„']['conversion_rate']
    expected_lift = 0.10  # æœŸæœ›æå‡10%

    required_sample_size = analyzer.calculate_sample_size(
        baseline_rate=baseline_rate,
        expected_lift=expected_lift,
        alpha=0.05,
        power=0.80
    )

    print(f"   - åŸºå‡†è½¬åŒ–çŽ‡: {baseline_rate:.2%}")
    print(f"   - æœŸæœ›æå‡: {expected_lift:.0%}")
    print(f"   - æ‰€éœ€æ ·æœ¬é‡: {required_sample_size:,} (æ¯ç»„)")
    print(f"   - å½“å‰æ ·æœ¬é‡: {len(data[data['å®žéªŒç»„åˆ«'] == 'å¯¹ç…§ç»„']):,} (å¯¹ç…§ç»„)")

    if len(data[data['å®žéªŒç»„åˆ«'] == 'å¯¹ç…§ç»„']) >= required_sample_size:
        print("   - âœ… æ ·æœ¬é‡å……è¶³")
    else:
        print("   - âš ï¸ æ ·æœ¬é‡ä¸è¶³ï¼Œå»ºè®®å¢žåŠ æ ·æœ¬")

    # 10. ç”Ÿæˆå¯è§†åŒ–
    print("\n10. ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨...")
    output_dir = Path(__file__).parent / "basic_output"
    output_dir.mkdir(exist_ok=True)

    # è½¬åŒ–çŽ‡å¯¹æ¯”å›¾
    fig1 = visualizer.plot_conversion_comparison(
        conversion_results['conversion_analysis'],
        title='ABæµ‹è¯•è½¬åŒ–çŽ‡å¯¹æ¯”',
        show_confidence_interval=True,
        save_path=str(output_dir / "conversion_comparison.png")
    )
    print("   âœ… è½¬åŒ–çŽ‡å¯¹æ¯”å›¾å·²ä¿å­˜")

    # ç•™å­˜çŽ‡æ›²çº¿å›¾
    fig2 = visualizer.plot_retention_curves(
        retention_results['retention_analysis'],
        title='ç”¨æˆ·ç•™å­˜çŽ‡å¯¹æ¯”',
        save_path=str(output_dir / "retention_curves.png")
    )
    print("   âœ… ç•™å­˜çŽ‡æ›²çº¿å›¾å·²ä¿å­˜")

    # ç½®ä¿¡åŒºé—´å›¾
    fig3 = visualizer.plot_confidence_intervals(
        conversion_results['conversion_analysis'],
        title='è½¬åŒ–çŽ‡ç½®ä¿¡åŒºé—´åˆ†æž',
        save_path=str(output_dir / "confidence_intervals.png")
    )
    print("   âœ… ç½®ä¿¡åŒºé—´å›¾å·²ä¿å­˜")

    # å…³é—­å›¾å½¢ä»¥é¿å…å†…å­˜æ³„æ¼
    import matplotlib.pyplot as plt
    plt.close('all')

    # 11. ä¿å­˜åˆ†æžç»“æžœ
    print("\n11. ä¿å­˜åˆ†æžç»“æžœ...")

    # ä¿å­˜è½¬åŒ–çŽ‡åˆ†æžç»“æžœ
    conversion_df = pd.DataFrame([
        {
            'å®žéªŒç»„åˆ«': group,
            'è½¬åŒ–çŽ‡': stats['conversion_rate'],
            'æ ·æœ¬é‡': stats['sample_size'],
            'è½¬åŒ–æ•°': stats['conversions']
        }
        for group, stats in conversion_results['conversion_analysis'].items()
    ])
    conversion_df.to_csv(output_dir / "conversion_results.csv", index=False, encoding='utf-8-sig')

    # ä¿å­˜ç»Ÿè®¡æ£€éªŒç»“æžœ
    stats_results = pd.DataFrame([
        {
            'æ£€éªŒæ–¹æ³•': 'tæ£€éªŒ',
            'ç»Ÿè®¡é‡': t_test_result['statistic'],
            'På€¼': t_test_result['p_value'],
            'æ•ˆåº”é‡': t_test_result['effect_size'],
            'æ˜¾è‘—æ€§': t_test_result['is_significant']
        },
        {
            'æ£€éªŒæ–¹æ³•': 'å¡æ–¹æ£€éªŒ',
            'ç»Ÿè®¡é‡': chi2_result['statistic'],
            'På€¼': chi2_result['p_value'],
            'æ•ˆåº”é‡': chi2_result['effect_size'],
            'æ˜¾è‘—æ€§': chi2_result['is_significant']
        }
    ])
    stats_results.to_csv(output_dir / "statistical_tests.csv", index=False, encoding='utf-8-sig')

    print("âœ… åˆ†æžç»“æžœå·²ä¿å­˜åˆ° basic_output/ ç›®å½•")

    # 12. æ€»ç»“æŠ¥å‘Š
    print("\n" + "=" * 60)
    print("ðŸŽ‰ ABæµ‹è¯•åŸºç¡€åˆ†æžå®Œæˆ!")
    print("=" * 60)

    print(f"\nðŸ“ ç”Ÿæˆçš„æ–‡ä»¶:")
    print(f"   - è½¬åŒ–çŽ‡ç»“æžœ: {output_dir}/conversion_results.csv")
    print(f"   - ç»Ÿè®¡æ£€éªŒç»“æžœ: {output_dir}/statistical_tests.csv")
    print(f"   - è½¬åŒ–çŽ‡å¯¹æ¯”å›¾: {output_dir}/conversion_comparison.png")
    print(f"   - ç•™å­˜çŽ‡æ›²çº¿å›¾: {output_dir}/retention_curves.png")
    print(f"   - ç½®ä¿¡åŒºé—´å›¾: {output_dir}/confidence_intervals.png")

    print(f"\nðŸŽ¯ å…³é”®å‘çŽ°:")

    # èŽ·å–æµ‹è¯•ç»„å’Œå¯¹ç…§ç»„çš„è½¬åŒ–çŽ‡
    if 'æµ‹è¯•ç»„' in conversion_results['conversion_analysis'] and 'å¯¹ç…§ç»„' in conversion_results['conversion_analysis']:
        test_rate = conversion_results['conversion_analysis']['æµ‹è¯•ç»„']['conversion_rate']
        control_rate = conversion_results['conversion_analysis']['å¯¹ç…§ç»„']['conversion_rate']
        lift = (test_rate - control_rate) / control_rate if control_rate > 0 else 0

        print(f"   - æµ‹è¯•ç»„è½¬åŒ–çŽ‡: {test_rate:.2%}")
        print(f"   - å¯¹ç…§ç»„è½¬åŒ–çŽ‡: {control_rate:.2%}")
        print(f"   - ç›¸å¯¹æå‡: {lift:.1%}")
        print(f"   - ç»Ÿè®¡æ˜¾è‘—æ€§: {'æ˜¾è‘—' if t_test_result['is_significant'] else 'ä¸æ˜¾è‘—'}")

        if t_test_result['is_significant'] and lift > 0:
            recommendation = "å»ºè®®æŽ¨å¹¿æµ‹è¯•ç»„æ–¹æ¡ˆ"
        elif t_test_result['is_significant'] and lift < 0:
            recommendation = "å»ºè®®ç»´æŒå¯¹ç…§ç»„æ–¹æ¡ˆ"
        else:
            recommendation = "éœ€è¦æ›´å¤šæ•°æ®æˆ–é‡æ–°è®¾è®¡å®žéªŒ"

        print(f"   - å»ºè®®: {recommendation}")

    print(f"\nðŸ’¡ ABæµ‹è¯•åˆ†æžæŠ€èƒ½ç‰¹æ€§:")
    print(f"   âœ… å®Œæ•´çš„è½¬åŒ–çŽ‡å’Œç•™å­˜çŽ‡åˆ†æž")
    print(f"   âœ… å¤šç§ç»Ÿè®¡æ˜¾è‘—æ€§æ£€éªŒæ–¹æ³•")
    print(f"   âœ… æ•ˆåº”é‡è®¡ç®—å’Œè§£é‡Š")
    print(f"   âœ… éšæœºåˆ†ç»„éªŒè¯")
    print(f"   âœ… æ ·æœ¬é‡è®¡ç®—å’ŒåŠŸæ•ˆåˆ†æž")
    print(f"   âœ… ä¸°å¯Œçš„å¯è§†åŒ–å›¾è¡¨")
    print(f"   âœ… ä¸“ä¸šçš„åˆ†æžæŠ¥å‘Šç”Ÿæˆ")

    return True


if __name__ == "__main__":
    try:
        success = main()
        if success:
            print("\nðŸš€ ABæµ‹è¯•åˆ†æžæŠ€èƒ½éªŒè¯æˆåŠŸï¼å¯ä»¥å¼€å§‹ä½¿ç”¨ã€‚")
        sys.exit(0 if success else 1)
    except Exception as e:
        print(f"\nâŒ ç¤ºä¾‹è¿è¡Œå¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
        sys.exit(1)